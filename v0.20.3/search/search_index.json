{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to WorkFlow Launcher","text":""},{"location":"#overview","title":"Overview","text":"<p>WorkFlow Launcher (WFL) is a workload manager.</p> <p>For example, a workload could be a set of Whole Genome samples to be reprocessed in a given project/bucket, the workflow is the processing of an individual sample in that workload running WGS reprocessing; a workload could also be a queue of incoming notifications that describe all of the required inputs to launch Arrays scientific pipelines in Cromwell.</p> <p>Most recent efforts leverage the general applicability of a staged workload model which automates fetching data from a source, pushing it into a workflow executor for analysis, and delivering the results of the analysis to an output location (also known as a sink).</p> <p>WFL is designed to be deployed to run as a service in the cloud, primarily on Kubernetes clusters.</p> <p>For more on Workflow Launcher's role in the Terra infrastructure see Workflow Launcher's role in Terra.</p>"},{"location":"#quickstart","title":"Quickstart","text":"Tip <p>This is the Quickstart section, which should cover the most frequent uses cases that interact with WFL. For more detailed information, please check other sections such as the development guide or modules design principles.</p>"},{"location":"#build","title":"Build","text":"<p>The easiest way to build WFL is via <code>make</code>, in addition, the following prerequisites are needed:</p> <ul> <li>The Docker daemon</li> <li>Clojure (<code>brew install clojure</code> on macOS)</li> <li>Python3 (<code>brew install python@3.9</code> on macOS)</li> <li>NodeJS (<code>brew install node</code> on macOS)</li> <li>Google Cloud SDK (<code>brew install --cask google-cloud-sdk</code> on macOS)</li> </ul> <p>Arch Linux tips</p> <ul> <li>Install clojure   from the official repository.</li> <li>Install   google-cloud-sdk   from the AUR.</li> </ul> <p>You could then invoke <code>make</code> at the project level to test and build all <code>workflow-launcher</code> modules:</p> <p><pre><code>$ make -j8\n</code></pre> where <code>8</code> can be replaced by any number that represents the concurrent jobs you wish to run.</p> <p>Info</p> <p>If the version of your <code>make</code> is above GNU Make 4.0 (you could check by running <code>make --version</code>), it's highly recommended to use <code>--output-sync</code> along with <code>-j</code> so the standard outputs are sorted, i.e. <pre><code>$ make -j8 --output-sync\n</code></pre></p> <p><code>make</code> will build each module in <code>workflow-launcher</code>, run tests and generate <code>Docker</code> images. All generated files go into a <code>derived</code> directory under the project root.</p> <p>You can also invoke <code>make</code> on a module from the top level directory by</p> <pre><code>$ make [MODULE] TARGET={prebuild|build|check|images|clean|distclean}\n</code></pre> <p>where currently available <code>MODULE</code>s are {api functions/aou docs helm ui}</p> <p>For most of the time, you would want to run something like:</p> <pre><code>$ make clean\n</code></pre> <p>to clean up the built modules (<code>-j8</code> is also available for <code>make clean</code>).</p> <p>and then run:</p> <pre><code>$ make ui api TARGET=images -j8\n</code></pre> <p>to only build the WFL and its docker images without running tests.</p> <p>Info</p> <p>Note if you updated the second party repositories such as <code>pipeline-config</code> or <code>gotc-deploy</code>, you might have to run: <pre><code>$ make distclean\n</code></pre> to remove them. This is not always needed but can help completely purge the local derived files.</p>"},{"location":"#test","title":"Test","text":"<p>If you only want to run tests on specific modules, you could run:</p> <pre><code>$ make [MODULE] TARGET=check\n</code></pre> <p>such as <code>make api TARGET=check</code> or <code>make functions/aou TARGET=check</code>. Note this automatically makes all of <code>check</code>'s prerequisites.</p>"},{"location":"#clojure-test","title":"Clojure Test","text":"<p>When it comes to clojure tests, sometimes it's useful to only run a subset of tests to save time and filter out noise. You can do this by directly invoke <code>clojure</code> cli from within the <code>api</code> directory, for example, <code>cd api</code> and:</p> <pre><code>$ clojure -M:test integration --focus wfl.integration.modules.copyfile-test\n</code></pre> <p>In general, we implement Clojure tests under the <code>test/</code> root directory and use the kaocha test runner. Test suites use a <code>-test</code> namespace suffix. You can pass extra command line arguments to <code>kaocha</code>, such as the above <code>--focus</code> flag. You can see the full list of options with the following:</p> <pre><code>clojure -M:test --help\n</code></pre> <p>At present, <code>wfl</code> api has three kinds of test, <code>unit</code>, <code>integration</code>, and <code>system</code>. These can be run via the <code>deps.edn</code>, optionally specifying the kind:</p> <pre><code>clojure -M:test [unit|integration|system]\n</code></pre> <p>Note that the integration tests currently require a little more configuration before they can be run, namely, they require a <code>wfl</code> server running locally:</p> <pre><code>./ops/server.sh\n</code></pre> <p>Additionally, there is a custom parallel test runner that can be invoked to help speed up the <code>system</code> tests. Rather than <code>clojure -M:test system</code> you'd just specify the namespace(s) to try to parallelize.</p> <pre><code>clojure -M:parallel-test wfl.system.v1-endpoint-test\n</code></pre> <p>Info</p> <p>Note for <code>system</code> tests, no matter it's kicked off through <code>clojure -M:test system</code> or <code>clojure -M:parallel-test wfl.system.v1-endpoint-test</code>, you can use an environment variable <code>WFL_CROMWELL_URL</code> to override the default Cromwell instance that's used in the test. For example:</p> <pre><code>WFL_CROMWELL_URL=https://cromwell-gotc-auth.gotc-prod.broadinstitute.org/ clojure -M:parallel-test wfl.system.v1-endpoint-test\n</code></pre> <p>will tell the test to submit workflows to the \"gotc-prod\" Cromwell instance no matter what the default instance was defined in the test. However, you need to make sure the validity of the Cromwell URL you passed in; certain IAM permissions will also be required in order for Cromwell to execute the testing workflows smoothly.</p>"},{"location":"#deploy","title":"Deploy","text":"<p>Currently, we mainly deploy WFL to <code>broad-gotc-dev</code> and <code>broad-gotc-prod</code> projects. When it's time to deploy WFL, for most of the time developers need to release a new version following the steps in Release Guide</p> <p>After which, the developers who have broad VPN connected can go to the Jenkins Page to deploy applicable versions of WFL to various available cloud projects.</p>"},{"location":"#implementation","title":"Implementation","text":""},{"location":"#top-level-files","title":"Top-level files","text":"<p>After cloning a new WFL repo, the top-level files are: <pre><code>.\n\u251c\u2500\u2500 api/            - `workflow-launcher` backend\n\u251c\u2500\u2500 functions/      - cloud functions deployed separately\n\u251c\u2500\u2500 database/       - database scheme migration changelog and changeset\n\u251c\u2500\u2500 derived/        - generated artifacts\n\u251c\u2500\u2500 docs/           - ancillary documentation\n\u251c\u2500\u2500 helm/           - helm-managed k8s configuration\n\u251c\u2500\u2500 LICENSE.txt\n\u251c\u2500\u2500 Makefile        - project level` Makefile`\n\u251c\u2500\u2500 makerules/      - common `Makefile` functionality\n\u251c\u2500\u2500 ops/            - scripts to support Operations\n\u251c\u2500\u2500 README.md       - symbolic link to docs/md/README.md\n\u2514\u2500\u2500 version         - holds the current semantic version\n</code></pre> Tip: Run <code>make</code> at least once after cloning the repo to make sure all the necessary files are in place.</p>"},{"location":"#api-module","title":"<code>api</code> Module","text":""},{"location":"#source-code","title":"Source code","text":"<p>The Clojure source code is in the <code>api/src/</code> directory.</p> <p>The entry point for the WFL executable is the <code>-main</code> function in <code>main.clj</code>. It takes the command line arguments as strings, validates the arguments, then launches the appropriate process.</p> <p>The <code>server.clj</code> file implements the WFL server. The <code>server_debug.clj</code> file adds some tools to aid in debugging the server.</p> <p>Some hacks specific to WFL are in <code>wfl.clj</code>.</p> <p>The <code>build.clj</code> file includes build and deployment code.</p> <p>The <code>debug.clj</code> file defines some macros useful when debugging or logging.</p> <p>The <code>util.clj</code> file contains a few functions and macros used in WFL that are not specific to its function.</p> <p>The <code>environments.clj</code> file defines configuration parameters for different execution contexts. It's a placeholder in this repo but will be loaded in build/deploy time from a private repo.</p> <p>The <code>module/xx.clj</code> file implements a command-line starter for reprocessing eXternal eXomes.</p> <p>The <code>module/wgs.clj</code> file helps implements a command-line starter for reprocessing Whole GenomeS.</p> <p>The <code>module/sg.clj</code> file implements Somatic Genomes support.</p> <p>The <code>module/all.clj</code> file hosts some utilities shared across modules.</p> <p>The <code>metadata.clj</code> file implements a tool to extract metadata from Cromwell that can be archived with the outputs generated by a workflow.</p> <p>The <code>dx.clj</code> file implements miscellaneous pipeline debugging tools.</p> <p>The <code>once.clj</code> file defines some initialization functions mostly supporting authentication.</p> <p>The <code>api/handlers.clj</code> file defines the handler functions used by server.</p> <p>The <code>api/routes.clj</code> file defines the routing strategy for server.</p> <p>Each of the other source files implement an interface to one of the services WFL talks to, and are named accordingly.</p> File Service cromwell.clj Cromwell workflow runner datarepo.clj DSP DataRepo db.clj On-prem and Cloud SQL databases gcs.clj Google Cloud Storage jms.clj Java Message Service queues postgres.clj Cloud SQL postgres databases server.clj the WFL server itself"},{"location":"#exomes-in-the-cloud-resources","title":"Exomes in the Cloud Resources","text":"<p>From Hybrid Selection in the Cloud V1</p> <ol> <li> <p>Clients</p> <ul> <li>Google Cloud Storage Client Library (Java)</li> <li>Google Cloud Client Library for Java</li> </ul> </li> <li> <p>Diagrams</p> <ul> <li>Zamboni Overview</li> </ul> </li> <li> <p>Sources</p> <ul> <li>/Users/tbl/Broad/zamboni/Client/src/scala/org/broadinstitute/zamboni/client/lightning/clp/Lightning.scala</li> <li>/Users/tbl/Broad/picard-private/src/java/edu/mit/broad/picard/lightning</li> <li>/Users/tbl/Broad/gppipeline-devtools/release<sub>client</sub></li> <li>/Users/tbl/Broad/gppipeline-devtools/starter<sub>control</sub></li> <li>/picard02:/seq/pipeline/gppipeline-devtools/current/defs/prod.defs</li> </ul> </li> </ol>"},{"location":"dev-logging/","title":"WFL Logging","text":"<p>There are macros for logging JSON records named for each reporting level (or severity) supported by Google Stackdriver.</p> <p>Each logging macro takes one required <code>expression</code> argument, and an optional sequence of <code>key</code>/<code>value</code> pairs</p> <p>For example:</p> <pre><code>(wfl.log/info (/ 22 7) :is \"pi\")\n</code></pre> <p>A list of the Google Cloud supported logging fields and severities can be found here: https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry</p> <p>Disable logging by binding <code>wfl.log/*logger*</code> to <code>wfl.log/disabled-logger</code>.</p> <p>Example: <pre><code>(binding [wfl.log/*logger* wfl.log/disabled-logger]\n  (wfl.log/info \"This message will not be logged.\"))\n</code></pre></p>"},{"location":"dev-logging/#logging-levels","title":"Logging Levels","text":"<p>The severity levels are defined by <code>wfl.log/levels</code>.</p> <p>By default, any calls at severity <code>\"INFO\"</code> or above are logged, and severities below <code>\"INFO\"</code> are ignored.</p> <p>The <code>GET /logging_level</code> API returns the least severe enabled level.</p> <pre><code>curl -X GET http://localhost:3000/api/v1/logging_level \\\n     -H 'accept: application/json' \\\n     -H \"authorization: Bearer \"$(gcloud auth print-access-token)\n</code></pre> <p>The response will look something like this: <pre><code>{\n  \"level\" : \"INFO\"\n}\n</code></pre></p> <p>Set the logging level with <code>POST</code>. <pre><code>curl -X POST http://localhost:3000/api/v1/logging_level?level=DEBUG \\\n     -H 'accept: application/json' \\\n     -H \"authorization: Bearer \"$(gcloud auth print-access-token)\n</code></pre></p> <p>The response is similar. <pre><code>{\n  \"level\" : \"DEBUG\"\n}\n</code></pre></p> <p>The above change allows all logs at<code>\"DEBUG\"</code> and higher through.</p>"},{"location":"dev-logging/#log-keys","title":"Log keys","text":"<p>Some logging frameworks, such as Stackdriver, treat some field keys in log records specially.</p> <p>The WFL log calls will translate keywords qualified to the <code>wfl.log</code> namespace into the keys expected by the framework. For example, when logging with Stackdriver, the following <code>info</code> call will add a <code>logging.googleapis.com/spanId</code> key to the record logging <code>some-query</code>.</p> <pre><code>(require '[wfl.log :as log])\n(let [spanId (+ 1000 (rand-int 1000)))]\n  (log/info some-query ::log/spanId spanId))\n</code></pre>"},{"location":"dev-logging/#testing","title":"Testing","text":"<p>Tests are in <code>wfl.unit/logging-test</code>.</p>"},{"location":"dev-logging/#debugging","title":"Debugging","text":"<p>Search locally for specific logs this way.</p> <ol> <li> <p>Make sure you have <code>jq</code> installed for your terminal.</p> </li> <li> <p>Run the server with <code>./ops/server.sh &gt;&gt; path/to/wfl/log 2&gt;&amp;1</code></p> </li> <li> <p>Look up logs by severity and only show the message.</p> </li> </ol> <pre><code>tail -f path/to/wfl/log |\n  grep --line-buffered -w '\"severity\":\"[YOUR_SEVERITY_HERE]\"' |\n  jq .message\n</code></pre> <ol> <li>Look up logs by a label and print the message.</li> </ol> <pre><code>tail -f path/to/wfl/log |\n  grep --line-buffered -w 'my-label' |\n  jq .message\n</code></pre>"},{"location":"dev-monitoring/","title":"Workflow Launcher Monitoring","text":"<p>Logs from stdout and stderr are sent to Google Logging (Stackdriver) where they can be queried. With the logs, metrics can be created to see developments from those logs over time. From those metrics, we can create alerts that are sent to notification channels of our choosing (slack, email, sms, pubsub, etc.).</p> <p>To create a metric via command line: <pre><code>gcloud auth login\ngcloud config set project PROJECT_ID\ngcloud beta logging metrics create MY-METRIC-NAME --description=\"description goes here\" --filter=\"filter goes here\"\n</code></pre></p> <p>The log entries for WFL should be located under a container name of <code>workflow-launcher-api</code> so logging queries to find said logs should contain <code>resource.labels.container_name=\"workflow-launcher-api\"</code>. To look for log severities of error and above, include <code>severity&gt;=ERROR</code> in the metric filter as well. You can exclude specific items in the query with the <code>NOT</code> keyword (ex: <code>NOT \"INFO: \"</code> excludes messages that contain <code>\"INFO: \"</code>)</p> <p>An example query for all wfl errors of severity ERROR and above: <pre><code>resource.labels.container_name=\"workflow-launcher-api\"\nseverity&gt;=ERROR\n</code></pre></p> <p>To create an alert via command line: <pre><code>gcloud auth login\ngcloud config set project PROJECT_ID\ngcloud alpha monitoring policies create --policy-from-file=\"path/to/file\"\n</code></pre></p> <p>Example policies can be found here: https://cloud.google.com/monitoring/alerts/policies-in-json</p> <p>When a metric goes over the threshold set by the policy, an alert is sent via the notification channels provided in the configuration. An incident is created in google cloud monitoring under alerts. These incidents will resolve themselves once the time series shows the metric condition of the alert going back under the configured threshold.</p>"},{"location":"dev-process/","title":"Development Process","text":"<p>This is a development process we are tying to standardize within the team and encourage ourselves to follow in most cases.</p>"},{"location":"dev-process/#the-swagger-page","title":"The Swagger page","text":"<p>WFL ships with a Swagger UI that documents all available endpoints. It's available at path <code>&lt;host&gt;/swagger</code>.</p> <ul> <li>Dev WFL Swagger</li> <li>Prod WFL Swagger</li> </ul> <p>For local access, see accessing Swagger Locally.</p>"},{"location":"dev-process/#development-setup","title":"Development Setup","text":"<p>Clojure development feels very different from Scala and Java development. It even differs markedly from development in other dynamic languages such as Python or Ruby.</p> <p>Get a demonstration from someone familiar with Clojure development before you spend too much time trying to figure things out on your own.</p> <p>Find a local Cursive user for guidance if you like IntelliJ. Olivia Kotsopoulos knows how to use it.</p> <p>Cursive licences are available here. If none are available, free non-commercial licenses are suitable for open-source development.</p> <p>The steps for getting this project set up with very recent versions of IntelliJ differ from Cursive's docs:</p> Tip <p>Run <code>make prebuild</code> before launching IntelliJ as it sets up all libraries and derived resources and sources: <pre><code>make TARGET=prebuild -jN\n</code></pre></p> <ol> <li>Outside of IntelliJ, <code>clone</code> the repo.</li> <li>Now inside of IntelliJ, import the project.</li> <li>Use the Project Structure window (Help -&gt; Find Action -&gt; Project Structure)    to set a JDK as the Project SDK</li> </ol> <p>There is also a Calva plugin for Visual Studio Code.</p> <p>Tom Lyons hacks Clojure in Emacs using CIDER and nREPL. CIDER is not trivial to set up, but not especially difficult if you are used to Emacs. (I can help if CIDER gives you trouble.)</p>"},{"location":"dev-process/#process","title":"Process","text":"<p>We base feature branches off <code>develop</code>, make pull requests, ask for reviews and merge back to <code>develop</code> on Github.</p> <p>For the release process, please refer to the release guide.</p> <ol> <li> <p>Clone the repo     <pre><code>git@github.com:broadinstitute/wfl.git\n</code></pre></p> </li> <li> <p>Start from the latest copy of the remote develop     <pre><code>git checkout develop\ngit pull origin develop\n</code></pre></p> </li> <li> <p>Create a feature branch</p> <p>It is highly recommended that you follow the naming convention shown below so JIRA could pick up the branch and link it to our JIRA board. <pre><code>git checkout -b tbl/GH-666-feature-branch-something\n</code></pre></p> </li> <li> <p>Start your work, add and commit your changes     <pre><code>git add \"README.md\"\ngit commit -m \"Update the readme file.\"\n</code></pre></p> </li> <li> <p>[Optional] Rebase onto latest develop if you want to get updates     <pre><code>git checkout develop\ngit pull origin develop --ff\ngit checkout tbl/GH-666-feature-branch-something\ngit rebase develop\n</code></pre></p> <p>alternatively, you could use the following commands without switching branches: <pre><code>git checkout tbl/GH-666-feature-branch-something\ngit fetch origin develop\ngit merge develop\n</code></pre></p> </li> <li> <p>Push branch to Github in the early stage of your development (recommended):     <pre><code>git push --set-upstream origin tbl/GH-666-feature-branch-something\n</code></pre></p> </li> <li> <p>Create the pull request on Github UI. Be sure to fill out the PR description    following the PR template instructions.</p> <ul> <li> <p>If the PR is still in development, make sure use the dropdown menu and   choose <code>Create draft pull request</code></p> </li> <li> <p>If the PR is ready for review, click <code>Create pull request</code>.</p> </li> </ul> </li> <li> <p>Look for reviewer(s) in the team.</p> </li> <li> <p>Address reviewer comments with more commits.</p> </li> <li> <p>Receive approval from reviewers.</p> </li> <li> <p>Merge the PR.</p> </li> </ol>"},{"location":"dev-process/#development-tips","title":"Development Tips","text":"<p>Here are some tips for WFL development.</p> <p>Some of this advice might help when testing Liquibase migration or other changes that affect WFL's Postgres database.</p>"},{"location":"dev-process/#setting-up-a-local-postgres","title":"setting up a local Postgres","text":"<p>You can test against a local Postgres before running Liquibase or SQL against a shared database in <code>gotc-dev</code> or gasp production.</p> <ol> <li> <p>Install Postgres locally.     You need version 11 because that is what Google's hosted service supports,     and there are differences in the SQL syntax.</p> <pre><code>brew install postgresql@11\n</code></pre> </li> <li> <p>Start Postgres.</p> <pre><code>pg_ctl -D /usr/local/var/postgresql@11 start\n</code></pre> <p>Tip</p> <p>It might be useful to set up some an alias for postgres if you are using zsh, for example: <pre><code>alias pq=\"pg_ctl -D /usr/local/var/postgresql@11\"\n</code></pre> thus you could use <code>pq start</code> or <code>pq stop</code> to easily spin up and turn down the db.</p> </li> <li> <p>[Optional] Create wfl DB.</p> <p>If you see errors like this when launching a local WFL server or applying liquibase updates:</p> <pre><code>FATAL:  database \"wfl\" does not exist\n</code></pre> <p>You should do as instructed within your terminal:</p> <pre><code>createdb wfl\n</code></pre> <p>Or to recreate an existing wfl DB:</p> <pre><code>dropdb wfl\ncreatedb wfl\n</code></pre> </li> </ol> <p>You are now free to launch a local WFL server pointing to your local DB.</p> <p>Assuming that <code>WFL_POSTGRES_URL</code> in <code>(wfl.environment/defaults)</code> is set to point at a running local Postgres (e.g. <code>jdbc:postgresql:wfl</code>), running <code>./ops/server.sh</code> (or however you launch a local WFL server) will connect the server to that running local Postgres.</p> <p>Now any changes to WFL state will affect only your local database. That includes running Liquibase, so don't forget to reset <code>:debug</code> to <code>env</code> before deploying your changes after merging a PR.</p>"},{"location":"dev-process/#migrating-a-database","title":"migrating a database","text":"<p>To change WFL's Postgres database schema, add a changeset XML file in the <code>database/changesets</code> directory. Name the file for a recent or the current date followed by something describing the change. That will ensure that the changesets list in the order in which they apply. Note that the <code>id</code> and <code>logicalFilePath</code> attributes are derived from the changeset's file name. Then add the changeset file to the <code>database/changlog.xml</code> file.</p> <p>Test the changes against a local scratch database. See the next section for suggestions.</p>"},{"location":"dev-process/#debugging-jdbc-sql","title":"debugging JDBC SQL","text":"<p>Something seems to swallow SQL exceptions raised by Postgres and the JDBC library. Wrap suspect <code>clojure.java.jdbc</code> calls in <code>wfl.util/do-or-nil</code> to ensure that any exceptions show up in the server logs.</p>"},{"location":"dev-process/#debugging-api-specs","title":"debugging API specs","text":"<p>If an API references an undefined spec, HTTP requests and responses might silently fail or the Swagger page will fail to render. Check the <code>clojure.spec.alpha/def</code>s in <code>wfl.api.routes</code> for typos before tearing your hair out.</p>"},{"location":"dev-process/#accessing-swagger-locally","title":"accessing Swagger locally","text":"<p>First, start a local WFL server.</p> <pre><code>./ops/server.sh\n</code></pre> <p>To view the rendered Swagger page: <pre><code>open http://localhost:3000/swagger\n</code></pre></p>"},{"location":"dev-process/#debugging-liquibase-locally","title":"debugging Liquibase locally","text":"<p>Running <code>liquibase update</code>: <pre><code>liquibase --classpath=$(clojure -Spath)          \\\n          --url=jdbc:postgresql:wfl               \\\n          --changeLogFile=database/changelog.xml \\\n          --username=$USER update\n</code></pre> For the above, the username and password need to be correct for the target environment.</p> <p>If you're running a local server with the postgres command above, you don't need a password and can omit it.</p> <p>Otherwise, you may be able to find this data in the Vault entry for the environment's server -- <code>resources/wfl/environments.clj</code> has some environments if you've built locally. You can use <code>--password=$ENV_SOMETHING</code> to supply it.</p> <p>Tip</p> <p>It is more convenient to use the following alias to migrate the database schema from within the <code>api</code> directory: <pre><code>clojure -M:liquibase\n</code></pre> if you are working with a local database.</p>"},{"location":"dev-process/#override-environment-variables-for-local-development","title":"Override ENVIRONMENT variables for local development","text":"<p>WFL uses <code>src/wfl/api/environment.clj</code> to read and process environment variables. Most of the variables have their default values, which can be overwritten for development purposes. For example, if we want to run system tests in parallel against a local WFL instance, use below under <code>api/</code> directory:</p> <pre><code>WFL_WFL_URL=http://localhost:3000 clojure -M:parallel-test wfl.system.v1-endpoint-test\n</code></pre>"},{"location":"dev-process/#repl-testing-with-fixtures","title":"REPL testing with fixtures.","text":"<p>Now that we're using fixtures, and so on, in our tests, it is no longer good enough to run <code>deftest</code> vars as functions. Running a test like this <code>(test-something)</code> does not set up the necessary fixtures.</p> <p>However, <code>clojure.test/test-vars</code> can run a test with all the surrounding <code>clojure.test</code> mechanism in place. It takes a vector of <code>var</code>s like this.</p> <pre><code>(comment (test-vars [#'test-something]))\n</code></pre>"},{"location":"dev-process/#no-tests-found","title":"No tests found","text":"<p>When trying to run tests in the command line, you may see the test suite exit prematurely -- but successfully -- with a warning indicating that no tests were found, and thus no tests were run.</p> <pre><code>$ make api TARGET=check\nexport CPCACHE=/Users/okotsopo/wfl/api/.cpcache;     \\\n    clojure  -M:test unit | \\\n    tee /Users/okotsopo/wfl/derived/api/unit.log\n...\nWARNING: No tests were found, make sure :test-paths and :ns-patterns are configured correctly in tests.edn.\napi unit finished on Thu Jun 24 15:03:33 EDT 2021\n...\n</code></pre> <p>This may indicate a compilation error in code not compiled as part of the build, e.g. tests.  Linting can help expose any such errors.</p> <p>Tip</p> <p>By default, linting will halt on the first thrown exception, requiring further linting after fixing until the process succeeds.</p> <pre><code>$ make api TARGET=lint\n...\n== Eastwood 0.4.2 Clojure 1.10.3 JVM 11.0.10 ==\nDirectories scanned for source files:\n\nsrc test\n...\n== Linting wfl.system.cdc-covid19-surveillance-demo ==\nException thrown during phase :analyze+eval of linting namespace wfl.system.cdc-covid19-surveillance-demo\nGot exception with extra ex-data:\n    msg='No such var: covid'\n    (keys dat)=(:form :file :end-column :column :line :end-line)\n    (:form dat)=\n^{:line 101} covid/rename-gather\n\nExceptionInfo No such var: covid\n...\nAn exception was thrown while analyzing namespace wfl.system.cdc-covid19-surveillance-demo\nLint results may be incomplete.  If there are compilation errors in\nyour code, try fixing those.  If not, check above for info on the\nexception.\n\nStopped analyzing namespaces after wfl.system.cdc-covid19-surveillance-demo\ndue to exception thrown.  28 namespaces left unanalyzed.\n\nIf you wish to force continuation of linting after an exception in one\nnamespace, make the option map key :continue-on-exception have the\nvalue true.\n...\n</code></pre>"},{"location":"dev-release/","title":"Release Process","text":"<p>The <code>main</code> branch contains tagged release commits. We follow a simple process in order to release a new version of WFL:</p> <ol> <li>Create a release branch based off <code>develop</code></li> <li>release branch names follow the convention <code>release/X.Y.Z-rc</code></li> <li>the version string should match that specified in <code>version</code></li> <li>Identify and cherry-pick additional commits from <code>develop</code> that you want    to release (e.g. late features and bug fixes).</li> <li>Create a release candidate and deploy to a testing environment. See    instructions bellow.</li> <li>Bash    the release candidate. Add/cherry-pick any bug fixes that result.</li> <li>Create a pull request into <code>main</code>. You will need to run    <code>./ops/cli.py release</code> to generate the changelog for this release (the <code>-d</code>    flag can be used to do a dry run without writing to the <code>CHANGELOG.md</code>file).</li> <li>When the PR is approved, merge it into <code>main</code>. The release action will run    automatically to build, test and build and push the tagged docker images of    WFL to DockerHub.    Please merge PRs that have passed all automated tests only.</li> </ol> <p>Tip</p> <p>It can take up to 30 minutes for the Github Action to finish! Please be patient!</p> <p>Tip</p> <p>Remember to create a PR to bump the version string in <code>version</code> in <code>develop</code> for the next release, including changes to <code>CHANGELOG.md</code> from the release.</p>"},{"location":"dev-release/#creating-a-release-candidate","title":"Creating a Release Candidate","text":"<p>In this example, we will create a release candidate for vX.Y.Z. We will assume the existence of a release branch <code>release/X.Y.Z-rc</code>. From <code>wfl</code>'s root directory:</p> <ol> <li> <p>Ensure your local repository clone is clean     <pre><code>$ make distclean\n</code></pre></p> </li> <li> <p>Prepare sources     <pre><code>$ git checkout release/X.Y.Z-rc\n$ git pull origin release/X.Y.Z-rc --ff\n</code></pre></p> </li> <li> <p>Build the docker images locally     <pre><code>$ make TARGET=images\n</code></pre></p> </li> <li> <p>Tag the commit and release the images to dockerhub with the release    candidate tag. Let us assume that this is the Mth release candidate.     <pre><code>$ ./ops/cli.py tag-and-push-images --version=X.Y.Z-rcN\n</code></pre></p> </li> </ol> <p>Tip</p> <p>You can run <code>make</code> in parallel by adding <code>-jN</code> to the end of your <code>make</code> command, where <code>N</code> is the number of concurrent jobs to run.</p>"},{"location":"modules-aou-arrays/","title":"All Of Us Arrays module","text":"<p>WorkFlow Launcher (WFL) implements <code>aou-arrays</code> module to support secure and efficient processing of the AllOfUs Arrays samples. This page documents the design principles and assumptions of the module as well as summarizes the general process of module development.</p> <p><code>aou-arrays</code> module implements arrays workload as a continuous workload, which means all samples are coming in like a continuous stream, and WFL does not make any assumption of how many samples will be in the workload or how to group the samples together: it hands off the workload creation and starting process to its caller.</p>"},{"location":"modules-aou-arrays/#api","title":"API","text":"<p><code>aou-arrays</code> module, like others, implements the following multimethod dispatchers:</p> <ul> <li>start-workload!</li> <li>add-workload!</li> </ul> <p>It supports the following API endpoints:</p> Verb Endpoint Description GET <code>/api/v1/workload</code> List all workloads, optionally filtering by uuid or project GET <code>/api/v1/workload/{uuid}/workflows</code> List all workflows for a specified workload uuid POST <code>/api/v1/create</code> Create a new workload POST <code>/api/v1/start</code> Start a workload POST <code>/api/v1/stop</code> Stop a running workload POST <code>/api/v1/exec</code> Create and start (execute) a workload POST <code>/api/v1/append_to_aou</code> Append one or more sample(s) to an existing AOU workload, unless stopped <p>Different from the fixed workload types that caller only needs to create a workload with a series of sample inputs and then simply start the workload, <code>aou-arrays</code> module requires the caller to manage the life cycle of a workload on their own in a multi-stage manner:</p> <ol> <li>The caller needs to create a workload and specify the type to be <code>AllOfUsArrays</code>, the caller will receive the information of the created workload if everything goes well, one of which is the <code>uuid</code> of the workload.</li> <li>Once the workload information is being reviewed, the caller needs to \"start\" the newly created workload to tell WFL that \"this workload is ready to accept incoming samples\". Without this \"start\" signal WFL will refuse to append any sample to this workload.</li> <li>The caller can append new individual samples to an existing started workload, and these new samples will be analyzed, processed and submitted to Cromwell as long as it has valid information.</li> </ol> <p>To give more information, here are some example inputs to the above endpoints:</p> <p>GET /api/v1/workload</p> Request <pre><code>curl 'http://localhost:8080/api/v1/workload' \\\n    -H 'Authorization: Bearer '$(gcloud auth print-access-token) \\\n    -H 'Accept: application/json'\n</code></pre> <p>GET /api/v1/workload?uuid={uuid}</p> Request <pre><code>curl 'http://localhost:8080/api/v1/workload?uuid=00000000-0000-0000-0000-000000000000' \\\n    -H 'Authorization: Bearer '$(gcloud auth print-access-token) \\\n    -H 'Accept: application/json'\n</code></pre> <p>GET /api/v1/workload?project={project}</p> Request <pre><code>curl 'http://localhost:8080/api/v1/workload?project=(Test)%20WFL%20Local%20Testing' \\\n    -H 'Authorization: Bearer '$(gcloud auth print-access-token) \\\n    -H 'Accept: application/json'\n</code></pre> <p>Note</p> <p><code>project</code> and <code>uuid</code> are optional path parameters to the <code>/api/v1/workload</code> endpoint, hitting this endpoint without them will return all workloads. However, they cannot be specified together.</p> <p>GET /api/v1/workload/{uuid}/workflows</p> Request <pre><code>curl 'http://localhost:8080/api/v1/workload/00000000-0000-0000-0000-000000000000/workflows' \\\n    -H 'Authorization: Bearer '$(gcloud auth print-access-token) \n</code></pre> <p>POST /api/v1/create</p> Request <pre><code>curl -X POST 'http://localhost:8080/api/v1/create' \\\n     -H 'Authorization: Bearer '$(gcloud auth print-access-token) \\\n     -H 'Accept: application/json' \\\n     -H 'Content-Type: application/json' \\\n     -d '{\n           \"executor\": \"https://cromwell-gotc-auth.gotc-dev.broadinstitute.org\",\n           \"output\":   \"gs://broad-gotc-dev-wfl-ptc-test-outputs/aou-test-output/\",\n           \"project\":  \"Example Project\",\n           \"pipeline\": \"AllOfUsArrays\"\n         }'\n</code></pre> <p>POST /api/v1/start</p> Request <pre><code>curl -X POST 'http://localhost:8080/api/v1/start' \\\n     -H 'Authorization: Bearer '$(gcloud auth print-access-token) \\\n     -H 'Accept: application/json' \\\n     -H 'Content-Type: application/json' \\\n     -d $'{ \"uuid\": \"00000000-0000-0000-0000-000000000000\" }'\n</code></pre> <p>POST /api/v1/stop</p> <p>Stops the workload from accepting new samples. See also: <code>/api/v1/append_to_aou</code>.</p> Request <pre><code>curl -X POST 'http://localhost:8080/api/v1/stop' \\\n     -H 'Authorization: Bearer '$(gcloud auth print-access-token) \\\n     -H 'Accept: application/json' \\\n     -H 'Content-Type: application/json' \\\n     -d $'{ \"uuid\": \"00000000-0000-0000-0000-000000000000\" }'\n</code></pre> <p>POST /api/v1/workload/append_to_aou</p> Request <pre><code>curl -X POST 'http://localhost:8080/api/v1/append_to_aou' \\\n     -H 'Authorization: Bearer '$(gcloud auth print-access-token) \\\n     -H 'Accept: application/json' \\\n     -H 'Content-Type: application/json' \\\n     -d $'{\n  \"uuid\": \"00000000-0000-0000-0000-000000000000\",\n  \"notifications\": [\n    {\n      \"zcall_thresholds_file\": \"foo\",\n      \"sample_lsid\": \"foo\",\n      \"bead_pool_manifest_file\": \"foo\",\n      \"chip_well_barcode\": \"foo\",\n      \"sample_alias\": \"foo\",\n      \"green_idat_cloud_path\": \"foo\",\n      \"red_idat_cloud_path\": \"foo\",\n      \"cluster_file\": \"foo\",\n      \"reported_gender\": \"foo\",\n      \"gender_cluster_file\": \"foo\",\n      \"params_file\": \"foo\",\n      \"extended_chip_manifest_file\": \"foo\",\n      \"analysis_version_number\": 1\n    },\n    {\n      \"zcall_thresholds_file\": \"foo\",\n      \"sample_lsid\": \"foo\",\n      \"bead_pool_manifest_file\": \"foo\",\n      \"chip_well_barcode\": \"foo\",\n      \"sample_alias\": \"foo\",\n      \"green_idat_cloud_path\": \"foo\",\n      \"red_idat_cloud_path\": \"foo\",\n      \"cluster_file\": \"foo\",\n      \"reported_gender\": \"foo\",\n      \"gender_cluster_file\": \"foo\",\n      \"params_file\": \"foo\",\n      \"extended_chip_manifest_file\": \"foo\",\n      \"analysis_version_number\": 5\n    }\n  ]\n}'\n</code></pre>"},{"location":"modules-aou-arrays/#workload-model","title":"Workload Model","text":"<p>WFL designed the following workload model in order to support the above API and workload submission mechanism.</p> <p>Initially, it has the following schema:</p> <pre><code>                                    List of relations\n Schema |              Name              |   Type   |  Owner   |    Size    | Description\n--------+--------------------------------+----------+----------+------------+-------------\n public | databasechangelog              | table    | foo      | 16 kB      |\n public | databasechangeloglock          | table    | foo      | 8192 bytes |\n public | workload                       | table    | foo      | 16 kB      |\n public | workload_id_seq                | sequence | foo      | 8192 bytes |\n</code></pre> <p>the <code>workload</code> table looks like:</p> <pre><code> id | commit | created | creator | cromwell | finished | input | items | output | pipeline | project | release | started | uuid | version | wdl\n----+--------+---------+---------+----------+----------+-------+-------+--------+----------+---------+---------+---------+------+---------+-----\n(0 rows)\n</code></pre> <p>Note that different from the fixed workload types, <code>input</code>, <code>output</code> and <code>items</code> are not useful to <code>aou-arrays</code> workload since these fields vary from sample to sample. Any information the caller provided to these fields will stored as placeholders.</p> <p>More importantly, even though <code>id</code> is the <code>primary</code> key here, <code>(pipeline, project, release)</code> works as the unique identifier for arrays workloads, for instance, if there's already a workload with values: <code>(AllOfUsArrays, gotc-dev, Arrays_v1.9)</code>, any further attempts to create a new workload with exact the same values will return the information of this existing workload rather than create a new row.</p> <p>Once the caller successfully creates a new sample, there will be a new row added to the above <code>workload</code> table, and a new table will be created accordingly:</p> <pre><code>                                    List of relations\n Schema |              Name              |   Type   |  Owner   |    Size    | Description\n--------+--------------------------------+----------+----------+------------+-------------\n public | allofusarrays_000000001        | table    | foo      | 16 kB      |\n public | allofusarrays_000000001_id_seq | sequence | foo      | 8192 bytes |\n public | databasechangelog              | table    | foo      | 16 kB      |\n public | databasechangeloglock          | table    | foo      | 8192 bytes |\n public | workload                       | table    | foo      | 16 kB      |\n public | workload_id_seq                | sequence | foo      | 8192 bytes |\n</code></pre> <p>The <code>allofusarrays_00000000X</code> table has the following fields:</p> <p><pre><code> id | analysis_version_number | chip_well_barcode |  status   |            updated            |                 uuid\n----+-------------------------+-------------------+-----------+-------------------------------+--------------------------------------\n  1 |                       1 | 0000000000_R01C01 | Succeeded | 2020-07-21 00:00:00.241218-04 | 00000000-0000-0000-0000-000000000000\n  2 |                       5 | 0000000000_R01C01 | Failed    | 2020-07-21 00:00:00.028976-04 | 00000000-0000-0000-0000-000000000000\n</code></pre> Among which <code>(analysis_version_number, chip_well_barcode)</code> works as the <code>primary key</code>, any new samples that collide with the existing <code>primary-keys</code> will be omitted.</p>"},{"location":"modules-external-exome-reprocessing/","title":"External Exome Reprocessing workload","text":""},{"location":"modules-external-exome-reprocessing/#inputs","title":"Inputs","text":"<p>An <code>ExternalExomeReprocessing</code> workload requires the specification of exactly one the following inputs for each workflow:</p> <ul> <li><code>input_bam</code>, OR</li> <li><code>input_cram</code></li> </ul> <p>Where <code>input_bam</code> and <code>input_cram</code> are GS URLs of the file to reprocess. Note that the <code>input_bam</code> and <code>input_cram</code> inputs should only be used with CRAM and BAM files, respectively. All other WDL inputs are optional - see the output below for all options.</p>"},{"location":"modules-external-exome-reprocessing/#usage","title":"Usage","text":"<p>External Exome Reprocessing workload supports the following API endpoints:</p> Verb Endpoint Description GET <code>/api/v1/workload</code> List all workloads, optionally filtering by uuid or project GET <code>/api/v1/workload/{uuid}/workflows</code> List all workflows for a specified workload uuid POST <code>/api/v1/create</code> Create a new workload POST <code>/api/v1/start</code> Start a workload POST <code>/api/v1/stop</code> Stop a running workload POST <code>/api/v1/exec</code> Create and start (execute) a workload Permissions in production <p>External Exome Reprocessing in <code>gotc-prod</code> uses a set of execution projects, please refer to  this page when you have questions about permissions.</p>"},{"location":"modules-external-exome-reprocessing/#create-workload-post-apiv1create","title":"Create Workload: <code>POST /api/v1/create</code>","text":"<p>Create a new workload. Ensure that <code>workflow-launcher</code> and <code>cromwell</code>'s service accounts have at least read access to the input files.</p> Request <pre><code>curl -X POST 'https://gotc-prod-wfl.gotc-prod.broadinstitute.org/api/v1/create' \\\n  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n        \"executor\": \"https://cromwell-gotc-auth.gotc-prod.broadinstitute.org\",\n        \"output\": \"gs://broad-gotc-dev-wfl-ptc-test-outputs/xx-test-output/\",\n        \"pipeline\": \"ExternalExomeReprocessing\",\n        \"project\": \"Example Project\",\n        \"items\": [{\n          \"inputs\": {\n            \"input_cram\" : \"gs://broad-gotc-dev-wfl-ptc-test-inputs/single_sample/plumbing/truth/develop/20k/NA12878_PLUMBING.cram\"\n          }\n        }]\n      }'\n</code></pre> Response <pre><code>{\n  \"creator\" : \"user@domain\",\n  \"pipeline\" : \"ExternalExomeReprocessing\",\n  \"executor\" : \"https://cromwell-gotc-auth.gotc-prod.broadinstitute.org\",\n  \"release\" : \"ExternalExomeReprocessing_vX.Y.Z\",\n  \"created\" : \"YYYY-MM-DDTHH:MM:SSZ\",\n  \"output\" : \"gs://broad-gotc-dev-wfl-ptc-test-outputs/xx-test-output/\",\n  \"commit\" : \"commit-ish\",\n  \"project\" : \"Example Project\",\n  \"uuid\" : \"1337254e-f7d8-438d-a2b3-a74b199fee3c\",\n  \"wdl\" : \"pipelines/broad/reprocessing/external/exome/ExternalExomeReprocessing.wdl\",\n  \"version\" : \"X.Y.Z\"\n}\n</code></pre> <p>Note that the ExternalExomeReprocessing pipeline supports specifying cromwell \"workflowOptions\" via the <code>options</code> map. See the reference page for more information.</p>"},{"location":"modules-external-exome-reprocessing/#start-workload-post-apiv1start","title":"Start Workload: <code>POST /api/v1/start</code>","text":"<p>Starts a Cromwell workflow for each item in the workload. If an output already exists in the output bucket for a particular input cram, WFL will not re-submit that workflow.</p> Request <pre><code>curl -X POST 'https://gotc-prod-wfl.gotc-prod.broadinstitute.org/api/v1/start' \\\n-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n-H 'Content-Type: application/json' \\\n-d '{\"uuid\": \"1337254e-f7d8-438d-a2b3-a74b199fee3c\"}'\n</code></pre> Response <pre><code>{\n  \"creator\" : \"user@domain\",\n  \"pipeline\" : \"ExternalExomeReprocessing\",\n  \"executor\" : \"https://cromwell-gotc-auth.gotc-prod.broadinstitute.org\",\n  \"release\" : \"ExternalExomeReprocessing_vX.Y.Z\",\n  \"created\" : \"YYYY-MM-DDTHH:MM:SSZ\",\n  \"started\" : \"YYYY-MM-DDTHH:MM:SSZ\",\n  \"output\" : \"gs://broad-gotc-dev-wfl-ptc-test-outputs/xx-test-output/\",\n  \"commit\" : \"commit-ish\",\n  \"project\" : \"Example Project\",\n  \"uuid\" : \"1337254e-f7d8-438d-a2b3-a74b199fee3c\",\n  \"wdl\" : \"pipelines/broad/reprocessing/external/exome/ExternalExomeReprocessing.wdl\",\n  \"version\" : \"X.Y.Z\"\n}\n</code></pre>"},{"location":"modules-external-exome-reprocessing/#stop-workload-post-apiv1stop","title":"Stop Workload: <code>POST /api/v1/stop</code>","text":"<p>Included for compatibility with continuous workloads. </p> Request <pre><code>curl -X POST 'https://gotc-prod-wfl.gotc-prod.broadinstitute.org/api/v1/stop' \\\n     -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n     -H 'Content-Type: application/json' \\\n     -d '{ \"uuid\": \"1337254e-f7d8-438d-a2b3-a74b199fee3c\" }'\n</code></pre> Response <pre><code>{\n  \"creator\" : \"user@domain\",\n  \"pipeline\" : \"ExternalExomeReprocessing\",\n  \"executor\" : \"https://cromwell-gotc-auth.gotc-prod.broadinstitute.org\",\n  \"release\" : \"ExternalExomeReprocessing_vX.Y.Z\",\n  \"created\" : \"YYYY-MM-ddTHH:mm:ssZ\",\n  \"started\" : \"YYYY-MM-ddTHH:mm:ssZ\",\n  \"stopped\" : \"YYYY-MM-ddTHH:mm:ssZ\",\n  \"output\" : \"gs://broad-gotc-dev-wfl-ptc-test-outputs/xx-test-output/\",\n  \"commit\" : \"commit-ish\",\n  \"project\" : \"Example Project\",\n  \"uuid\" : \"1337254e-f7d8-438d-a2b3-a74b199fee3c\",\n  \"wdl\" : \"pipelines/broad/reprocessing/external/exome/ExternalExomeReprocessing.wdl\",\n  \"version\" : \"X.Y.Z\"\n}\n</code></pre>"},{"location":"modules-external-exome-reprocessing/#exec-workload-post-apiv1exec","title":"Exec Workload: <code>POST /api/v1/exec</code>","text":"<p>Creates and then starts a Cromwell workflow for each item in the workload.</p> Request <pre><code>curl -X POST 'https://dev-wfl.gotc-dev.broadinstitute.org/api/v1/exec' \\\n  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n        \"executor\": \"https://cromwell-gotc-auth.gotc-prod.broadinstitute.org\",\n        \"output\": \"gs://broad-gotc-dev-wfl-ptc-test-outputs/xx-test-output/\",\n        \"pipeline\": \"ExternalExomeReprocessing\",\n        \"project\": \"Example Project\",\n        \"items\": [{\n          \"inputs\" : {\n            \"input_cram\" : \"gs://broad-gotc-dev-wfl-ptc-test-inputs/single_sample/plumbing/truth/develop/20k/NA12878_PLUMBING.cram\",\n          }\n        }]\n      }'\n</code></pre> Response <pre><code>{\n  \"creator\" : \"user@domain\",\n  \"pipeline\" : \"ExternalExomeReprocessing\",\n  \"executor\" : \"https://cromwell-gotc-auth.gotc-prod.broadinstitute.org\",\n  \"release\" : \"ExternalExomeReprocessing_vX.Y.Z\",\n  \"created\" : \"YYYY-MM-DDTHH:MM:SSZ\",\n  \"started\" : \"YYYY-MM-DDTHH:MM:SSZ\",\n  \"output\" : \"gs://broad-gotc-dev-wfl-ptc-test-outputs/xx-test-output/\",\n  \"commit\" : \"commit-ish\",\n  \"project\" : \"Example Project\",\n  \"uuid\" : \"1337254e-f7d8-438d-a2b3-a74b199fee3c\",\n  \"wdl\" : \"pipelines/broad/reprocessing/external/exome/ExternalExomeReprocessing.wdl\",\n  \"version\" : \"X.Y.Z\"\n}\n</code></pre>"},{"location":"modules-external-exome-reprocessing/#query-workload-get-apiv1workloaduuiduuid","title":"Query Workload: <code>GET /api/v1/workload?uuid=&lt;uuid&gt;</code>","text":"<p>Queries the WFL database for workloads. Specify the uuid to query for a specific workload.</p> Request <pre><code>curl -X GET 'https://gotc-prod-wfl.gotc-prod.broadinstitute.org/api/v1/workload?uuid=1337254e-f7d8-438d-a2b3-a74b199fee3c' \\\n  -H \"Authorization: Bearer $(gcloud auth print-access-token)\"\n</code></pre> Response <pre><code>[{\n  \"creator\" : \"user@domain\",\n  \"pipeline\" : \"ExternalExomeReprocessing\",\n  \"executor\" : \"https://cromwell-gotc-auth.gotc-prod.broadinstitute.org\",\n  \"release\" : \"ExternalExomeReprocessing_vX.Y.Z\",\n  \"created\" : \"YYYY-MM-DDTHH:MM:SSZ\",\n  \"started\" : \"YYYY-MM-DDTHH:MM:SSZ\",\n  \"output\" : \"gs://broad-gotc-dev-wfl-ptc-test-outputs/xx-test-output/\",\n  \"commit\" : \"commit-ish\",\n  \"project\" : \"Example Project\",\n  \"uuid\" : \"1337254e-f7d8-438d-a2b3-a74b199fee3c\",\n  \"wdl\" : \"pipelines/broad/reprocessing/external/exome/ExternalExomeReprocessing.wdl\",\n  \"version\" : \"X.Y.Z\"\n}]\n</code></pre>"},{"location":"modules-external-exome-reprocessing/#query-workload-with-project-get-apiv1workloadprojectproject","title":"Query Workload with project: <code>GET /api/v1/workload?project=&lt;project&gt;</code>","text":"<p>Queries the WFL database for workloads. Specify the project name to query for a list of specific workload(s).</p> Request <pre><code>curl -X GET 'https://gotc-prod-wfl.gotc-prod.broadinstitute.org/api/v1/workload?project=PO-1234' \\\n  -H \"Authorization: Bearer $(gcloud auth print-access-token)\"\n</code></pre> Response <pre><code>[{\n  \"creator\" : \"user@domain\",\n  \"pipeline\" : \"ExternalExomeReprocessing\",\n  \"executor\" : \"https://cromwell-gotc-auth.gotc-prod.broadinstitute.org\",\n  \"release\" : \"ExternalExomeReprocessing_vX.Y.Z\",\n  \"created\" : \"YYYY-MM-DDTHH:MM:SSZ\",\n  \"started\" : \"YYYY-MM-DDTHH:MM:SSZ\",\n  \"output\" : \"gs://broad-gotc-dev-wfl-ptc-test-outputs/xx-test-output/\",\n  \"commit\" : \"commit-ish\",\n  \"project\" : \"Example Project\",\n  \"uuid\" : \"1337254e-f7d8-438d-a2b3-a74b199fee3c\",\n  \"wdl\" : \"pipelines/broad/reprocessing/external/exome/ExternalExomeReprocessing.wdl\",\n  \"version\" : \"X.Y.Z\"\n}]\n</code></pre> <p>Note</p> <p><code>project</code> and <code>uuid</code> are optional query parameters to the <code>/api/v1/workload</code> endpoint, hitting this endpoint without them will return all workloads. However, they cannot be specified together.</p>"},{"location":"modules-external-exome-reprocessing/#list-workflows-in-a-workload-get-apiv1workloaduuidworkflows","title":"List Workflows in a Workload: <code>GET /api/v1/workload/{uuid}/workflows</code>","text":"<p>Returns the workflows created and managed by the workload with <code>uuid</code>.</p> Request <pre><code>curl -X GET 'https://gotc-prod-wfl.gotc-prod.broadinstitute.org/api/v1/workload/1337254e-f7d8-438d-a2b3-a74b199fee3c/workflows' \\\n     -H \"Authorization: Bearer $(gcloud auth print-access-token)\"\n</code></pre> Response <pre><code>[{\n    \"status\" : \"Submitted\",\n    \"updated\" : \"YYYY-MM-DDTHH:MM:SSZ\",\n    \"uuid\" : \"bb0d93e3-1a6a-4816-82d9-713fa58fb235\",\n    \"inputs\" : {\n        \"input_cram\" : \"gs://broad-gotc-dev-wfl-ptc-test-inputs/single_sample/plumbing/truth/develop/20k/NA12878_PLUMBING.cram\",\n        \"destination_cloud_path\" : \"gs://broad-gotc-dev-wfl-ptc-test-outputs/xx-test-output/8600be1a-48df-4a51-bdba-0044e0af8d33/single_sample/plumbing/truth/develop/20k\",\n        \"sample_name\" : \"NA12878_PLUMBING\",\n        \"base_file_name\" : \"NA12878_PLUMBING.cram\",\n        \"final_gvcf_base_name\" : \"NA12878_PLUMBING.cram\"\n    }\n}]\n</code></pre> <p>\"workflows\" lists out each workflow managed by this workload, including their status. It is also possible to use the Job Manager to check workflow progress and easily see information about any workflow failures.</p>"},{"location":"modules-external-exome-reprocessing/#a1-external-exome-workload-request-json-spec","title":"A1 External Exome Workload-Request JSON Spec","text":"<pre><code>{\n     \"pipeline\": \"string\",\n     \"executor\": \"string\",\n     \"output\": \"string\",\n     \"project\": \"string\",\n     \"common\": {\n       \"options\": {}, // see workflow-options\n       \"inputs\": {\n         \"unmapped_bam_suffix\":    \"string\",\n         \"cram_ref_fasta\":       \"string\",\n         \"cram_ref_fasta_index\": \"string\",\n         \"bait_set_name\":        \"string\",\n         \"bait_interval_list\":   \"string\",\n         \"target_interval_list\": \"string\",\n         \"references\": {\n           \"calling_interval_list\":      \"string\",\n           \"contamination_sites_bed\":    \"string\",\n           \"contamination_sites_mu\":     \"string\",\n           \"contamination_sites_ud\":     \"string\",\n           \"dbsnp_vcf\":                  \"string\",\n           \"dbsnp_vcf_index\":            \"string\",\n           \"evaluation_interval_list\":   \"string\",\n           \"haplotype_database_file\":     \"string\",\n           \"known_indels_sites_vcfs\":    [\"string\"],\n           \"known_indels_sites_indices\": [\"string\"],\n           \"reference_fasta\": {\n             \"ref_pac\":         \"string\",\n             \"ref_bwt\":         \"string\",\n             \"ref_dict\":        \"string\",\n             \"ref_ann\":         \"string\",\n             \"ref_fasta_index\": \"string\",\n             \"ref_alt\":         \"string\",\n             \"ref_fasta\":       \"string\",\n             \"ref_sa\":          \"string\",\n             \"ref_amb\":         \"string\"\n           }\n         },\n         \"scatter_settings\": {\n           \"haplotype_scatter_count\":     \"integer\",\n           \"break_bands_at_multiples_of\": \"integer\"\n         },\n         \"papi_settings\": {\n           \"agg_preemptible_tries\": \"integer\",\n           \"preemptible_tries\":     \"integer\"\n         },\n         \"fingerprint_genotypes_file\": \"string\",\n         \"fingerprint_genotypes_index\": \"string\"\n       }\n     },\n     \"items\": [{\n         \"options\": {}, // see workflow-options\n         \"inputs\": {\n             // required - specify either \"input_bam\" or \"input_cram\"\n             \"input_bam\":  \"string\",\n             \"input_cram\": \"string\",\n\n             // optional inputs\n             \"sample_name\":          \"string\",\n             \"final_gvcf_base_name\":  \"string\",\n             \"unmapped_bam_suffix\":    \"string\",\n             \"cram_ref_fasta\":       \"string\",\n             \"cram_ref_fasta_index\": \"string\",\n             \"bait_set_name\":        \"string\",\n             \"bait_interval_list\":   \"string\",\n             \"target_interval_list\": \"string\",\n             \"references\": {\n                 \"calling_interval_list\":      \"string\",\n                 \"contamination_sites_bed\":    \"string\",\n                 \"contamination_sites_mu\":     \"string\",\n                 \"contamination_sites_ud\":     \"string\",\n                 \"dbsnp_vcf\":                  \"string\",\n                 \"dbsnp_vcf_index\":            \"string\",\n                 \"evaluation_interval_list\":   \"string\",\n                 \"haplotype_database_file\":     \"string\",\n                 \"known_indels_sites_vcfs\":    [\"string\"],\n                 \"known_indels_sites_indices\": [\"string\"],\n                 \"reference_fasta\": {\n                     \"ref_pac\":         \"string\",\n                     \"ref_bwt\":         \"string\",\n                     \"ref_dict\":        \"string\",\n                     \"ref_ann\":         \"string\",\n                     \"ref_fasta_index\": \"string\",\n                     \"ref_alt\":         \"string\",\n                     \"ref_fasta\":       \"string\",\n                     \"ref_sa\":          \"string\",\n                     \"ref_amb\":         \"string\"\n                 }\n             },\n             \"scatter_settings\": {\n                 \"haplotype_scatter_count\":     \"integer\",\n                 \"break_bands_at_multiples_of\": \"integer\"\n             },\n             \"papi_settings\": {\n                 \"agg_preemptible_tries\": \"integer\",\n                 \"preemptible_tries\":     \"integer\"\n             },\n             \"fingerprint_genotypes_file\":  \"string\",\n             \"fingerprint_genotypes_index\": \"string\",\n             \"destination_cloud_path\":      \"string\"\n         }\n     }]\n}\n</code></pre>"},{"location":"modules-general/","title":"Modules Design Principles and Assumptions","text":"<p>WorkFlow Launcher is responsible for preparing the required workflow WDLs, inputs and options for Cromwell in a large scale. This work involves in inputs validation, pipeline WDL orchestration and Cromwell workflow management. Similar to other WFL modules, the <code>aou-arrays</code> module takes advantage of the <code>workload</code> concept in order to manage workflows efficiently.</p> <p></p> <p>In general, WFL classify all workloads into 2 categories: continuous and fixed. For instance, <code>aou-arrays</code> module implements arrays workload as a continuous workload, which means all samples are coming in like a continuous stream, and WFL does not make any assumption of how many samples will be in the workload or how to group the samples together: it hands off the workload creation and starting process to its caller. <code>wgs</code> module implements External Whole Genome workloads as a discrete workload that WFL has full knowledge about the number and properties of the samples it's going to process, and the samples can be grouped into batches (workloads) by a set of properties.</p> <p>To learn more about the details of each module, please check their own sections in this documentation.</p>"},{"location":"modules-general/#create-a-workload","title":"Create a workload","text":"<p>Defining a workload type usually requires these top-level parameters.</p> Parameter Type Required executor URL :fontawesome-regular-check-square: output URL prefix :fontawesome-regular-check-square: pipeline pipeline :fontawesome-regular-check-square: project text :fontawesome-regular-check-square: common object input URL prefix items object <p>The parameters are used this way.</p> <ul> <li>The <code>executor</code> URL specifies the Cromwell instance or  other execution engine to service the workload.</li> <li>The <code>output</code> URL prefix specifies the path you'd like WFL to dump the results to. It usually is a gs bucket.</li> <li>The <code>pipeline</code> enumeration implicitly identifies a data schema for the inputs to and outputs from the workload. You can think of it as the kind of workflow specified for the workload. People sometimes refer to this as the tag in that it is a well-known name for a Cromwell pipeline defined in WDL. You might also think of <code>pipeline</code> as the external or official name of a WFL processing module.</li> <li>The <code>project</code> is just some text to identify a researcher, billing entity, or cost object responsible for the workload.</li> <li>The <code>common</code> is something common for all of the samples, such as the workflow options. For more details, check the docs for the specific type of workload you are trying to submit.</li> <li>The <code>input</code> URL prefix specifies the path you'd like WFL to read (a batch of) sample(s) from. It usually is a gs bucket.</li> <li>The <code>items</code> is used to configure individual units of a workload. You can use it to tell WFL to treat arbitrary parts of the workload sepcially. For more details, check the docs for the specific type of workload you are trying to submit.</li> </ul>"},{"location":"modules-sg/","title":"GDCWholeGenomeSomaticSingleSample","text":""},{"location":"modules-sg/#inputs","title":"Inputs","text":"<p>In addition to the standard workload request inputs:</p> <ul> <li><code>executor</code> : URL of the Cromwell service</li> <li><code>output</code>   : GCS URL prefix for output files</li> <li><code>pipeline</code> : literally <code>\"GDCWholeGenomeSomaticSingleSample\"</code></li> <li><code>project</code>  : some tracking label you can choose</li> </ul> <p>a <code>GDCWholeGenomeSomaticSingleSample</code> workload requires the following inputs for each workflow.</p> <ul> <li><code>base_file_name</code></li> <li><code>contamination_vcf_index</code></li> <li><code>contamination_vcf</code></li> <li><code>cram_ref_fasta_index</code></li> <li><code>cram_ref_fasta</code></li> <li><code>dbsnp_vcf_index</code></li> <li><code>dbsnp_vcf</code></li> <li><code>input_cram</code></li> </ul> <p>Here is what those are.</p>"},{"location":"modules-sg/#base_file_name","title":"<code>base_file_name</code>","text":"<p>The leaf name of a sample input or output path without the <code>.</code> suffix. The <code>base_file_name</code> is usually the same as the sample name and differs in every workflow.</p>"},{"location":"modules-sg/#contamination_vcf_index-and-contamination_vcf","title":"<code>contamination_vcf_index</code> and <code>contamination_vcf</code>","text":"<p>These are GCS pathnames of the contamination detection data for the input samples. This commonly depends on the reference genome for the samples, and is shared across all the workflows.</p>"},{"location":"modules-sg/#cram_ref_fasta_index-and-cram_ref_fasta","title":"<code>cram_ref_fasta_index</code> and <code>cram_ref_fasta</code>","text":"<p>These are GCS pathnames of the reference FASTA to which the input CRAM is aligned. This FASTA is used to expand CRAMs to BAMs and again is generally shared across all the workflows.</p>"},{"location":"modules-sg/#dbsnp_vcf_index-and-dbsnp_vcf","title":"<code>dbsnp_vcf_index</code> and <code>dbsnp_vcf</code>","text":"<p>These are GCS pathnames of a VCF containing a database of known variants from the reference. As with the contamination and reference FASTA files, typically these are shared across all the workflows.</p>"},{"location":"modules-sg/#input_cram","title":"<code>input_cram</code>","text":"<p>This is a GCS pathname to the input CRAM. It's last component will typically be the <code>base_file_name</code> value with <code>\".cram\"</code> appended. The <code>GDCWholeGenomeSomaticSingleSample.wdl</code> workflow definition expects to find a <code>base_file_name.cram.crai</code> file for every <code>base_file_name.cram</code> file specified as an <code>input_cram</code>.</p>"},{"location":"modules-sg/#usage","title":"Usage","text":"<p>GDCWholeGenomeSomaticSingleSample workload supports the following API endpoints:</p> Verb Endpoint Description GET <code>/api/v1/workload</code> List all workloads, optionally filtering by uuid or project GET <code>/api/v1/workload/{uuid}/workflows</code> List all workflows for a specified workload uuid POST <code>/api/v1/create</code> Create a new workload POST <code>/api/v1/start</code> Start a workload POST <code>/api/v1/stop</code> Stop a running workload POST <code>/api/v1/exec</code> Create and start (execute) a workload Permissions in production <p>External Whole Genome Reprocessing in <code>gotc-prod</code> uses a set of execution projects, please refer to this page when you have questions about permissions.</p>"},{"location":"modules-sg/#create-workload-apiv1create","title":"Create Workload: <code>/api/v1/create</code>","text":"<p>Create a WFL workload running in production.</p> Request <pre><code>curl --location --request POST \\\nhttps://gotc-prod-wfl.gotc-prod.broadinstitute.org/api/v1/create \\\n--header \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n--header 'Content-Type: application/json' \\\n--data-raw '\n{\n  \"executor\": \"https://cromwell-gotc-auth.gotc-prod.broadinstitute.org\",\n  \"output\": \"gs://broad-prod-somatic-genomes-output\",\n  \"pipeline\": \"GDCWholeGenomeSomaticSingleSample\",\n  \"project\": \"PO-1234\",\n  \"items\": [\n    {\n      \"inputs\": {\n        \"base_file_name\": \"27B-6\",\n        \"contamination_vcf\": \"gs://gatk-best-practices/somatic-hg38/small_exac_common_3.hg38.vcf.gz\",\n        \"contamination_vcf_index\": \"gs://gatk-best-practices/somatic-hg38/small_exac_common_3.hg38.vcf.gz.tbi\",\n        \"cram_ref_fasta\": \"gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta\",\n        \"cram_ref_fasta_index\": \"gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai\",\n        \"dbsnp_vcf\": \"gs://gcp-public-data--broad-references/hg38/v0/gdc/dbsnp_144.hg38.vcf.gz\",\n        \"dbsnp_vcf_index\": \"gs://gcp-public-data--broad-references/hg38/v0/gdc/dbsnp_144.hg38.vcf.gz.tbi\",\n        \"input_cram\": \"gs://broad-gotc-prod-storage/pipeline/PO-1234/27B-6/v1/27B-6.cram\"\n      },\n      \"options\": {\n        \"monitoring_script\": \"gs://broad-gotc-prod-storage/scripts/monitoring_script.sh\"\n      }\n    }\n  ]\n}'\n</code></pre> Response <pre><code>{\n  \"commit\": \"477bb195c40cc5f5afb81ca1b57e97c9cc18fa2c\",\n  \"created\": \"2021-04-05T16:02:31Z\",\n  \"creator\": \"tbl@broadinstitute.org\",\n  \"executor\": \"https://cromwell-gotc-auth.gotc-prod.broadinstitute.org\",\n  \"output\": \"gs://broad-prod-somatic-genomes-output\",\n  \"pipeline\": \"GDCWholeGenomeSomaticSingleSample\",\n  \"project\": \"PO-1234\",\n  \"release\": \"GDCWholeGenomeSomaticSingleSample_v1.1.0\",\n  \"started\": \"2021-04-05T16:02:32Z\",\n  \"uuid\": \"efb00901-378e-4365-86e7-edd0fbdaaab2\",\n  \"version\": \"0.7.0\",\n  \"wdl\": \"pipelines/broad/dna_seq/somatic/single_sample/wgs/gdc_genome/GDCWholeGenomeSomaticSingleSample.wdl\"\n}\n</code></pre> <p>Note that the <code>GDCWholeGenomeSomaticSingleSample</code> pipeline supports Cromwell <code>workflowOptions</code> via the <code>options</code> map. See the reference page for more information.</p>"},{"location":"modules-sg/#start-workload-apiv1start","title":"Start Workload: <code>/api/v1/start</code>","text":"<p>Start all the workflows in the workload.</p> Request <pre><code>curl --location --request POST \\\nhttps://gotc-prod-wfl.gotc-prod.broadinstitute.org/api/v1/start \\\n--header \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\"uuid\": \"efb00901-378e-4365-86e7-edd0fbdaaab2\"}'\n</code></pre> Response <pre><code>{\n  \"commit\": \"477bb195c40cc5f5afb81ca1b57e97c9cc18fa2c\",\n  \"created\": \"2021-04-05T16:02:31Z\",\n  \"creator\": \"tbl@broadinstitute.org\",\n  \"executor\": \"https://cromwell-gotc-auth.gotc-prod.broadinstitute.org\",\n  \"output\": \"gs://broad-prod-somatic-genomes-output\",\n  \"pipeline\": \"GDCWholeGenomeSomaticSingleSample\",\n  \"project\": \"PO-1234\",\n  \"release\": \"GDCWholeGenomeSomaticSingleSample_v1.1.0\",\n  \"started\": \"2021-04-05T16:02:32Z\",\n  \"uuid\": \"efb00901-378e-4365-86e7-edd0fbdaaab2\",\n  \"version\": \"0.7.0\",\n  \"wdl\": \"pipelines/broad/dna_seq/somatic/single_sample/wgs/gdc_genome/GDCWholeGenomeSomaticSingleSample.wdl\"\n}\n</code></pre>"},{"location":"modules-sg/#start-workload-apiv1start_1","title":"Start Workload: <code>/api/v1/start</code>","text":"<p>Included for compatibility with continuous workloads.</p> Request <pre><code>curl -X POST 'https://gotc-prod-wfl.gotc-prod.broadinstitute.org/api/v1/stop' \\\n     -X \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n     -X 'Content-Type: application/json' \\\n     -d '{ \"uuid\": \"efb00901-378e-4365-86e7-edd0fbdaaab2\" }'\n</code></pre> Response <pre><code>{\n  \"commit\": \"477bb195c40cc5f5afb81ca1b57e97c9cc18fa2c\",\n  \"created\": \"2021-04-05T16:02:31Z\",\n  \"creator\": \"tbl@broadinstitute.org\",\n  \"executor\": \"https://cromwell-gotc-auth.gotc-prod.broadinstitute.org\",\n  \"output\": \"gs://broad-prod-somatic-genomes-output\",\n  \"pipeline\": \"GDCWholeGenomeSomaticSingleSample\",\n  \"project\": \"PO-1234\",\n  \"release\": \"GDCWholeGenomeSomaticSingleSample_v1.1.0\",\n  \"started\": \"2021-04-05T16:02:32Z\",\n  \"stopped\": \"2021-04-05T16:02:33Z\",\n  \"uuid\": \"efb00901-378e-4365-86e7-edd0fbdaaab2\",\n  \"version\": \"0.7.0\",\n  \"wdl\": \"pipelines/broad/dna_seq/somatic/single_sample/wgs/gdc_genome/GDCWholeGenomeSomaticSingleSample.wdl\"\n}\n</code></pre>"},{"location":"modules-sg/#exec-workload-apiv1exec","title":"Exec Workload: <code>/api/v1/exec</code>","text":"<p>Create a workload, then start every workflow in the workload.</p> <p>Except for the different WFL URI, the request and response are the same as for Create Workload above.</p> <pre><code>curl --location --request POST \\\nhttps://gotc-prod-wfl.gotc-prod.broadinstitute.org/api/v1/exec \\\n... and so on ...\n</code></pre>"},{"location":"modules-sg/#query-workload-apiv1workloaduuiduuid","title":"Query Workload: <code>/api/v1/workload?uuid=&lt;uuid&gt;</code>","text":"<p>Query WFL for a workload by its UUID.</p> Request <pre><code>curl --location --request GET \\\nhttps://gotc-prod-wfl.gotc-prod.broadinstitute.org/api/v1/workload?uuid=efb00901-378e-4365-86e7-edd0fbdaaab2 \\\n--header 'Authorization: Bearer '$(gcloud auth print-access-token)\n</code></pre> Response <p>A successful response from <code>/api/v1/workload</code> is always an array of workload objects, but specifying a UUID returns only one.</p> <pre><code>[\n {\n   \"commit\": \"477bb195c40cc5f5afb81ca1b57e97c9cc18fa2c\",\n   \"created\": \"2021-04-05T16:02:31Z\",\n   \"creator\": \"tbl@broadinstitute.org\",\n   \"executor\": \"https://cromwell-gotc-auth.gotc-prod.broadinstitute.org\",\n   \"output\": \"gs://broad-prod-somatic-genomes-output\",\n   \"pipeline\": \"GDCWholeGenomeSomaticSingleSample\",\n   \"project\": \"PO-1234\",\n   \"release\": \"GDCWholeGenomeSomaticSingleSample_v1.1.0\",\n   \"started\": \"2021-04-05T16:02:32Z\",\n   \"uuid\": \"efb00901-378e-4365-86e7-edd0fbdaaab2\",\n   \"version\": \"0.7.0\",\n   \"wdl\": \"pipelines/broad/dna_seq/somatic/single_sample/wgs/gdc_genome/GDCWholeGenomeSomaticSingleSample.wdl\"\n }\n]\n</code></pre>"},{"location":"modules-sg/#query-workload-with-project-apiv1workloadprojectproject","title":"Query Workload with project: <code>/api/v1/workload?project=&lt;project&gt;</code>","text":"<p>Query WFL for all workloads with a specified <code>project</code> label.</p> <pre><code>curl --location --request GET \\\n/api/v1/workload?project=wgs-dev \\\nhttps://gotc-prod-wfl.gotc-prod.broadinstitute.org/api/v1/workload?project=PO-1234 \\\n--header 'Authorization: Bearer '$(gcloud auth print-access-token)\n</code></pre> <p>The response is the same as when specifying a UUID, except the array may contain multiple workload objects that share the same <code>project</code> value.</p> <p>Note</p> <p>A request to the <code>/api/v1/workload</code> endpoint without a <code>project</code> or <code>uuid</code> parameter returns all of the workloads that WFL knows about. That response might be large and take a while to process.</p>"},{"location":"modules-sg/#list-workflows-managed-by-the-workload-get-apiv1workloaduuidworkflows","title":"List workflows managed by the workload <code>GET /api/v1/workload/{uuid}/workflows</code>","text":"Request <pre><code>curl -X GET '/api/v1/workload/efb00901-378e-4365-86e7-edd0fbdaaab2/workflows' \\\n     -H 'Authorization: Bearer '$(gcloud auth print-access-token)\n</code></pre> Response <p>A successful response from <code>/api/v1/workload/{uuid}/workload</code> is always an array of Cromwell workflows with their statuses.</p> <pre><code>[{\n      \"status\": \"Submitted\",\n      \"updated\": \"2021-04-05T16:02:32Z\",\n      \"uuid\": \"8c1f586e-036b-4690-87c2-2af5d7e00450\",\n      \"inputs\": {\n          \"base_file_name\": \"27B-6\",\n          \"contamination_vcf\": \"gs://gatk-best-practices/somatic-hg38/small_exac_common_3.hg38.vcf.gz\",\n          \"contamination_vcf_index\": \"gs://gatk-best-practices/somatic-hg38/small_exac_common_3.hg38.vcf.gz.tbi\",\n          \"cram_ref_fasta\": \"gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta\",\n          \"cram_ref_fasta_index\": \"gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai\",\n          \"dbsnp_vcf\": \"gs://gcp-public-data--broad-references/hg38/v0/gdc/dbsnp_144.hg38.vcf.gz\",\n          \"dbsnp_vcf_index\": \"gs://gcp-public-data--broad-references/hg38/v0/gdc/dbsnp_144.hg38.vcf.gz.tbi\",\n          \"input_cram\": \"gs://broad-gotc-prod-storage/pipeline/PO-1234/27B-6/v1/27B-6.cram\"\n      },\n      \"options\": {\n          \"monitoring_script\": \"gs://broad-gotc-prod-storage/scripts/monitoring_script.sh\"\n      }\n}]\n</code></pre>"},{"location":"modules-wgs/","title":"ExternalWholeGenomeReprocessing workload","text":""},{"location":"modules-wgs/#inputs","title":"Inputs","text":"<p>An <code>ExternalWholeGenomeReprocessing</code> workload specifies the following inputs for each workflow:</p> <ul> <li><code>input_cram</code> or <code>input_bam</code> (required)</li> <li><code>base_file_name</code></li> <li><code>sample_name</code></li> <li><code>final_gvcf_base_name</code></li> <li><code>unmapped_bam_suffix</code></li> <li><code>reference_fasta_prefix</code></li> </ul>"},{"location":"modules-wgs/#input_cram-or-input_bam-required","title":"<code>input_cram</code> or <code>input_bam</code> (required)","text":"<ul> <li>Absolute GCS file path (like <code>gs://...</code>)</li> </ul>"},{"location":"modules-wgs/#base_file_name","title":"<code>base_file_name</code>","text":"<ul> <li>Used for naming intermediate/output files</li> <li>Defaults to the filename of the <code>input_cram</code> or <code>input_bam</code> without the <code>.cram</code> or <code>.bam</code> extension</li> </ul>"},{"location":"modules-wgs/#sample_name","title":"<code>sample_name</code>","text":"<ul> <li>Defaults to the filename of the <code>input_cram</code> or <code>input_bam</code> without the <code>.cram</code> or <code>.bam</code> extension</li> </ul>"},{"location":"modules-wgs/#final_gvcf_base_name","title":"<code>final_gvcf_base_name</code>","text":"<ul> <li>Path to the final VCF (<code>.vcf</code> will be added by the WDL)</li> <li>Defaults to the filename of the <code>input_cram</code> or <code>input_bam</code> without the <code>.cram</code> or <code>.bam</code> extension</li> </ul>"},{"location":"modules-wgs/#unmapped_bam_suffix","title":"<code>unmapped_bam_suffix</code>","text":"<ul> <li>Defaults to <code>.unmapped.bam</code></li> </ul>"},{"location":"modules-wgs/#reference_fasta_prefix","title":"<code>reference_fasta_prefix</code>","text":"<ul> <li>Defaults to <code>gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38</code></li> </ul> <p>Note that this pipeline supports specifying arbitrary WDL inputs, either at the workload level through <code>common</code> or individually via <code>items</code>.</p>"},{"location":"modules-wgs/#usage","title":"Usage","text":"<p>ExternalWholeGenomeReprocessing workload supports the following API endpoints:</p> Verb Endpoint Description GET <code>/api/v1/workload</code> List all workloads, optionally filtering by uuid or project GET <code>/api/v1/workload/{uuid}/workflows</code> List all workflows for a specified workload uuid POST <code>/api/v1/create</code> Create a new workload POST <code>/api/v1/start</code> Start a workload POST <code>/api/v1/stop</code> Stop a running workload POST <code>/api/v1/exec</code> Create and start (execute) a workload Permissions in production <p>External Whole Genome Reprocessing in <code>gotc-prod</code> uses a set of execution projects, please refer to this page when you have questions about permissions.</p>"},{"location":"modules-wgs/#create-workload-post-apiv1create","title":"Create Workload: <code>POST /api/v1/create</code>","text":"<p>Creates a WFL workload. Before processing, confirm that the WFL and Cromwell service accounts have at least read access to the input files.</p> Request <pre><code>curl -X POST 'https://dev-wfl.gotc-dev.broadinstitute.org/api/v1/create' \\\n     -H 'Authorization: Bearer '$(gcloud auth print-access-token) \\\n     -H 'Content-Type: application/json' \\\n     -d '{\n           \"executor\": \"https://cromwell-gotc-auth.gotc-dev.broadinstitute.org\",\n           \"output\": \"gs://broad-gotc-dev-wfl-ptc-test-outputs/wgs-test-output/\",\n           \"pipeline\": \"ExternalWholeGenomeReprocessing\",\n           \"project\": \"PO-1234\",\n           \"items\": [{\n             \"inputs\": {\n               \"input_cram\": \"gs://broad-gotc-dev-wfl-ptc-test-inputs/single_sample/plumbing/truth/develop/20k/NA12878_PLUMBING.cram\",\n               \"sample_name\": \"TestSample1234\"\n             }\n           }]\n         }'\n</code></pre> Response <pre><code>    {\n      \"creator\": \"sehsan@broadinstitute.org\",\n      \"pipeline\": \"ExternalWholeGenomeReprocessing\",\n      \"executor\": \"https://cromwell-gotc-auth.gotc-dev.broadinstitute.org\",\n      \"release\": \"ExternalWholeGenomeReprocessing_v1.0\",\n      \"created\": \"2020-10-05T15:50:01Z\",\n      \"output\": \"gs://broad-gotc-dev-wfl-ptc-test-outputs/wgs-test-output/\",\n      \"project\": \"PO-1234\",\n      \"commit\": \"d65371ca983b4f0d4fa06868e2946a8e3cab291b\",\n      \"wdl\": \"pipelines/reprocessing/external/wgs/ExternalWholeGenomeReprocessing.wdl\",\n      \"input\": \"gs://broad-gotc-dev-wfl-ptc-test-inputs/single_sample/plumbing/truth\",\n      \"uuid\": \"74d96a04-fea7-4270-a02b-a319dae2dd5e\",\n      \"version\": \"0.7.0\"\n    }\n</code></pre> <p>Note that the ExternalWholeGenomeReprocessing pipeline supports specifying cromwell \"workflowOptions\" via the <code>options</code> map. See the reference page for more information.</p>"},{"location":"modules-wgs/#start-workload-post-apiv1start","title":"Start Workload: <code>POST /api/v1/start</code>","text":"<p>Starts a Cromwell workflow for each item in the workload. If an output already exists in the output bucket for a particular input cram, WFL will not re-submit that workflow.</p> Request <pre><code>curl -X POST 'https://dev-wfl.gotc-dev.broadinstitute.org/api/v1/start' \\\n      -H 'Authorization: Bearer '$(gcloud auth print-access-token) \\\n      -H 'Content-Type: application/json' \\\n      -d '{ \"uuid\": \"74d96a04-fea7-4270-a02b-a319dae2dd5e\" }'\n</code></pre> Response <pre><code>    {\n      \"started\": \"2020-10-05T15:50:51Z\",\n      \"creator\": \"username@broadinstitute.org\",\n      \"pipeline\": \"ExternalWholeGenomeReprocessing\",\n      \"executor\": \"https://cromwell-gotc-auth.gotc-dev.broadinstitute.org\",\n      \"release\": \"ExternalWholeGenomeReprocessing_v1.0\",\n      \"created\": \"2020-10-05T15:50:01Z\",\n      \"output\": \"gs://broad-gotc-dev-wfl-ptc-test-outputs/wgs-test-output/\",\n      \"project\": \"PO-1234\",\n      \"commit\": \"d65371ca983b4f0d4fa06868e2946a8e3cab291b\",\n      \"wdl\": \"pipelines/reprocessing/external/wgs/ExternalWholeGenomeReprocessing.wdl\",\n      \"input\": \"gs://broad-gotc-dev-wfl-ptc-test-inputs/single_sample/plumbing/truth\",\n      \"uuid\": \"74d96a04-fea7-4270-a02b-a319dae2dd5e\",\n      \"version\": \"0.7.0\"\n    }\n</code></pre>"},{"location":"modules-wgs/#stop-workload-post-apiv1stop","title":"Stop Workload: <code>POST /api/v1/stop</code>","text":"<p>Included for compatibility with continuous workloads.</p> Request <pre><code>curl -X POST 'https://dev-wfl.gotc-dev.broadinstitute.org/api/v1/start' \\\n     -H 'Authorization: Bearer '$(gcloud auth print-access-token) \\\n     -H 'Content-Type: application/json' \\\n     -d '{ \"uuid\": \"74d96a04-fea7-4270-a02b-a319dae2dd5e\" }'\n</code></pre> Response <p><code>json     {       \"started\": \"2020-10-05T15:50:51Z\",       \"creator\": \"username@broadinstitute.org\",       \"pipeline\": \"ExternalWholeGenomeReprocessing\",       \"executor\": \"https://cromwell-gotc-auth.gotc-dev.broadinstitute.org\",       \"release\": \"ExternalWholeGenomeReprocessing_v1.0\",       \"created\": \"2020-10-05T15:50:01Z\",       \"output\": \"gs://broad-gotc-dev-wfl-ptc-test-outputs/wgs-test-output/\",       \"project\": \"PO-1234\",       \"commit\": \"d65371ca983b4f0d4fa06868e2946a8e3cab291b\",       \"wdl\": \"pipelines/reprocessing/external/wgs/ExternalWholeGenomeReprocessing.wdl\",       \"input\": \"gs://broad-gotc-dev-wfl-ptc-test-inputs/single_sample/plumbing/truth\",       \"uuid\": \"74d96a04-fea7-4270-a02b-a319dae2dd5e\",       \"version\": \"0.7.0\"     }</code></p>"},{"location":"modules-wgs/#exec-workload-post-apiv1exec","title":"Exec Workload: <code>POST /api/v1/exec</code>","text":"<p>Creates and then starts a Cromwell workflow for each item in the workload.</p> Request <pre><code>curl -X POST 'https://dev-wfl.gotc-dev.broadinstitute.org/api/v1/exec' \\\n     -H 'Authorization: Bearer '$(gcloud auth print-access-token) \\\n     -H 'Content-Type: application/json' \\\n     -d '{\n           \"executor\": \"https://cromwell-gotc-auth.gotc-dev.broadinstitute.org\",\n           \"output\": \"gs://broad-gotc-dev-wfl-ptc-test-outputs/wgs-test-output/\",\n           \"pipeline\": \"ExternalWholeGenomeReprocessing\",\n           \"project\": \"PO-1234\",\n           \"items\": [{\n             \"inputs\": {\n               \"input_cram\": \"gs://broad-gotc-dev-wfl-ptc-test-inputs/single_sample/plumbing/truth/develop/20k/NA12878_PLUMBING.cram\",\n               \"sample_name\": \"TestSample1234\"\n             }\n           }]\n         }'\n</code></pre> Response <pre><code>    {\n      \"started\": \"2020-10-05T16:15:32Z\",\n      \"creator\": \"username@broadinstitute.org\",\n      \"pipeline\": \"ExternalWholeGenomeReprocessing\",\n      \"executor\": \"https://cromwell-gotc-auth.gotc-dev.broadinstitute.org\",\n      \"release\": \"ExternalWholeGenomeReprocessing_v1.0\",\n      \"created\": \"2020-10-05T16:15:32Z\",\n      \"output\": \"gs://broad-gotc-dev-wfl-ptc-test-outputs/wgs-test-output/\",\n      \"project\": \"PO-1234\",\n      \"commit\": \"d65371ca983b4f0d4fa06868e2946a8e3cab291b\",\n      \"wdl\": \"pipelines/reprocessing/external/wgs/ExternalWholeGenomeReprocessing.wdl\",\n      \"input\": \"gs://broad-gotc-dev-wfl-ptc-test-inputs/single_sample/plumbing/truth\",\n      \"uuid\": \"3a13f732-9743-47a9-ab83-c467b3bf0ca4\",\n      \"version\": \"0.7.0\"\n    }\n</code></pre>"},{"location":"modules-wgs/#query-workload-get-apiv1workloaduuiduuid","title":"Query Workload: <code>GET /api/v1/workload?uuid=&lt;uuid&gt;</code>","text":"<p>Queries the WFL database for workloads. Specify the uuid to query for a specific workload.</p> Request <pre><code>curl -X GET 'https://dev-wfl.gotc-dev.broadinstitute.org/api/v1/workload?uuid=813e3c38-9c11-4410-9888-435569d91d1d' \\\n     -H 'Authorization: Bearer '$(gcloud auth print-access-token)\n</code></pre> Response <pre><code>    [{\n      \"creator\": \"username\",\n      \"pipeline\": \"ExternalWholeGenomeReprocessing\",\n      \"executor\": \"https://cromwell-gotc-auth.gotc-dev.broadinstitute.org/\",\n      \"release\": \"ExternalWholeGenomeReprocessing_v1.0\",\n      \"created\": \"2020-08-27T16:26:59Z\",\n      \"output\": \"gs://broad-gotc-dev-zero-test/wgs-test-output\",\n      \"workflows\": [\n        {\n          \"updated\": \"2020-10-05T16:15:32Z\",\n          \"uuid\": \"2c543b29-2db9-4643-b81b-b16a0654c5cc\",\n          \"inputs\": {\n            \"input_cram\": \"gs://broad-gotc-dev-wfl-ptc-test-inputs/single_sample/plumbing/truth/develop/20k/NA12878_PLUMBING.cram\",\n            \"sample_name\": \"TestSample1234\"\n          }\n        }\n      ],\n      \"project\": \"wgs-dev\",\n      \"commit\": \"d2fc38c61c62c44f4fd4d24bdee3121138e6c09e\",\n      \"wdl\": \"pipelines/reprocessing/external/wgs/ExternalWholeGenomeReprocessing.wdl\",\n      \"input\": \"gs://broad-gotc-test-storage/single_sample/plumbing/truth\",\n      \"uuid\": \"813e3c38-9c11-4410-9888-435569d91d1d\",\n      \"version\": \"0.7.0\"\n    }]\n</code></pre>"},{"location":"modules-wgs/#query-workload-with-project-get-apiv1workloadprojectproject","title":"Query Workload with project: <code>GET /api/v1/workload?project=&lt;project&gt;</code>","text":"<p>Queries the WFL database for workloads. Specify the project name to query for a list of specific workload(s).</p> Request <pre><code>curl -X GET 'https://dev-wfl.gotc-dev.broadinstitute.org/api/v1/workload?project=wgs-dev' \\\n     -H 'Authorization: Bearer '$(gcloud auth print-access-token)\n</code></pre> Response <pre><code>    [{\n      \"creator\": \"username\",\n      \"pipeline\": \"ExternalWholeGenomeReprocessing\",\n      \"executor\": \"https://cromwell-gotc-auth.gotc-dev.broadinstitute.org/\",\n      \"release\": \"ExternalWholeGenomeReprocessing_v1.0\",\n      \"created\": \"2020-08-27T16:26:59Z\",\n      \"output\": \"gs://broad-gotc-dev-zero-test/wgs-test-output\",\n      \"project\": \"wgs-dev\",\n      \"commit\": \"d2fc38c61c62c44f4fd4d24bdee3121138e6c09e\",\n      \"wdl\": \"pipelines/reprocessing/external/wgs/ExternalWholeGenomeReprocessing.wdl\",\n      \"input\": \"gs://broad-gotc-test-storage/single_sample/plumbing/truth\",\n      \"uuid\": \"813e3c38-9c11-4410-9888-435569d91d1d\",\n      \"version\": \"0.7.0\"\n    }]\n</code></pre> <p>The \"workflows\" field lists out each Cromwell workflow that was started, and includes their status information. It is also possible to use the Job Manager to check workflow progress and easily see information about any workflow failures.</p> <p>Note</p> <p><code>project</code> and <code>uuid</code> are optional path parameters to the <code>/api/v1/workload</code> endpoint, hitting this endpoint without them will return all workloads. However, they cannot be specified together.</p>"},{"location":"modules-wgs/#list-workflows-managed-by-a-workload-get-apiv1workloaduuidworkflows","title":"List workflows managed by a workload <code>GET /api/v1/workload/{uuid}/workflows</code>","text":"Request <pre><code>curl -X GET 'https://dev-wfl.gotc-dev.broadinstitute.org/api/v1/workload/813e3c38-9c11-4410-9888-435569d91d1d/workflows' \\\n     -H 'Authorization: Bearer '$(gcloud auth print-access-token)\n</code></pre> Response <pre><code>[{\n    \"updated\": \"2020-10-05T16:15:32Z\",\n    \"uuid\": \"2c543b29-2db9-4643-b81b-b16a0654c5cc\",\n    \"inputs\": {\n      \"input_cram\": \"gs://broad-gotc-dev-wfl-ptc-test-inputs/single_sample/plumbing/truth/develop/20k/NA12878_PLUMBING.cram\",\n      \"sample_name\": \"TestSample1234\"\n    }\n}]\n</code></pre> <p>The \"workflows\" endpoint lists out each Cromwell workflow that was started, and includes their status information. It is also possible to use the Job Manager to check workflow progress and easily see information about any workflow failures.</p>"},{"location":"staged-api-usage/","title":"API Usage","text":""},{"location":"staged-api-usage/#staged-workload-api-overview","title":"Staged Workload API Overview","text":"<p>Users interact with staged workloads via the following WFL API endpoints:</p> Verb Endpoint Description GET <code>/api/v1/workload</code> List all workloads, optionally filtering by uuid or project GET <code>/api/v1/workload/{uuid}/workflows</code> List all unretried workflows for workload <code>uuid</code>, filtering by supplied filters POST <code>/api/v1/workload/{uuid}/retry</code> Retry unretried workflows matching given filters in workload <code>uuid</code> POST <code>/api/v1/create</code> Create a new workload POST <code>/api/v1/start</code> Start a workload POST <code>/api/v1/stop</code> Stop a running workload POST <code>/api/v1/exec</code> Create and start (execute) a workload <p>These endpoints play into a staged workload's lifecycle thusly:</p> <ol> <li> <p>The caller needs to create a workload and specify the    source,    executor, and    sink.</p> <p>If all stage requests pass verification, the caller will receive the newly created workload object with an assigned <code>uuid</code> in response.</p> </li> <li> <p>Next, the caller needs to start the newly created workload.    Once started, WFL will continue to poll for new inputs in the source until stopped.</p> <p>(Alternatively: callers can execute a workload request to create and start a workload within a single call, sidestepping the possibility of creating a workload but forgetting to start it.)</p> </li> <li> <p>Stopping a workload will not cancel analysis in progress,    but WFL will stop polling for new inputs,    and will mark the workload finished    once any previously-identified inputs have undergone processing.</p> <ul> <li>Example: the caller may wish to stop a continuous workflow   if maintenance is required on the underlying method.</li> </ul> </li> </ol> <p>The caller can also retry workflows in a workload matching a Terra submission ID and optional workflow status (ex. \"Failed\").</p>"},{"location":"staged-api-usage/#api-usage-examples","title":"API Usage Examples","text":"<p>Here you'll find example requests and responses for the endpoints enumerated above.</p>"},{"location":"staged-api-usage/#workload-response-format","title":"Workload Response Format","text":"<p>Many of the API endpoints return staged workloads in their responses.</p> <p>An example workload response at the time of this writing is formatted thusly:</p> <pre><code>{\n    \"started\" : \"2021-07-14T15:36:47Z\",\n    \"watchers\" : [\n        [\"slack\", \"C000XXX0XXX\"]\n    ],\n    \"labels\" : [ \"hornet:test\", \"project:okotsopo testing enhanced source, executor, sink logging\" ],\n    \"creator\" : \"okotsopo@broadinstitute.org\",\n    \"updated\" : \"2021-08-06T21:41:28Z\",\n    \"created\" : \"2021-07-14T15:36:07Z\",\n    \"source\" : {\n    \"snapshots\" : [ \"67a2bfd7-88e4-4adf-9e41-9b0d04fb32ea\" ],\n    \"name\" : \"TDR Snapshots\"\n    },\n    \"finished\" : \"2021-08-06T21:41:28Z\",\n    \"commit\" : \"9719eda7424bf5b0804f5493875681fa014cdb29\",\n    \"uuid\" : \"e66c86b2-120d-4f7f-9c3a-b9eaadeb1919\",\n    \"executor\" : {\n    \"workspace\" : \"wfl-dev/CDC_Viral_Sequencing_okotsopo_20210707\",\n    \"methodConfiguration\" : \"wfl-dev/sarscov2_illumina_full\",\n    \"fromSource\" : \"importSnapshot\",\n    \"name\" : \"Terra\"\n    },\n    \"version\" : \"0.8.0\",\n    \"sink\" : {\n    \"workspace\" : \"wfl-dev/CDC_Viral_Sequencing_okotsopo_20210707\",\n    \"entityType\" : \"flowcell\",\n    \"fromOutputs\" : {\n    \"submission_xml\" : \"submission_xml\",\n    \"assembled_ids\" : \"assembled_ids\",\n    ...\n    },\n    \"identifier\" : \"run_id\",\n    \"name\" : \"Terra Workspace\"\n    }\n    }\n</code></pre> <p>Worth mentioning is that the contents of the <code>source</code>, <code>executor</code> and <code>sink</code> blocks within the response will be formatted according to the corresponding stage implementation.</p>"},{"location":"staged-api-usage/#get-workloads","title":"Get Workloads","text":"<p>Note</p> <p>A request to the <code>/api/v1/workload</code> endpoint without a <code>uuid</code> or <code>project</code> parameter returns all workloads known to WFL. That response might be large and take awhile to process.</p> <p>GET /api/v1/workload?uuid={uuid}</p> <p>Query WFL for a workload by its UUID.</p> <p>Note that a successful response from <code>/api/v1/workload</code> will always be an array of workload objects, but specifying a <code>uuid</code> will return a singleton array.</p> Request <pre><code>curl -X GET 'http://localhost:3000/api/v1/workload' \\\n    -H 'Authorization: Bearer '$(gcloud auth print-access-token) \\\n    -H 'Accept: application/json' \\\n    -d $'{ \"uuid\": \"e66c86b2-120d-4f7f-9c3a-b9eaadeb1919\" }'\n</code></pre> <p>GET /api/v1/workload?project={project}</p> <p>Query WFL for all workloads with a specified <code>project</code> label.</p> <p>The response is an array of workload objects.</p> Request <pre><code>curl -X GET 'http://localhost:3000/api/v1/workload' \\\n    -H 'Authorization: Bearer '$(gcloud auth print-access-token) \\\n    -H 'Accept: application/json' \\\n    -d $'{ \"project\": \"PO-1234\" }'\n</code></pre>"},{"location":"staged-api-usage/#get-workflows","title":"Get Workflows","text":"<p>GET /api/v1/workload/{uuid}/workflows</p> <p>Query WFL for all unretried workflows associated with workload <code>uuid</code>.</p> <p>Note</p> <p>Fetching workflows in a workload without any other filters may yield a large response and take a long time to process, such as for long-running continuous workloads.</p> Request <pre><code>WFL=http://localhost:3000\nUUID=e8bc2c14-2396-469e-80fe-ebfed8d60a22  # workload UUID\n\ncurl -X GET \\\n     \"$WFL/api/v1/workload/$UUID/workflows\" \\\n     -H 'Authorization: Bearer '$(gcloud auth print-access-token) \\\n     -H 'Accept: application/json'\n</code></pre> <p>The response is a list of WFL workflow records, each of which tell us:</p> <ul> <li> <p>WFL submitted <code>methodConfiguration</code> in <code>workspace</code> with the   snapshot <code>reference</code> as its root entity type</p> </li> <li> <p>The resulting submission had Terra <code>submission</code> UUID</p> </li> <li> <p>The workflow had Terra <code>workflow</code> UUID and is processing the snapshot row   with <code>entity</code> as its row UUID</p> </li> <li> <p>At WFL's last poll at <code>updated</code>, we noted that the workflow had <code>status</code></p> </li> <li> <p>Once a workflow has succeeded, <code>consumed</code> is populated with a timestamp   when WFL has \"sunk\" its outputs to their destination</p> </li> </ul> Response <pre><code>[ {\n\"retry\": null,\n\"workspace\": \"emerge_prod/Arrays_test_AD\",\n\"updated\": \"2022-01-03T21:35:40Z\",\n\"workflow\": \"99956c17-26af-4d50-8954-9de9ecc4f733\",\n\"reference\": \"4de4b53c-3904-43f9-a155-9f2e2460eb42\",\n\"status\": \"Aborted\",\n\"id\": 1,\n\"methodConfiguration\": \"emerge_prod/Arrays_TDR_GH-1560\",\n\"consumed\": null,\n\"submission\": \"fbb96180-f6a5-4895-a154-72d133e442db\",\n\"entity\": \"d4ce2b14-49d6-4209-95ab-424e2edf1741\"\n} ]\n</code></pre> <p>GET /api/v1/workload/{uuid}/workflows?submission={submission}&amp;status={status}</p> <p>Query WFL for all unretried workflows associated with workload <code>uuid</code>, filtering by any workflow filters specified as query params:</p> <ul> <li><code>submission</code> - Terra submission ID (must be a valid UUID)</li> <li><code>status</code> - Workflow status (must be a valid Cromwell workflow status)</li> </ul> <p>The response has the same format as when querying without filters.</p> Request <pre><code>WFL=http://localhost:3000\nUUID=e8bc2c14-2396-469e-80fe-ebfed8d60a22       # workload UUID\nSUBMISSION=fbb96180-f6a5-4895-a154-72d133e442db # Terra submission UUID\nSTATUS=Aborted                                  # Cromwell workflow status\n\ncurl -X GET \\\n     \"$WFL/api/v1/workload/$UUID/workflows?submission=$SUBMISSION&amp;status=$STATUS\" \\\n     -H 'Authorization: Bearer '$(gcloud auth print-access-token) \\\n     -H 'Accept: application/json'\n</code></pre>"},{"location":"staged-api-usage/#retry-workload","title":"Retry Workload","text":"<p>POST /api/v1/workload/{uuid}/retry</p> <p>Resubmit all unretried workflows associated with workload <code>uuid</code> and request body filters.</p> <p>Prerequisites:</p> <ul> <li>The request body filters must be valid</li> <li>Workflows must exist in the workload for the specified filters</li> <li>The workload should be started</li> </ul> <p>With all prerequisite fulfilled, WFL will then...</p> <ul> <li>Submit the retry to the executor</li> <li>(If necessary) remark the workload as active   so that it will be visible in the update loop</li> </ul> <p>The response is the updated workload object.</p> <p>Further information found in general retry documentation.</p> Request <pre><code>WFL=http://localhost:3000\nUUID=e8bc2c14-2396-469e-80fe-ebfed8d60a22       # workload UUID\nSUBMISSION=fbb96180-f6a5-4895-a154-72d133e442db # Terra submission UUID\n\ncurl -X POST \\\n    \"$WFL/api/v1/workload/$UUID/retry\" \\\n    -H 'Authorization: Bearer '$(gcloud auth print-access-token) \\\n    -H \"Content-Type: application/json\" \\\n    -d \"{ \\\"submission\\\": \\\"$SUBMISSION\\\" }\"\n</code></pre>"},{"location":"staged-api-usage/#create-workload","title":"Create Workload","text":"<p>POST /api/v1/create</p> <p>Create a new workload from a request. Expected request format documented within staged workload navigation.</p> <p>The response is the newly created workload object with an assigned <code>uuid</code>.</p> Request <pre><code>curl -X \"POST\" \"http://localhost:3000/api/v1/create\" \\\n     -H 'Authorization: Bearer '$(gcloud auth print-access-token) \\\n     -H 'Content-Type: application/json' \\\n     -d '{\n            \"watchers\": [\n                [\"slack\", \"C000XXX0XXX\"],\n                [\"email\", \"tester@broadinstitute.org\"]\n            ],\n            \"labels\": [\n                \"hornet:test\"\n            ],\n            \"project\": \"wfl-dev/CDC_Viral_Sequencing _ranthony_bashing_copy\",\n            \"source\": {\n                \"name\": \"Terra DataRepo\",\n                \"dataset\": \"4bb51d98-b4aa-4c72-b76a-1a96a2ee3057\",\n                \"table\": \"flowcells\",\n                \"snapshotReaders\": [\n                    \"workflow-launcher-dev@firecloud.org\"\n                ]\n            },\n            \"executor\": {\n                \"name\": \"Terra\",\n                \"workspace\": \"wfl-dev/CDC_Viral_Sequencing _ranthony_bashing_copy\",\n                \"methodConfiguration\": \"wfl-dev/sarscov2_illumina_full\",\n                \"fromSource\": \"importSnapshot\"\n            },\n            \"sink\": {\n                \"name\": \"Terra Workspace\",\n                \"workspace\": \"wfl-dev/CDC_Viral_Sequencing _ranthony_bashing_copy\",\n                \"entityType\": \"flowcell\",\n                \"identifier\": \"run_id\",\n                \"fromOutputs\": {\n                    \"submission_xml\" : \"submission_xml\",\n                    \"assembled_ids\" : \"assembled_ids\",\n                    \"num_failed_assembly\" : \"num_failed_assembly\",\n                    ...\n                }\n            }\n        }'\n</code></pre>"},{"location":"staged-api-usage/#start-workload","title":"Start Workload","text":"<p>POST /api/v1/start?uuid={uuid}</p> <p>Start an existing, unstarted workload <code>uuid</code>.</p> <p>The response is the updated workload object.</p> Request <pre><code>curl -X POST 'http://localhost:3000/api/v1/start' \\\n     -H 'Authorization: Bearer '$(gcloud auth print-access-token) \\\n     -H 'Content-Type: application/json' \\\n     -d $'{ \"uuid\": \"fb06bcf3-bc10-471b-a309-b2f99e4f5a67\" }'\n</code></pre>"},{"location":"staged-api-usage/#stop-workload","title":"Stop Workload","text":"<p>POST /api/v1/stop?uuid={uuid}</p> <p>Stop a running workload <code>uuid</code>.</p> <p>The response is the updated workload object.</p> Request <pre><code>curl -X POST 'http://localhost:3000/api/v1/stop' \\\n     -H 'Authorization: Bearer '$(gcloud auth print-access-token) \\\n     -H 'Content-Type: application/json' \\\n     -d $'{ \"uuid\": \"fb06bcf3-bc10-471b-a309-b2f99e4f5a67\" }'\n</code></pre>"},{"location":"staged-api-usage/#execute-workload","title":"Execute Workload","text":"<p>POST /api/v1/exec</p> <p>Create and start (execute) a workload from a request. Expected request format documented within staged workload navigation.</p> <p>The response is the newly created and started workload object with an assigned <code>uuid</code>.</p> Request <pre><code>curl -X \"POST\" \"http://localhost:3000/api/v1/exec\" \\\n     -H 'Authorization: Bearer '$(gcloud auth print-access-token) \\\n     -H 'Content-Type: application/json' \\\n     -d '{\n            \"watchers\": [\n                [\"slack\", \"C000XXX0XXX\"]\n            ],\n            \"labels\": [\n                \"hornet:test\"\n            ],\n            \"project\": \"wfl-dev/CDC_Viral_Sequencing _ranthony_bashing_copy\",\n            \"source\": {\n                \"name\": \"Terra DataRepo\",\n                \"dataset\": \"4bb51d98-b4aa-4c72-b76a-1a96a2ee3057\",\n                \"table\": \"flowcells\",\n                \"snapshotReaders\": [\n                    \"workflow-launcher-dev@firecloud.org\"\n                ]\n            },\n            \"executor\": {\n                \"name\": \"Terra\",\n                \"workspace\": \"wfl-dev/CDC_Viral_Sequencing _ranthony_bashing_copy\",\n                \"methodConfiguration\": \"wfl-dev/sarscov2_illumina_full\",\n                \"fromSource\": \"importSnapshot\"\n            },\n            \"sink\": {\n                \"name\": \"Terra Workspace\",\n                \"workspace\": \"wfl-dev/CDC_Viral_Sequencing _ranthony_bashing_copy\",\n                \"entityType\": \"flowcell\",\n                \"identifier\": \"run_id\",\n                \"fromOutputs\": {\n                    \"submission_xml\" : \"submission_xml\",\n                    \"assembled_ids\" : \"assembled_ids\",\n                    \"num_failed_assembly\" : \"num_failed_assembly\",\n                    ...\n                }\n            }\n        }'\n</code></pre>"},{"location":"staged-executor/","title":"Executor","text":"<p>The workload <code>Executor</code> models an intermediate stage of a processing pipeline. An <code>Executor</code> uses a supported service in the cloud to execute workflows.</p>"},{"location":"staged-executor/#user-guide","title":"User Guide","text":"<p>You can configure the type of <code>Executor</code> used in your workload by changing the <code>executor</code> attribute of your workload request.</p>"},{"location":"staged-executor/#terra-executor","title":"<code>Terra</code> Executor","text":"<p>You can execute workflows in a Terra workspace using the <code>Terra</code> executor.</p> <p>The <code>Terra</code> executor will...</p> <ul> <li>Coerce available outputs from an upstream <code>Source</code> to a data type acceptable   for submission creation   (i.e. import a snapshot to the <code>executor</code> workspace as a reference)</li> <li>Update the method configuration with the coerced object as its root entity   type</li> <li>Launch a submission</li> <li>Periodically update the statuses of eligible workflows to enable a   downstream <code>Sink</code> to consume their outputs</li> </ul> <p>A typical <code>Terra</code> executor configuration in the workload request looks like: <pre><code>{\n  \"name\": \"Terra\",\n  \"workspace\": \"{workspace-namespace}/{workspace-name}\",\n  \"methodConfiguration\": \"{method-configuration-namespace}/{method-configuration-name}\",\n  \"fromSource\": \"importSnapshot\"\n}\n</code></pre></p> <p>And a real-life example for a known method configuration: <pre><code>{\n  \"name\": \"Terra\",\n  \"workspace\": \"wfl-dev/CDC_Viral_Sequencing\",\n  \"memoryRetryMultiplier\": 1.5\n  \"methodConfiguration\": \"wfl-dev/sarscov2_illumina_full\",\n  \"fromSource\": \"importSnapshot\"\n}\n</code></pre></p> <p></p> <p>The table below summarises the purpose of each attribute in the above request.</p> Attribute Description <code>name</code> Selects the <code>Terra</code> executor implementation. <code>workspace</code> Terra Workspace in which to execute workflows. <code>memoryRetryMultiplier</code> Retry OutOfMemory Cromwell tasks with memory increased by this factor. <code>methodConfiguration</code> Method configuration from which to generate submissions. <code>fromSource</code> Instruction to coerce an output from an upstream <code>Source</code> to a type understood by this <code>executor</code>."},{"location":"staged-executor/#workspace","title":"<code>workspace</code>","text":"<p>A <code>{workspace-namespace}/{workspace-name}</code> string as it appears in the URL path in the Terra UI.</p> <p>Prerequisites:</p> <ul> <li>The workspace must exist prior to workload creation.</li> <li><code>workflow-launcher@firecloud.org</code> must be a workspace \"Writer\" in order to   import snapshots to the workspace.</li> <li>The workspace must be compatible with any downstream processing stage that   consumes its workflows.</li> </ul>"},{"location":"staged-executor/#methodconfiguration","title":"<code>methodConfiguration</code>","text":"<p>A <code>{method-configuration-namespace}/{method-configuration-name}</code> string as it appears in the URL path in the Terra UI.</p> <p>Prerequisites:</p> <ul> <li>The method configuration must exist within <code>workspace</code> prior to   workload creation.</li> </ul>"},{"location":"staged-executor/#fromsource","title":"<code>fromSource</code>","text":"<p>This attribute tells workflow-launcher how to coerce an output from an upstream <code>Source</code> into a data type understood by the executor.</p> <p>Prerequisites:</p> <ul> <li>Must be one of the following supported coercion instructions.</li> </ul>"},{"location":"staged-executor/#importsnapshot","title":"<code>importSnapshot</code>","text":"<p>Workflow-launcher should import Terra Data Repository (TDR) snapshots into <code>workspace</code> as snapshot references, updating <code>methodConfiguration</code> with the reference as its root entity type.</p>"},{"location":"staged-executor/#developer-guide","title":"Developer Guide","text":"<p>An executor is a <code>Queue</code> that satisfies the <code>Executor</code> protocol below: <pre><code>(defprotocol Executor\n  (update-executor!\n    ^Workload\n    [^Workload workload\n    ]\n    \"Consume items from the `workload`'s source queue and enqueue to its executor\n     queue for consumption by a later processing stage,\n     performing any external effects as necessary.\n     Implementations should avoid maintaining in-memory state and making long-\n     running external calls, favouring internal queues to manage such tasks\n     asynchronously between invocations.  Note that the `Workload`'s Source queue and Executor\n     are parameterised types and the Source queue's parameterisation must be\n     convertible to the Executor's.\")\n  (executor-workflows\n    ^IPersistentVector\n    [^Connection        transaction  ;; JDBC Connection\n     ^Executor          executor     ;; This executor instance\n     ^IPersistentVector filters      ;; Optional workflow filters to match\n                                     ;; (ex. status, submission)\n    ]\n    \"Use database `transaction` to return workflows created by the `executor`\n     matching the optional workflow `filters` (ex. status, submission).\")\n  (executor-throw-if-invalid-retry-filters\n    ;; Executed for side effects\n    [^IPersistentHashmap workload  ;; Workload for which a retry is requested\n     ^IPersistentVector  filters   ;; Workflow filters\n    ]\n    \"Throw if workflow `filters` are invalid for `workload`'s retry request.\")\n  (executor-retry-workflows!\n    ;; Executed for side effects\n    [^Executor          executor   ;; This executor instance\n     ^IPersistentVector workflows  ;; Workflows to retry\n    ]\n    \"Retry/resubmit the `workflows` managed by the `executor`.\"))\n</code></pre></p> <p>Note</p> <p>The <code>Executor</code> protocol is implemented by a set of multimethods of the same name. The use of a protocol is to illustrate the difference between the in-memory data model of a <code>Executor</code> and the metadata seen by a user.</p> <p>To be used in a workload, an <code>Executor</code> implementation should satisfy the processing <code>Stage</code> protocol and the <code>to-edn</code> multimethod in addition to the following multimethods specific to executors:</p> <pre><code>(defmulti create-executor\n  \"Create an `Executor` instance using the database `transaction` and\n   configuration in the executor `request` and return a\n   `[type items]` pair to be written to a workload record as\n   `executor_type` and `executor_items`.\n   Notes:\n   - This is a factory method registered for workload creation.\n   - The `Executor` type string must match a value of the `executor` enum\n     in the database schema.\n   - This multimethod is type-dispatched on the `:name` association in the\n     `request`.\"\n  (fn ^[^String ^String]\n      [^Connection         transaction  ;; JDBC Connection\n       ^long               workload-id  ;; ID of the workload being created\n       ^IPersistentHashMap request      ;; Data forwarded to the handler\n      ]\n      (:name request)))\n\n(defmulti load-executor!\n  \"Return the `Executor` implementation associated with the `executor_type` and\n   `executor_items` fields of the `workload` row in the database.\n   Note that this multimethod is type-dispatched on the `:executor_type`\n   association in the `workload`.\"\n  (fn ^Executor\n      [^Connection         transaction  ;; JDBC Connection\n       ^IPersistentHashMap workload     ;; Row from workload table\n      ]\n      (:executor_type workload)))\n</code></pre>"},{"location":"staged-sink/","title":"Sink","text":"<p>The workload <code>Sink</code> models the terminal stage of a processing pipeline. A <code>Sink</code> writes workflow outputs to a desired location in the cloud.</p>"},{"location":"staged-sink/#user-guide","title":"User Guide","text":"<p>You can configure the type of <code>Sink</code> used in your workload by changing the <code>sink</code> attribute of your workload request.</p>"},{"location":"staged-sink/#terra-workspace-sink","title":"Terra Workspace Sink","text":"<p>You can write workflow outputs to a Terra Workspace using the <code>Terra Workspace</code> sink. A typical <code>Terra Workspace</code> sink configuration in the workload request looks like: <pre><code>{\n  \"name\": \"Terra Workspace\",\n  \"workspace\": \"{workspace-namespace}/{workspace-name}\",\n  \"entityType\": \"{entity-type-name}\",\n  \"identifier\": \"{workflow-identifier}\",\n  \"fromOutputs\": {\n    \"attribute0\": \"output0\",\n    \"attribute1\": [\"output1\", \"output2\"],\n    \"attribute3\": \"$literal\",\n    ...\n  }\n}\n</code></pre> The table below summarises the purpose of each attribute in the above request.</p> Attribute Description <code>name</code> Selects the <code>Terra Workspace</code> sink implementation. <code>workspace</code> The Terra Workspace to write pipeline outputs to. <code>entityType</code> The entity type in the <code>workspace</code> to write outputs to. <code>identifier</code> Selects the workflow attribute (output or input) to use as the entity name. <code>fromOutputs</code> Mapping from outputs to attribute names in the <code>entityType</code>."},{"location":"staged-sink/#workspace","title":"<code>workspace</code>","text":"<p>The workspace is a <code>\"{workspace-namespace}/{workspace-name}\"</code> string as it appears in the URL path in the Terra UI. The workspace must exist prior to workload creation. You must ensure that <code>workflow-launcher@firecloud.org</code> is a workspace \"Writer\" in order to write entities to the workspace.</p>"},{"location":"staged-sink/#entitytype","title":"<code>entityType</code>","text":"<p>The <code>entityType</code> is the name of the entity type in the workspace that entities will be created as. The entity type must exist prior to workload creation and must be a table in the workspace.</p>"},{"location":"staged-sink/#identifier","title":"<code>identifier</code>","text":"<p>WFL tries to find a workflow output whose name matches <code>identifier</code>, checking workflow input names as a fallback. The matching value will be the name of the newly created entity.</p> <p>Both workflow outputs and inputs are checked for matches since depending on use case, the logical unique identifier may be either.</p> <p>Warnings</p> <ul> <li>If an <code>identifier</code> has no matching workflow output or input, WFL will not be able to resolve a workflow to an entity name and will fail to write its outputs to the workspace data table.</li> <li>When two workflows share the same <code>identifier</code> value, the first set of outputs will be overwritten by the second in the workspace.</li> </ul> <p>Example:</p> <p>An eMerge Arrays workflow has an output called \"chip_well_barcode_output\" that uniquely identifies its inputs and outputs.</p> <p>By setting <code>\"identifier\": \"chip_well_barcode_output\"</code> in the sink configuration, entities will be created using the \"chip_well_barcode_output\" as the entity name.</p> <p>Below, the outputs for a successful workflow with a \"chip_well_barcode_output\" of \"204126290052_R01C01\" have been written to the destination data table.</p> <p></p>"},{"location":"staged-sink/#fromoutputs","title":"<code>fromOutputs</code>","text":"<p><code>fromOutputs</code> configures how to create new entities from pipeline outputs by mapping the output names to attributes in the <code>entityType</code>. Note that all attribute names must exist in the entityType before the workload  creation.</p> <p><code>fromOutputs</code> allows a small amount of flexibility in how to construct an entity and supports the following relations:</p> <ul> <li> <p><code>\"attribute\": \"output\"</code>   Direct mapping from an output to an attribute</p> </li> <li> <p><code>\"attribute\": [\"output0\", \"output2\"]</code>   Make an attribute from a list of pipeline outputs.</p> </li> <li> <p><code>\"attribute\": \"$value\"</code>   Use the literal \"value\" for an attribute.</p> </li> </ul>"},{"location":"staged-sink/#terra-data-repository-sink","title":"Terra Data Repository Sink","text":"<p>You can write workflow outputs to a Terra Data Repository dataset using the <code>Terra DataRepo</code> sink. A typical <code>Terra DataRepo</code> sink configuration in the workload request looks like: <pre><code>{\n  \"name\": \"Terra DataRepo\",\n  \"dataset\": \"{dataset-id}\",\n  \"table\": \"{table-name}\",\n  \"fromOutputs\": {\n    \"column0\": \"output0\",\n    \"column1\": [\"output1\", \"output2\"],\n    \"column3\": \"$literal\",\n    ...\n  }\n}\n</code></pre> The table below summarises the purpose of each attribute in the above request.</p> Attribute Description <code>name</code> Selects the <code>Terra Workspace</code> sink implementation. <code>dataset</code> The <code>UUID</code> of dataset to monitor and read from. <code>table</code> The name of the <code>dataset</code> table to monitor and read from. <code>fromOutputs</code> Mapping from outputs to columns in the <code>table</code>."},{"location":"staged-sink/#dataset","title":"<code>dataset</code>","text":"<p>The dataset attribute is the <code>UUID</code> that uniquely identifies the TDR dataset you want workflow-launcher to write workflow outputs to.</p>"},{"location":"staged-sink/#table","title":"<code>table</code>","text":"<p>The <code>table</code> is the name of the table in the dataset that you want workflow-launcher to write workflow outputs to. Once a workflow succeeds, its outputs will be ingested as new rows in that table (see note). You cannot write to more than one table per <code>Terra DataRepo</code> sink.</p> <p>Note</p> <p>workflow-launcher transforms outputs into a form conformant with the table in the dataset using the transformation described by <code>fromOutputs</code>. The columns in your table don't have to be an exact match for the output names. See below for more details.</p>"},{"location":"staged-sink/#fromoutputs_1","title":"<code>fromOutputs</code>","text":"<p><code>fromOutputs</code> configures how to create new rows in the <code>table</code> from pipeline outputs by mapping the output names to columns in the <code>table</code>.</p> <p><code>fromOutputs</code> allows a small amount of flexibility in how to construct an entity and supports the following relations:</p> <ul> <li> <p><code>\"column\": \"output\"</code>   Direct mapping from an output to a column</p> </li> <li> <p><code>\"column\": [\"output0\", \"output2\"]</code>   Make a column from an array of pipeline outputs.</p> </li> <li> <p><code>\"column\": \"$value\"</code>   Use the literal \"value\" for a column.</p> </li> </ul> <p>Note</p> <p>Any output not included in <code>fromOutputs</code> will not be ingested into the dataset</p> <p>Note</p> <p>Any column not included in <code>fromOuputs</code> will not have a value in the newly added row.</p>"},{"location":"staged-sink/#developer-guide","title":"Developer Guide","text":"<p>A sink is one satisfying the <code>Sink</code> protocol as below: <pre><code>(defprotocol Sink\n  (update-sink!\n    ^Workload\n    [^Workload workload  ;; This sink instance\n    ]\n    \"Update the internal state of the `Workload`'s sink, consuming objects from the\n     `Workload`'s executor queue, performing any external effects as required.\n     Implementations should avoid maintaining in-memory state and making long-\n     running external calls, favouring internal queues to manage such tasks\n     asynchronously between invocations. Note that the `Workload`'s Sink and its Executor Queue are\n     parameterised types and the Executor Queue's parameterisation must be convertible\n     to the Sink's.\"))\n</code></pre></p> <p>Note</p> <p>The <code>Sink</code> protocol is implemented by the <code>update-sink!</code> multimethod. It's documented thus as a means of differentiating the in-memory data model from the metadata a user sees.</p> <p>To be used in a workload, a <code>Sink</code> implementation should satisfy <code>Stage</code>, the <code>to-edn</code> multimethod and the following multimethods specific to <code>Sink</code>s:</p> <pre><code>(defmulti create-sink\n  \"Create a `Sink` instance using the database `transaction` and configuration\n   in the sink `request` and return a `[type items]` pair to be written to a\n   workload record as `sink_type` and `sink_items`.\n   Notes:\n   - This is a factory method registered for workload creation.\n   - The `Sink` type string must match a value of the `sink` enum in the\n     database schema.\n   - This multimethod is type-dispatched on the `:name` association in the\n     `request`.\"\n  (fn ^[^String ^String]\n      [^Connection         transaction  ;; JDBC Connection\n       ^long               workload-id  ;; ID of the workload being created\n       ^IPersistentHashMap request      ;; Data forwarded to the handler\n      ]\n      (:name request)))\n\n(defmulti load-sink!\n  \"Return the `Sink` implementation associated with the `sink_type` and\n   `sink_items` fields of the `workload` row in the database. Note that this\n   multimethod is type-dispatched on the `:sink_type` association in the\n   `workload`.\"\n  (fn ^Sink\n      [^Connection         transaction  ;; JDBC Connection\n       ^IPersistentHashMap workload     ;; Row from workload table\n      ]\n      (:sink_type workload)))\n</code></pre>"},{"location":"staged-source/","title":"Source","text":"<p>The workload <code>Source</code> models the first stage of a processing pipeline. A <code>Source</code> reads workflow inputs from a specified location or service in the cloud.</p>"},{"location":"staged-source/#user-guide","title":"User Guide","text":"<p>You can configure the type of <code>Source</code> used in your workload by changing the <code>source</code> attribute of your workload request.</p>"},{"location":"staged-source/#terra-datarepo-source","title":"<code>Terra DataRepo</code> Source","text":"<p>You can configure workflow-launcher to fetch workflow inputs from a Terra Data Repository (TDR) dataset in real-time using the <code>Terra DataRepo</code> source.</p> <p><code>Terra DataRepo</code> source polls a <code>dataset.table</code>'s row metadata table for newly ingested rows. It snapshots those rows for downstream processing by an <code>Executor</code>. The <code>Terra DataRepo</code> source can only read inputs from a single table.</p> <p>When you <code>start</code> the workload, the <code>Terra DataRepo</code> source will start looking for new rows from that instant.</p> <p>When you <code>stop</code> the workload, the <code>Terra DataRepo</code> source will stop looking for new rows from that instant. All pending snapshots may continue to be processed by a later workload stage.</p> <p>Note</p> <p>workflow-launcher creates snapshots of your data to be processed by a later stage of the workload. Therefore, you must ensure the account <code>workflow-launcher@firecloud.org</code> is a <code>custodian</code> of your dataset.</p> <p>A typical <code>Terra DataRepo</code> source configuration in the workload request looks like: <pre><code>{\n  \"name\": \"Terra DataRepo\",\n  \"dataset\": \"{dataset-id}\",\n  \"table\": \"{dataset-table-name}\",\n  \"snapshotReaders\": [\n    \"{user}@{domain}\",\n    ...\n  ],\n  \"pollingIntervalMinutes\": 1,\n  \"loadTag\": \"{load-tag-to-poll}\"\n}\n</code></pre> The table below summarises the purpose of each attribute in the above request.</p> Attribute Description <code>name</code> Selects the <code>Terra DataRepo</code> source implementation. <code>dataset</code> The <code>UUID</code> of dataset to monitor and read from. <code>table</code> The name of the <code>dataset</code> table to monitor and read from. <code>snapshotReaders</code> A list of email addresses to set as <code>readers</code> of all snapshots created in this workload. <code>pollingIntervalMinutes</code> Optional.  Rate at which WFL will poll TDR for new rows to snapshot. <code>loadTag</code> Optional.  Snapshot only new rows ingested with <code>loadTag</code>."},{"location":"staged-source/#dataset","title":"<code>dataset</code>","text":"<p>The <code>UUID</code> uniquely identifying the TDR dataset which WFL will poll for new workflow inputs.</p>"},{"location":"staged-source/#table","title":"<code>table</code>","text":"<p>The name of the table in <code>dataset</code> which WFL will poll for new workflow inputs.</p> <p>You should design this table such that each row contains all the inputs required to execute a workflow by the workload <code>Executor</code> downstream.</p>"},{"location":"staged-source/#snapshotreaders","title":"<code>snapshotReaders</code>","text":"<p>The email addresses of those whom should be \"readers\" of all snapshots created by workflow-launcher in this workload. You can specify individual users and/or Terra/Firecloud groups.</p>"},{"location":"staged-source/#optional-pollingintervalminutes","title":"Optional <code>pollingIntervalMinutes</code>","text":"<p>The rate, in minutes, at which WFL will poll TDR for new rows to snapshot. If not provided, the default interval is 20 minutes.</p>"},{"location":"staged-source/#optional-loadtag","title":"Optional <code>loadTag</code>","text":"<p>Specifying a <code>loadTag</code> restricts the set of new rows that WFL will include in a snapshot. The <code>loadTag</code> specifies a prefix to be matched against the <code>loadTag</code> used to ingest data to TDR. Thus <code>my-ingest-tag</code> matches all of the following load tags.</p> <pre><code>- `my-ingest-tag`\n- `my-ingest-tag-`\n- `my-ingest-tag-0`\n- `my-ingest-tag-a`\n- `my-ingest-tag-2022-07-14\n</code></pre> <p>When not provided, all new rows will be eligible for snapshotting.</p> <p>Note</p> <p>When initiating a TDR ingest, one can optionally set a load tag. Each row ingested will have this load tag set in its corresponding TDR row metadata table (only visible in BigQuery and not TDR UI). The same load tag can be reused across multiple ingests to logically link them.</p> <p>For questions on how to set the load tag for a particular flavor of ingest, contact #dsp-jade</p>"},{"location":"staged-source/#tdr-snapshots-source","title":"<code>TDR Snapshots</code> Source","text":"<p>You can configure workflow-launcher to use a list of TDR snapshots directly. This may be useful if you don't want workflow-launcher to be a custodian of your dataset or if you already have snapshots you want to process. In this case you must ensure that <code>workflow-launcher@firecloud.org</code> is a <code>reader</code> of all snapshots you want it to process.</p> <p>A typical <code>TDR Snapshots</code> source configuration in the workload request looks like: <pre><code>{\n  \"name\": \"TDR Snapshots\",\n  \"snapshots\": [\n    \"{snapshot-id}\",\n    ...\n  ]\n}\n</code></pre></p> <p>The table below summarises the purpose of each attribute in the above request.</p> Attribute Description <code>name</code> Selects the <code>TDR Snapshots</code> source implementation. <code>snapshots</code> A List of <code>UUID</code>s of snapshots to process. <p>Note</p> <p>You must ensure that the snapshots you list are compatible with the downstream processing stage that consumes them.</p>"},{"location":"staged-source/#developer-guide","title":"Developer Guide","text":"<p>A source is a <code>Queue</code> that satisfies the <code>Source</code> protocol below: <pre><code>(defprotocol Source\n  (start-source!\n    ^Unit\n    [^Connection transaction  ;; JDBC Connection\n     ^Source     source       ;; This source instance\n    ]\n    \"Start enqueuing items onto the `source`'s queue to be consumed by a later\n     processing stage. This operation should not perform any long-running\n     external effects other than database operations via the `transaction`. This\n     function is called at most once during a workload's operation.\")\n  (stop-source!\n    ^Unit\n    [^Connection transaction  ;; JDBC Connection\n     ^Source     source       ;; This source instance\n    ]\n    \"Stop enqueuing inputs onto the `source`'s queue to be consumed by a later\n     processing stage. This operation should not perform any long-running\n     external effects other than database operations via the `transaction`. This\n     function is called at most once during a workload's operation and will only\n     be called after `start-source!`. Any outstanding items on the `source`\n     queue may still be consumed by a later processing stage.\")\n  (update-source!\n    ^Workload\n    [^Workload workload]\n    \"Enqueue items onto the `workload`'s source queue to be consumed by a later processing\n     stage unless stopped, performing any external effects as necessary.\n     Implementations should avoid maintaining in-memory state and making long-\n     running external calls, favouring internal queues to manage such tasks\n     asynchronously between invocations. This function is called one or more\n     times after `start-source!` and may be called after `stop-source!`\"))\n</code></pre></p> <p>Note</p> <p>The <code>Source</code> protocol is implemented by a set of multimethods of the same name. The use of a protocol is to illustrate the difference between the in-memory data model of a <code>Source</code> and the metadata seen by a user.</p> <p>To be used in a workload, a <code>Source</code> implementation should satisfy the processing <code>Stage</code> protocol and the <code>to-edn</code> multimethod in addition to the following multimethods specific to sinks:</p> <pre><code>(defmulti create-source\n  \"Create a `Source` instance using the database `transaction` and configuration\n   in the source `request` and return a `[type items]` pair to be written to a\n   workload record as `source_type` and `source_items`.\n   Notes:\n   - This is a factory method registered for workload creation.\n   - The `Source` type string must match a value of the `source` enum in the\n     database schema.\n   - This multimethod is type-dispatched on the `:name` association in the\n     `request`.\"\n  (fn ^[^String ^String]\n      [^Connection         transaction  ;; JDBC Connection\n       ^long               workload-id  ;; ID of the workload being created\n       ^IPersistentHashMap request      ;; Data forwarded to the handler\n      ]\n      (:name request)))\n\n(defmulti load-source!\n  \"Return the `Source` implementation associated with the `source_type` and\n   `source_items` fields of the `workload` row in the database. Note that this\n   multimethod is type-dispatched on the `:source_type` association in the\n   `workload`.\"\n  (fn ^Sink\n      [^Connection         transaction  ;; JDBC Connection\n       ^IPersistentHashMap workload     ;; Row from workload table\n      ]\n      (:source_type workload)))\n</code></pre>"},{"location":"staged-workload/","title":"Staged Workloads","text":"<p>A staged workload takes data from a source, pushes it into a workflow executor for analysis, and then delivers the results of the analysis to an output location (also known as a sink).</p> <p>Depending on the workload's source, processing may be continuous -- inputs streaming in until stopped -- or discrete -- all inputs known at workload creation.</p>"},{"location":"staged-workload/#staged-workload-components","title":"Staged Workload Components","text":""},{"location":"staged-workload/#source","title":"Source","text":"<p>The workload Source models the first stage of a processing pipeline. A <code>Source</code> reads workflow inputs from a specified location or service in the cloud.</p>"},{"location":"staged-workload/#executor","title":"Executor","text":"<p>The workload Executor models an intermediate stage of a processing pipeline. An <code>Executor</code> uses a supported service in the cloud to execute workflows.</p>"},{"location":"staged-workload/#sink","title":"Sink","text":"<p>The workload Sink models the terminal stage of a processing pipeline. A <code>Sink</code> writes workflow outputs to a desired location in the cloud.</p>"},{"location":"staged-workload/#example-staged-workload","title":"Example Staged Workload","text":"<p>The specific values below are derived from the \"COVID-19 Surveillance in Terra\" project.</p> <p>Workloads for other projects may leverage different implementations for source, executor or sink.</p> <p>For guidance on how to interact with staged workloads via WFL API, see API Usage.</p> <pre><code>{\n    \"watchers\": [\n        [\"slack\", \"C000XXX0XXX\"],\n        [\"slack\", \"C000YYY0YYY\", \"#optional-channel-name-for-context\"]\n    ],\n    \"labels\": [\n        \"hornet:test\"\n    ],\n    \"project\": \"wfl-dev/CDC_Viral_Sequencing\",\n    \"source\": {\n        \"name\": \"Terra DataRepo\",\n        \"dataset\": \"4bb51d98-b4aa-4c72-b76a-1a96a2ee3057\",\n        \"table\": \"flowcells\",\n        \"snapshotReaders\": [\n            \"workflow-launcher-dev@firecloud.org\"\n        ]\n    },\n    \"executor\": {\n        \"name\": \"Terra\",\n        \"workspace\": \"wfl-dev/CDC_Viral_Sequencing\",\n        \"methodConfiguration\": \"wfl-dev/sarscov2_illumina_full\",\n        \"fromSource\": \"importSnapshot\"\n    },\n    \"sink\": {\n        \"name\": \"Terra Workspace\",\n        \"workspace\": \"wfl-dev/CDC_Viral_Sequencing\",\n        \"entityType\": \"flowcell\",\n        \"identifier\": \"run_id\",\n        \"fromOutputs\": {\n            \"submission_xml\" : \"submission_xml\",\n            \"assembled_ids\" : \"assembled_ids\",\n            \"num_failed_assembly\" : \"num_failed_assembly\",\n            ...\n        }\n    }\n}\n</code></pre>"},{"location":"staged-workload/#staged-workload-anatomy-high-level","title":"Staged Workload Anatomy (High Level)","text":"Field Type Description watchers List An optional list of Slack channels to notify labels List A list of user-defined labels. They must be a string of the form <code>\"name\":\"value\u201d</code>, where <code>name</code> must start with a letter followed by any combination of digits, letters, spaces, underscores and hyphens and <code>value</code> is any non-blank string project String A non-null string to allow querying workloads by project source Object The source of new workflow inputs executor Object The mechanism executing the analysis sink Object The destination for workflow outputs"},{"location":"staged-workload/#slack-notifications-for-watchers","title":"Slack Notifications for Watchers","text":"<p>The optional <code>watchers</code> field in a workload request registers Slack channels as watchers of the workload.</p> <pre><code>\"watchers\": [\n        [\"slack\", \"C000XXX0XXX\"],\n        [\"slack\", \"C000YYY0YYY\", \"#optional-channel-name-for-context\"]\n    ]\n</code></pre> <p>When specified, WFL expects a list of Slack channel IDs. You can also add the channel name as the watcher's third element, but because channel names can change this is for decoration and debugging assistance only and not accessed programmatically.</p> <p>Slack channel IDs start with a <code>C</code> and can be found at the bottom of your channel's \"Get channel details\" dropdown:</p> <p></p>"},{"location":"staged-workload/#what-notifications-are-emitted","title":"What notifications are emitted?","text":"<p>User-facing exceptions - Ex. Issues accessing TDR dataset, snapshot, etc.</p> <p>Notable state changes - TDR snapshot creation job failed - Terra submission created - Terra workflow has completed</p> <p></p> <p>In the future, WFL may allow for these two notification streams to be configured separately. High-volume use cases (ex. 100s of workflows/day) may find some state change notifications too noisy.</p>"},{"location":"staged-workload/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Channel must live in the <code>broadinstitute.slack.com</code>   Slack organization</p> </li> <li> <p>The WFL notifier Slack App has been added to your channel --   <code>/invite @WorkFlow Launcher Notifier</code></p> </li> </ul>"},{"location":"staged-workload/#stopping-notifications","title":"Stopping Notifications","text":"<p>If notifications are too noisy, you can <code>/remove @WorkFlow Launcher Notifier</code> from your channel as a quick fix.</p> <p>At this time, there isn't a way to update the <code>watchers</code> list for an existing workload.</p> <p>The long-term approach is to stop your workload, recreate it with an updated <code>watchers</code> list, and start the new workload.</p>"},{"location":"terra/","title":"WorkFlow Launcher's Role in Terra","text":""},{"location":"terra/#summary","title":"Summary","text":"<p>The Data Sciences Platform (DSP) is building a new system (around Terra) for storing and processing biological data. The system design includes a Data Repository where data is stored, and a Methods Repository that executably describes transformations on that data.</p> <p>In the new system, the DSP needs something to fulfill the role that Zamboni currently plays in DSP's current infrastructure to support the Genomics Platform (GP).</p> <p>Zamboni watches various queues for messages describing new data and how to process it. Zamboni interprets those messages to dispatch work to workflow engines (running on the premises or in the cloud) and monitors the progress of those workflows. The Zamboni web UI allows users to track the progress of workflows, and enables Ops engineers to debug problems and resume or restart failed workflows. Zamboni can run workflows on both a local Sun Grid Engine (SGE), and on Cromwell on premises and in the cloud.</p> <p>We think that WFL can fill the role of Zamboni in the new data storage and processing system that DSP is developing now.</p>"},{"location":"terra/#history","title":"History","text":"<p>WFL began as a project to replace a Zamboni starter, with the old name \"Zero\". A starter is a Zamboni component that brokers messages among the queues that Zamboni watches. It interprets messages queued from a Laboratory Information Management System (LIMS), such as the Mercury web service, and demultiplexes them to other Zamboni queues.</p> <p>Zero was later adapted to manage the reprocessing of the first batch of UK Biobank exomes. It has since been adapted to drive workflows for other projects at the Broad.</p> <p>Zero is unusual in that it usually runs as a stateless command line program without special system privilege, and interfaces with services running both on premises and in Google Cloud. It also manages a processing workload as a set of inputs mapped to outputs instead of tracking the progress of individual sample workflows.</p> <p>A Zero user need only specify a source of inputs, a workflow to run, an execution environment, and an output location. Then each time it is invoked, Zero ensures that workflows are started and retried as needed until an output exists for every input.</p> <p>Zero has recently been adapted again to deploy as a web service under Google App Engine (GAE) though most of the value of Zero is still not available to the server. And now it has the new name WFL.</p>"},{"location":"terra/#the-role-of-wfl-in-terra","title":"The role of WFL in Terra","text":"<p>Diagrams of the new DSP processing system show a WFL service subscribed to event streams from the Data Repository (DR), with interfaces to both the Data and the Method Repositories.</p> <p>The implication is that something notifies WFL of new data in the Data Repository and WFL determines how to process it somehow. WFL then looks up whatever is required from the Method Repository, calls on other services as necessary to process the data and writes the results back to the DR. There is also, presumably, a web UI to track and debug the workflows managed by WFL.</p> <p>Many details are yet to be worked out.</p>"},{"location":"terra/#wfl-concepts","title":"WFL Concepts","text":"<p>WFL is designed around several novel concepts.</p> <ol> <li> <p>Manage workloads instead of workflows.</p> <p>This is the biggest difference between Zamboni and Zero (WFL). Zamboni manages workflows whereas WFL manages workloads.</p> <p>Zamboni's unit of work is the workflow. Zamboni manages each workflow separately.</p> <p>A workflow is a transformation specified in WDL or Scala code that succeeds or fails to produce a result. The input to a workflow and its result may consist of multiple files, but they represent a single unit of work managed by a workflow engine such as Cromwell. Zamboni prepares a new workflow for each message it receives by packaging up the input and submitting it to a workflow engine. It then monitors that workflow and reports on its success or failure.</p> <p>WFL manages a workload, which indirectly comprises multiple workflows. Each workflow maps an input to some output, but WFL generally tracks only the inputs and outputs instead of the workflows themselves.</p> <p>Think of a workload as a set of inputs transformed via a workflow engine into a set of outputs. Call that set of outputs the result set. WFL generally does not care whether any individual workflow succeeds or fails. It merely considers all possible inputs specified by the workload, and looks for inputs whose outputs are missing from the result set. If some input lacks an output in the result set, WFL starts a new workflow to process that input.</p> <p>Note: This characterization is unfair to Zamboni. Zamboni also had to manage multiple workflows before the advent of Cromwell and still does when running workflows on SGE. But WFL can take advantage of Cromwell's job management to simplify its implementation.</p> </li> <li> <p>Specify inputs and outputs by general predicates.</p> <p>Each Zamboni message explicitly specifies an input to be processed. Zamboni then starts a workflow for that input and reports its status. Zamboni reports failure so a user can debug and manually succeed, reconsider, or restart the workflow. The output of a successful workflow is not Zamboni's concern.</p> <p>WFL finds inputs by applying a predicate specified by the user subject to some run-time constraint. Then WFL applies a function to each input to find how that input maps to the result set. Another predicate applied to the input, and its output in the result set, determines whether WFL will launch a workflow on that input.</p> <p>Those predicates and function can be anything expressed in a programming language. The run-time constraint is some strings passed on the command line.</p> </li> <li> <p>Minimize user input and decisions at run time.</p> <p>WFL gathers the predicates and mapping functions described above into a module that also knows how to generate everything a workflow engine needs to launch the workflow to process an input into a result output.</p> <p>That module name is one of a few run-time constraints specified by strings in a web form or on a command line. Further constraints are usually one or two of the following:</p> <pre><code>- a processing environment (`dev` `prod` `pharma5`),\n- a file system directory (`/seq/tng/tbl/`\n`file://home/tbl/`),\n- a cloud object prefix (`gs://bucket/folder/`\n`s3://bucket/`),\n- a pathname suffix (`.cram` `.vcf`),\n- a spreadsheet (or JSON, TSV, CSV, XML) filename\n- or a count to limit the scope of a predicate.\n</code></pre> <p>The module interprets the other constraints, determines which processing environments are allowed, and parses any files named accordingly.</p> </li> <li> <p>Maintain provenance.</p> <p>WFL runs out of a single JARfile built entirely from sources pulled from Git repositories. WFL records the Git commit hashes in the JARfile and adds them to every Cromwell workflow it starts. WFL can also preserve the Cromwell metadata alongside any result files generated by the workflow.</p> </li> <li> <p>Run with minimal privilege.</p> <p>Zamboni runs as a service with system account credentials such as <code>picard</code>.</p> <p>WFL is designed to run as whoever invokes it, such as <code>tbl@broadinstitute.org</code>. WFL fetches the users credentials from the environment when invoked from the command line. WFL requires authentication when running as a server, and constructs a JSON Web Token (JWT) to authorize other services as needed.</p> </li> <li> <p>Limit dependencies.</p> <p>WFL depends on a Java runtime, and Gnu make and the Clojure CLI to manage dependencies. Of course, it also pulls in numerous Clojure and Java libraries at build time, and sources WDL files from the <code>warp</code> repository. A programmer need only install Clojure, clone the <code>wfl</code> Git repository, and run <code>make</code> to bootstrap WFL from source.</p> </li> </ol>"},{"location":"terra/#wfl-server","title":"WFL server","text":"<p>The WFL client is a command-line batch program that a person runs intermittently on a laptop or virtual machine (VM).</p> <p>We are working to port the client functions of WFL to a ubiquitous web service (WFL server) running in Google Cloud. That port requires we solve several problems.</p> <ol> <li> <p>State</p> <p>The WFL client is a stateless program that relies on consistent command line arguments to provide the constraints needed to drive the input discovery predicates and so on. Each user runs a separate process that lasts only as long as necessary to complete some stage of a workload.</p> <p>The WFL server is shared among all its users and runs continually. Therefore it requires some kind of data store (a database) to maintain the state of each workload across successive connections from web browsers.</p> <p>We intend to use the hosted Postgres service available to GAE applications for this. This work is already underway (GH-573).</p> </li> <li> <p>Authorization</p> <p>The WFL client assumes it runs in an authenticated context. It can pull credentials from the environment on every invocation that requires authorization to a service.</p> <p>The WFL server will also need to authorize services to run as some authenticated user, but cannot assume the credentials are always available, nor that there is a user present to provide them. WFL can already use OAuth2.0 to authenticate users against an identity provider and use the resulting credentials to build a JWT. It can also derive the bearer token required by most of our authorized services from a JWT.</p> <p>But WFL also needs some secure JWT store, so tokens are available to authorize services even when there is no active user connection. It also needs some mechanism to refresh tokens as they expire to support long-running workloads.</p> </li> <li> <p>Workload specification</p> <p>The user of a WFL service needs some way to specify a workload. A workload may be some set of inputs and the kind of workflow to run on them.</p> <p>A WFL client user now specifies a workload with a module name and a constraint. For example, <code>ukb pharma5 110000 gs://broad-ukb/in/ gs://broad-ukb/out/</code> means find up to <code>110000</code> cloud objects with names prefixed with <code>gs://broad-ukb/in/</code>, process them in the Cromwell set up for <code>pharma5</code>, and store their outputs under <code>gs://broad-ukb/out/</code> somewhere.</p> <p>The <code>ukb</code> module knows how to find <code>.aligned.cram</code> files under the <code>gs://broad-ukb/in/</code> cloud path and set up the WDL and Cromwell dependencies and options necessary to reprocess them into <code>.cram</code> output files. The <code>ukb</code> module also knows how to find the Cromwell deployed to support <code>pharma5</code> workloads, how to authorize the user to that Cromwell, and how to read any supporting data from other services. And finally, the <code>ukb</code> module knows how to determine which inputs do not yet have outputs under the <code>gs://broad-ukb/out/</code> cloud path, and do not have workflows running in the <code>pharma5</code> Cromwell.</p> <p>In an ideal design, this workload specification would integrate conveniently with the Data Repository's subscription or eventing service. In any case though, WFL needs some interface through which a user can specify what needs to be done.</p> </li> <li> <p>Workload management</p> <p>Workloads need to be started, stopped, and monitored somehow. This implies that there is some way to find active or suspended workloads, and affordances for acting on them.</p> <p>Users need some way to monitor the progress of a workload, and to find and debug workloads encountering unacceptable workflow failures.</p> <p>Monitoring and diagnostic code already exists in various WFL modules, but there is no easy way to use them from a web browser.</p> </li> <li> <p>Service interface</p> <p>WFL should be useful to programs other than web browsers. It is easy to imagine Terra users wanting to query WFL for the status of workloads directly without buggy and tedious screen scraping.</p> <p>WFL should at least export a query endpoint for use by other reporting services as well as its own browser interface. It would be nice to provide a familiar JSON or GraphQL query syntax to other services.</p> </li> <li> <p>Browser interface</p> <p>A browser interface should require little in addition to WFL's service interface. Ideally, one should be able to adapt WFL to new workloads via a browser interface without requiring a redeployment.</p> </li> </ol>"},{"location":"usage-abort/","title":"Aborting a WFL Workload","text":"<p>Aborting a workload is done by aborting individual workflows directly with Cromwell. </p> Tip <p>This only works with Cromwell! Don't try this script for things running in Terra, it won't work.</p> <p>Here's a script that can help with that:</p> <pre><code># Usage: bash abort.sh QUERY [WFL_URL] [THREADS]\n#   QUERY is either like `project=PO-123` or `uuid=1a2b3c4d`\n#   WFL_URL is optionally the WFL to abort workflows from\n#       Default is the gotc-prod WFL\n#   THREADS is optionally the number of threads to use to talk to Cromwell\n#       Default is 2\n\n# Usage: bash abort.sh QUERY [WFL_URL]\n#   QUERY is either like `project=PO-123` or `uuid=1a2b3c4d`\n#   WFL_URL is the WFL instance to abort workflows from [default: gotc-prod]\n\nWFL_URL=\"${2:-https://gotc-prod-wfl.gotc-prod.broadinstitute.org}\"\nAUTH_HEADER=\"Authorization: Bearer $(gcloud auth print-access-token)\"\n\ngetWorkloads () {\n    # Query -&gt; [Workload]\n    curl -s -X GET \"${WFL_URL}/api/v1/workload?$1\" \\\n         -H \"${AUTH_HEADER}\" \\\n         | jq\n}\n\ngetWorkflows() {\n    # Workload -&gt; [Workflow]\n    uuid=$(jq -r .uuid &lt;&lt;&lt; \"$1\")\n    curl -s -X GET \"${WFL_URL}/api/v1/workload/${uuid}/workflows\" \\\n         -H \"${AUTH_HEADER}\" \\\n         | jq\n}\n\nmapjq () {\n    jq -c '.[]' &lt;&lt;&lt; \"${2}\" \\\n    | while read elem; do ${1} \"${elem}\"; done \\\n    | jq '[ .[] ]'\n}\n\nmain() {\n    # Query -&gt; ()\n    workloads=$(getWorkloads \"${1}\")\n    cromwell=$(jq -r 'map(.executor) | .[0]' &lt;&lt;&lt; \"$WORKLOAD\")\n\n    mapjq getWorkflows \"${workloads}\"\n        | jq -s 'flatten\n                | map(select(.status != \"Failed\" and .status != \"Succeeded\") | .uuid)\n                | .[]' \\\n        | xargs -I % -n 1 -P ${3:-2} curl -w \"\\n\" -s -X POST \"$CROMWELL/api/workflows/v1/%/abort\" \\\n                -H \"${AUTH_HEADER}\" \\\n                -H \"Content-Type: application/json\"\n}\n\nmain \"$1\"\n</code></pre> <p>The 'QUERY' part is like you'd pass to retry.sh.</p> <p>You don't necessarily need to query WFL for the workload. As of this writing, the response from <code>/start</code> or <code>/exec</code> includes the workflow UUIDs, so if you stored that response in <code>WORKLOAD</code> then you could abort it without having to query WFL (and trigger WFL's potentially lengthy update process).</p>"},{"location":"usage-across-directory/","title":"Using WFL Across a Directory","text":"<p>WFL supports starting workflows from a single file each--depending on the pipeline you specify, other inputs will be extrapolated (see WFL's docs for the specific pipeline for more information).</p> <p>If you have a set of files uploaded to a GCS bucket and you'd like to start a workflow for each one, you can do that via shell scripting.</p> <p>Suppose we have a set of CRAMs in a folder in some bucket, and we'd like to submit them all to WFL for <code>ExternalExomeReprocessing</code> (perhaps associated with some project or ticket, maybe PO-1234). We'll write a short bash script that will handle this for us.</p> <p>Tip</p> <p>Make sure you're able to list the files yourself! You'll need permissions and you may need to run <code>gcloud auth login</code></p>"},{"location":"usage-across-directory/#step-1-list-files","title":"Step 1: List Files","text":"<p>We need a list of all the files you intend to process. This'll depend on the file location, <code>gs://broad-gotc-dev-wfl-ptc-test-inputs/</code> for example. We can use wildcards to list out the individual files we'd like. Make some scratch file like <code>script.sh</code> and store the list of CRAMs in a variable:</p> <pre><code># In script.sh\n\nCRAMS=$(gsutil ls 'gs://broad-gotc-dev-wfl-ptc-test-inputs/**.cram')\n</code></pre>"},{"location":"usage-across-directory/#step-2-format-items","title":"Step 2: Format Items","text":"<p>First, we need to turn that string output into an actual list of file paths. We can use <code>jq</code> to <code>split</code> into lines and <code>select</code> ones that are paths:</p> <pre><code>FILES=$(jq -sR 'split(\"\\n\") | map(select(startswith(\"gs://\")))' &lt;&lt;&lt; \"$CRAMS\")\n</code></pre> <p>Next, we need to format each of those file paths into inputs. WFL doesn't just accept a list of files because we allow configuration of many other inputs and options.</p> <pre><code>ITEMS=$(jq 'map({ inputs: { input_cram: .} })' &lt;&lt;&lt; \"$FILES\")\n</code></pre> <p>Info</p> <p>If you want to process BAMs, you'll need to use <code>input_bam</code> instead of <code>input_cram</code> above.</p>"},{"location":"usage-across-directory/#step-3-make-request","title":"Step 3: Make Request","text":"<p>Now, we can simply insert those items into a normal ExternalExomeReprocessing workload request:</p> <pre><code>REQUEST=$(jq '{\n    cromwell: \"https://cromwell-gotc-auth.gotc-prod.broadinstitute.org\",\n    output: \"gs://broad-gotc-dev-wfl-ptc-test-outputs/xx-test-output\",\n    pipeline: \"ExternalExomeReprocessing\",\n    project: \"PO-1234\",\n    items: .\n}' &lt;&lt;&lt; \"$ITEMS\")\n</code></pre> <p>Info</p> <p>Remember to change the <code>output</code> bucket! And the <code>project</code> isn't used by WFL but we keep track of it to help you organize workloads based on tickets or anything else.</p> <p>Info</p> <p>You can make other customizations here too, like specifying some input or option across all the workflows by adding a <code>common</code> block. See the docs for your pipeline or the workflow options page for more info.</p> <p>Last, we can use <code>curl</code> to send off the request to WFL:</p> <pre><code>curl -X POST 'https://dev-wfl.gotc-dev.broadinstitute.org/api/v1/exec' \\\n    -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n    -H 'Content-Type: application/json' \\\n    -d \"$REQUEST\"\n</code></pre> <p>Warning</p> <p>Curl will complain if the <code>$REQUEST</code> here contains more than thousand lines of data. Remember to dump the payload to a file such as <code>payload.json</code> and let Curl read from that file instead in that case. For example, the last step can be replaced by:</p> <pre><code>echo \"$REQUEST\" &gt;&gt; \"payload.json\"\n\ncurl -X POST 'https://dev-wfl.gotc-dev.broadinstitute.org/api/v1/exec' \\\n    -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n    -H 'Content-Type: application/json' \\\n    -d \"@payload.json\"\n</code></pre> <p>With this, the final result is something like the following:</p> <pre><code>CRAMS=$(gsutil ls 'gs://broad-gotc-dev-wfl-ptc-test-inputs/**.cram')\n\nFILES=$(jq -sR 'split(\"\\n\") | map(select(startswith(\"gs://\")))' &lt;&lt;&lt; \"$CRAMS\")\n\nITEMS=$(jq 'map({ inputs: { input_cram: .} })' &lt;&lt;&lt; \"$FILES\")\n\nREQUEST=$(jq '{\n    cromwell: \"https://cromwell-gotc-auth.gotc-prod.broadinstitute.org\",\n    output: \"gs://broad-gotc-dev-wfl-ptc-test-outputs/xx-test-output\",\n    pipeline: \"ExternalExomeReprocessing\",\n    project: \"PO-1234\",\n    items: .\n}' &lt;&lt;&lt; \"$ITEMS\")\n\ncurl -X POST 'https://gotc-prod-wfl.gotc-prod.broadinstitute.org/api/v1/exec' \\\n    -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n    -H 'Content-Type: application/json' \\\n    -d \"$REQUEST\"\n</code></pre> <p>Save that as <code>script.sh</code> and run with <code>bash myscript.sh</code> and you should be good to go!</p>"},{"location":"usage-across-directory/#other-notes","title":"Other Notes","text":"<p>Have a lot of workflows to submit? You can use array slicing to help split things up:</p> <pre><code>FILES=$(jq -sR 'split(\"\\n\") | map(select(startswith(\"gs://\")))[0:5000]' &lt;&lt;&lt; \"$CRAMS\")\n</code></pre> <p>Need to select files matching some other query too? You can chain the <code>map</code>-<code>select</code> commands and use other string filters on the file names:</p> <pre><code>FILES=$(jq -sR 'split(\"\\n\") | map(select(startswith(\"gs://\"))) |\n    map(select(contains(\"foobar\")))' &lt;&lt;&lt; \"$CRAMS\")\n</code></pre> <p>If <code>contains</code>/<code>startswith</code>/<code>endswith</code> aren't enough, you can use <code>test</code> with PCRE regex:</p> <pre><code>FILES=$(jq -sR 'split(\"\\n\") | map(select(startswith(\"gs://\"))) |\n    map(select(test(\"fo+bar\")))' &lt;&lt;&lt; \"$CRAMS\")\n</code></pre> <p>See this page for more <code>jq</code> info.</p>"},{"location":"usage-retry/","title":"Retrying Workflows","text":""},{"location":"usage-retry/#retrying-terra-workflows-via-wfl-api","title":"Retrying Terra Workflows via WFL API","text":"<p>WFL staged workloads with a Terra executor have a <code>/retry</code> endpoint that selects unretried workflows by their submission ID and re-submits them.</p> <p>The following <code>curl</code> shell command finds the unretried workflows launched by submission <code>$SUBMISSION</code> in workload <code>$UUID</code> and resubmits the underlying snapshot for processing.</p> <pre><code>WFL=https://gotc-prod-wfl.gotc-prod.broadinstitute.org/api/v1/workload\n\nAUTH=\"Authorization: Bearer $(gcloud auth print-access-token)\"\n\nUUID=0d307eb3-2b8e-419c-b687-8c08c84e2a0c       # workload UUID\nSUBMISSION=14bffc69-6ce7-4615-b318-7ef1c457c894 # Terra submission UUID\n\ncurl -X POST -H \"$AUTH\" $WFL/$UUID/retry \\\n  --data \"{\\\"submission\\\":\\\"$SUBMISSION\\\"}\" \\\n  | jq\n</code></pre> <p>A successful <code>/retry</code> request returns the workload specified by <code>$UUID</code>. A failed <code>/retry</code> request will return a description of the failure.</p> <p>For legacy (non-staged) workloads, the <code>/retry</code> endpoint is unimplemented and returns a <code>501</code> HTTP failure status. In such cases, retries may be facilitated by the runbook below.</p>"},{"location":"usage-retry/#request-body","title":"Request Body","text":"<p>The request body filters must be valid:</p> <ul> <li>Mandatory <code>submission</code> - Terra submission ID (must be a valid UUID)</li> <li>Optional <code>status</code> - Workflow status   (if specified, must be a retriable Cromwell workflow status)</li> </ul> <p>The only Cromwell statuses supported with the <code>/retry</code> API are the terminal workflow statuses:</p> <ul> <li><code>\"Aborted\"</code></li> <li><code>\"Failed\"</code></li> <li><code>\"Succeeded\"</code></li> </ul> Why would you retry succeeded workflows? <p>A workflow may have functionally succeeded, but be scientifically inaccurate and need to be rerun, e.g. if the initial run contained incorrect metadata.</p> <p>Attempting to retry workflows of any other status will return a <code>400</code> HTTP failure status, as will a valid combination of filters with no matching workflows in WFL's DB. Examples:</p> <ul> <li>A valid Terra submission ID for a different workload</li> <li><code>\"Failed\"</code> workflow status when all unretried workflows had <code>\"Succeeded\"</code></li> </ul>"},{"location":"usage-retry/#warnings-and-caveats","title":"Warnings and Caveats","text":""},{"location":"usage-retry/#submission-of-snapshot-subsets-not-yet-supported","title":"Submission of snapshot subsets not yet supported","text":"<p>WFL is limited by Rawls functionality and cannot yet submit a subset of a snapshot. So retrying any workflow from a workload snapshot will resubmit all entities from that snapshot.</p> <p>(Because of this, the optional workflow status filter is purely decorative: all sibling workflows from the same submission will be resubmitted, regardless of their status.)</p> <p>Example - a running submission from a snapshot has 500 workflows:</p> <ul> <li> <p>1 failed</p> </li> <li> <p>249 running</p> </li> <li> <p>250 succeeded</p> </li> </ul> <p>Retrying the failed workflow will create a new submission where all 500 original workflows are retried.</p> <p>Consider whether you should wait for all workflows in the submission to complete before initiating a retry to avoid multiple workflows running concurrently in competition for the same output files.</p>"},{"location":"usage-retry/#race-condition-when-retrying-the-same-workload-concurrently","title":"Race condition when retrying the same workload concurrently","text":"<p>A caller could hit this endpoint for the same workload multiple times in quick succession, making possible a race condition where each run retries the same set of workflows.</p> <p>Future improvements will make this operation threadsafe, but in the interim try to wait for a response from your retry request before resubmitting.</p>"},{"location":"usage-retry/#retrying-failures-via-wfl-runbook","title":"Retrying Failures via WFL Runbook","text":"<p>For legacy (non-staged) workloads, WFL remembers enough about submissions to let you quickly resubmit failed workflows with the same inputs/options as they were originally submitted.</p> <p>All you need is a query string like you'd pass to the <code>/workflows</code> endpoint, either:</p> <ul> <li><code>uuid=&lt;UUID&gt;</code> where <code>&lt;UUID&gt;</code> is the identifier of the specific workload you'd like to retry failures from<ul> <li>Ex: <code>uuid=95d536c7-ce3e-4ffc-8c9c-2b9c710d625a</code></li> </ul> </li> <li><code>project=&lt;PROJECT&gt;</code> where <code>&lt;PROJECT&gt;</code> is the value of the project field of the workloads you'd like to retry<ul> <li>Ex: <code>project=PO-29619</code></li> </ul> </li> </ul> <p>With the below script, WFL will find matching workloads and resubmit any unique failures of individual workflows in a new workload (with the same parameters as the originals).</p> <p>Usage: <code>bash retry.sh QUERY</code></p> <p>Ex: <code>bash retry.sh project=PO-29619</code></p> <pre><code># Usage: bash abort.sh QUERY [WFL_URL]\n#   QUERY is either like `project=PO-123` or `uuid=1a2b3c4d`\n#   WFL_URL is the WFL instance to retry workflows from [default: gotc-prod]\n\nWFL_URL=\"${2:-https://gotc-prod-wfl.gotc-prod.broadinstitute.org}\"\nAUTH_HEADER=\"Authorization: Bearer $(gcloud auth print-access-token)\"\n\ngetWorkloads () {\n    # Query -&gt; [Workload]\n    curl -s -X GET \"${WFL_URL}/api/v1/workload?$1\" \\\n         -H \"${AUTH_HEADER}\" \\\n         | jq\n}\n\ngetWorkflows() {\n    # Workload -&gt; [Workflow]\n    uuid=$(jq -r .uuid &lt;&lt;&lt; \"$1\")\n    curl -s -X GET \"${WFL_URL}/api/v1/workload/${uuid}/workflows\" \\\n         -H \"${AUTH_HEADER}\" \\\n         | jq\n}\n\nfailedWorkflowsToSubmit() {\n    # [[Workflow]] -&gt; [Workflow]\n    jq 'flatten\n        | map ( select ( .status==\"Failed\" )\n                | {inputs: .inputs, options: .options}\n                | del ( .[] | nulls ) )\n        ' &lt;&lt;&lt; \"$1\"\n}\n\nmakeRetryRequest() {\n    # [Workload], [Workflow] -&gt; Request\n    jq --argjson 'workflows' \"$2\" \\\n        '.[0]\n         | { executor: .executor\n           , input: .input\n           , output: .output\n           , pipeline: .pipeline\n           , project: .project\n           , items: $workflows\n           }\n        | del(.[] | nulls)\n        ' &lt;&lt;&lt; \"$1\"\n}\n\nmapjq () {\n    jq -c '.[]' &lt;&lt;&lt; \"${2}\" \\\n    | while read elem; do ${1} \"${elem}\"; done \\\n    | jq -s '[ .[] ]'\n}\n\nmain() {\n    # Query -&gt; ()\n    workloads=$(getWorkloads \"${1}\")\n    workflows=$(mapjq getWorkflows \"${workloads}\")\n    toSubmit=$(failedWorkflowsToSubmit \"${workflows}\")\n    makeRetryRequest \"${workloads[0]}\" \"${toSubmit}\" &gt; /tmp/retry.json\n\n    curl -X POST \"${WFL_URL}/api/v1/exec\" \\\n         -H \"${AUTH_HEADER}\" \\\n         -H \"Content-Type: application/json\" \\\n         -d @/tmp/retry.json\n}\n\nmain \"$1\"\n</code></pre>"},{"location":"usage-retry/#tips","title":"Tips","text":""},{"location":"usage-retry/#customizing-inputsoptions","title":"Customizing Inputs/Options","text":"<p>If you want to inject a new input or option into all of the retried workflows, you can do that with a <code>common</code> block. For example, replace this:</p> <pre><code>jq '{\n    executor: .executor,\n</code></pre> <p>with this:</p> <pre><code>jq '{\n    common: { inputs: { \"WholeGenomeReprocessing.WholeGenomeGermlineSingleSample.BamToCram.ValidateCram.memory_multiplier\": 2 } },\n    executor: .executor,\n</code></pre> <p>That example uses WFL's arbitrary input feature to bump up the memory multiplier for a particular WGS task.</p> <ul> <li>Nested inputs will have periods in them, you'll need to use quotes around it</li> <li>You can't override inputs or options that the workflows originally had (the <code>common</code> block has lower precedence)</li> </ul>"},{"location":"usage-workflow-options/","title":"Customizing Workflow Options","text":"Tip <p>This page covers customizing workflow options, which are different from the inputs passed to the WDL. Workflow options are interpreted directly by Cromwell, though WDLs can customize them too. For more information see Cromwell's documentation.</p> <p>Another important piece of context for this page is the difference between a workflow that actually gets run on Cromwell versus a workload (a WFL-managed set of individual workflows).</p>"},{"location":"usage-workflow-options/#usage","title":"Usage","text":"<p>Summary</p> <ul> <li>Workflow options are an arbitrary JSON object stored in a key of <code>options</code></li> <li>Can be provided per-workflow, for an entire workload, or both</li> <li>Optional -- you only need to specify options if you'd like to override something</li> </ul> <p>Suppose the following valid workload request that you might <code>POST</code> to <code>/create</code> or <code>/exec</code>:</p> <pre><code>{\n  \"executor\": \"https://cromwell-gotc-auth.gotc-dev.broadinstitute.org\",\n  \"input\": \"gs://broad-gotc-dev-wfl-ptc-test-inputs/single_sample/plumbing/truth\",\n  \"output\": \"gs://broad-gotc-dev-wfl-ptc-test-outputs/wgs-test-output/\",\n  \"pipeline\": \"ExternalWholeGenomeReprocessing\",\n  \"project\": \"PO-1234\",\n  \"items\": [\n    {\n      \"inputs\": {\n        \"input_cram\": \"develop/20k/NA12878_PLUMBING.cram\",\n        \"sample_name\": \"TestSample1234\"\n      }\n    },\n    {\n      \"inputs\": {\n        \"input_cram\": \"develop/20k/NA12878_PLUMBING.cram\",\n        \"sample_name\": \"TestSample5678\"\n      }\n    }\n  ]\n}\n</code></pre> <p>You may optionally add arbitrary JSON objects as <code>options</code> either for individual workflows, for the entire workload, or both:</p> <pre><code>{\n  \"executor\": \"https://cromwell-gotc-auth.gotc-dev.broadinstitute.org\",\n  \"input\": \"gs://broad-gotc-dev-wfl-ptc-test-inputs/single_sample/plumbing/truth\",\n  \"output\": \"gs://broad-gotc-dev-wfl-ptc-test-outputs/wgs-test-output/\",\n  \"pipeline\": \"ExternalWholeGenomeReprocessing\",\n  \"project\": \"PO-1234\",\n  \"common\": {\n    \"options\": {\n        \"write_to_cache\": false,\n        \"google_project\": \"broad-google-project\"\n    }\n  },\n  \"items\": [\n    {\n      \"inputs\": {\n        \"input_cram\": \"develop/20k/NA12878_PLUMBING.cram\",\n        \"sample_name\": \"TestSample1234\"\n      },\n      \"options\": {\n        \"jes_gcs_root\": \"gs://broad-gotc-something-execution\"\n      }\n    },\n    {\n      \"inputs\": {\n        \"input_cram\": \"develop/20k/NA12878_PLUMBING.cram\",\n        \"sample_name\": \"TestSample5678\"\n      },\n      \"options\": {\n        \"jes_gcs_root\": \"gs://broad-gotc-different-execution\",\n        \"default_runtime_attributes\": {\"maxRetries\": 3},\n        \"google_project\": \"broad-google-project-2\"\n      }\n    }\n  ]\n}\n</code></pre> Info <p>This behavior isn't supported for All-of-Us-related processing.</p> <p>To recap, in the above example the following workflow options will be set:</p> <ul> <li>\"jes_gcs_root\" will have different values for the different samples</li> <li>\"default_runtime_attributes\" will override the \"maxRetries\" value to be 3</li> <li>\"write_to_cache\" will be false for all samples</li> <li>\"google_project\" is broad-google-project for the entire workload but is overridden in the second sample to be broad-google-project-2 (providing an option with more granularity gives it higher precedence)</li> </ul> <p>In other words, WFL will recursively merge the options objects together to resolve the options for individual workflows. You can see this in WFL's response, which includes all workflow options calculated for each workflow:</p> <pre><code>[{  \n  \"inputs\": {\n    \"input_cram\": \"develop/20k/NA12878_PLUMBING.cram\",\n    \"sample_name\": \"TestSample1234\"\n  },\n  \"options\": {\n    \"jes_gcs_root\": \"gs://broad-gotc-something-execution\",\n    \"write_to_cache\": false,\n    \"google_project\": \"broad-google-project\"\n  }\n},\n{\n  \"inputs\": {\n    \"input_cram\": \"develop/20k/NA12878_PLUMBING.cram\",\n    \"sample_name\": \"TestSample5678\"\n  },\n  \"options\": {\n    \"jes_gcs_root\": \"gs://broad-gotc-different-execution\",\n    \"default_runtime_attributes\": {\"maxRetries\": 3},\n    \"write_to_cache\": false,\n    \"google_project\": \"broad-google-project-2\"\n  }\n}]\n</code></pre> <p>One note is that WFL already has some default values it passes for workflow options and you'll see those defaults when you look at the returned options for a given workflow. See below for more information.</p>"},{"location":"usage-workflow-options/#behavior","title":"Behavior","text":"<p>The diagram below lists the different sources of options for a particular workflow. Precedence is from the bottom up, so \"higher\" sources override lower ones.</p> <p></p> <p>The green \"sources\" are where you may optionally provide configuration via <code>options</code>, the white \"sources\" are where WFL may create and supply options by default, and the gray \"sources\" are outside of WFL's visibility but can still affect the result.</p> <p>WFL supplies its own derived options usually on a per-module basis, meaning different pipelines that make use of different modules may have different options they derive and supply by default.</p> <p>Individual module documentation can help provide more info, as will simply looking at WFL's response from the <code>/create</code> or <code>/exec</code> endpoints, which includes those defaults.</p>"}]}