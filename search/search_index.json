{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to WorkFlow Launcher \u00b6 Build Board \u00b6 Type Master Branch Dev Prod Unit Test N/A N/A Integration Overview \u00b6 WorkFlow Launcher (WFL) is a workload manager. For example, a workload could be a set of WGS samples to be reprocessed in a given project/bucket, the workflow is the processing of an individual sample in that workload running WGS reprocessing. It runs as you, with your credentials, from your laptop, and communicates with other services as necessary to manage a workload. It can also be deployed to run as a service in the cloud. For more on Workflow Launcher's role in the Terra infrastructure see Workflow Launcher's role in Terra . Set up \u00b6 Run boot build at the top of a wfl.git repo to build an uberjar. The resulting jar is in target/wfl-*.jar relative to the wfl.git clone. With some start-up and performance penalty, you can also run Workflow Launcher as a script. See below for details. Versioning \u00b6 Workflow Launcher needs to manage its version and the versions of dsde-pipelines.git which contribute the WDL files. There may be as many dsde-pipelines.git versions as there are workflow wdls. The wfl jar includes a manifest with at least that information in it, and a version command that returns it. Capabilities \u00b6 When Workflow Launcher is on-premises or in the cloud, it can currently talk to the following services: service on premises in cloud Cloud SQL x x Cromwell x x Google App Engine x x Google Cloud Platform Admin x x Google Cloud Pub/Sub x x Google Cloud Storage x x Oracle DB x Vault x x Wfl x Workflow Launcher has a diagnostic mode, dx , for debugging problems. Run zero dx to get a list of the diagnostics available. wm28d-f87:wfl yanc$ java -jar ./target/wfl-2020-03-13t17-29-12z.jar dx zero dx: tools to help debug workflow problems. Usage: zero dx <tool> [ <arg> ... ] Where: <tool> is the name of some diagnostic tool. <arg> ... are optional arguments to <tool>. The <tool>s and their <arg>s are named here. all-metadata environment & ids All workflow metadata for IDS from Cromwell in ENVIRONMENT. event-timing environment id Time per event type for workflow with ID in ENVIRONMENT. ... Error: Must specify a dx <tool> to run. BTW: You ran: zero dx wm28d-f87:wfl yanc$ Implementation \u00b6 For frontend details, check Frontend Section Top-level files \u00b6 After cloning a new WFL repo, the top-level files are. README.md is this file, which is just a symlink to the actual doc file under docs/md/ . boot.properties overrides some defaults in boot-clj . ( boot.properties is something like build.properties for sbt .) build.boot is a Clojure script to bootstrap WFL with boot-clj . build.txt holds a monotonically increasing integer for build versioning. database/ holds database scheme migration changelog and changeset files for liquibase. docs/ has ancillary documentation. It's compiled as a static doc website. ops/ is a directory of standard scripts to support operations. It includes scripts to deploy the server in Google App Engine, and to run it locally for easier debugging. (See server.md for more information.) resources/ contains the simplelogger properties and files staged from other repositories that need to be on the Java classpath when running from the .jar file. src/ contains the Workflow Launcher source code. test/ contains a mixture of unit and integration tests and supplementary test tooling. After building and working with WFL a while, you may notice a couple of other top-level files and directories. project.clj is a lein project file to support IntelliJ. zero is a link to build.boot that runs WFL as a script. Run boot build at least once after cloning the repo to make sure all the necessary files are in place. Source code \u00b6 The Clojure source code is in the src/ directory. The entry point for the WFL executable is the -main function in main.clj . It takes the command line arguments as strings, validates the arguments, then launches the appropriate process. The server.clj file implements the WFL server. The server_debug.clj file adds some tools to aid in debugging the server. Some hacks specific to WFL are in zero.clj . The boot.clj offloads code from the build.boot file for easier development and debugging. The debug.clj file defines some macros useful when debugging or logging. The util.clj file contains a few functions and macros used in WFL that are not specific to its function. The environments.clj file defines configuration parameters for different execution contexts. It's a placeholder in this repo but will be loaded in build/deploy time from a private repo. The module/ukb.clj file implements a command-line starter for the White Album , Pharma5 , or UK Biobank project. The module/xx.clj file implements a command-line starter for reprocessing eXternal eXomes . The module/wgs.clj file helps implements a command-line starter for reprocessing Whole GenomeS . The module/all.clj file hosts some utilities shared across modules. The metadata.clj file implements a tool to extract metadata from Cromwell that can be archived with the outputs generated by a workflow. The dx.clj file implements miscellaneous pipeline debugging tools. The once.clj file defines some initialization functions mostly supporting authentication. The api/handlers.clj file defines the handler functions used by server. The api/routes.clj file defines the routing strategy for server. Each of the other source files implement an interface to one of the services WFL talks to, and are named accordingly. File Service cromwell.clj Cromwell workflow runner datarepo.clj DSP DataRepo db.clj On-prem and Cloud SQL databases gcs.clj Google Cloud Storage jms.clj Java Message Service queues postgres.clj Cloud SQL postgres databases pubsub.clj Google Cloud Pub/Sub server.clj the WFL server itself wdl.clj parse WDL and manage dependencies Test code \u00b6 Test code lives under the project test/ root. At present, wfl has two kinds of test, unit and integration . These can be run via the deps.edn , optionally specifying the kind: clojure -A:test [ unit | integration ] Note that the integration tests currently require a little more configuration before they can be run, namely, they require a wfl server running locally: ./ops/server.sh See the development guide for more information. Development \u00b6 WFL is implemented in Clojure and uses a tool named boot or boot-clj to manage dependencies and so on. The boot tool is a Clojure bootstrapper: it's job is to turn a standard Linux, MacOS, or Windows process into something that can host a Clojure program. WFL uses a gcloud auth command line to authenticate the user. You need to be authenticated to Google Cloud and have a recent version of google-cloud-sdk in your path to run zero or its jar successfully. I verified that Google Cloud SDK 161.0.0 works. That or any later version should be OK. Installation \u00b6 See this link to install boot-clj . Running boot is enough to \"install\" Clojure. The build.boot file is equivalent to the build.sbt file for SBT in Scala projects. It specified project dependencies and the build and release pipeline. It also functions as a script for running and testing the project without a separate compilation step. There is another tool like boot named lein , which is short for \" Leiningen \". You currently need lein to develop with IntelliJ using its Clojure plugin Cursive . MacOS \u00b6 On MacOS, I suggest installing Homebrew and then running this. # brew install boot-clj leiningen You can brew install maven , and java too if necessary. Arch Linux \u00b6 install clojure and leiningen from the official repositories. Install boot and google-cloud-sdk from the AUR. Hacking \u00b6 Clojure development feels very different from Scala and Java development. It even differs markedly from development in other dynamic languages such as Python or Ruby. Get a demonstration from someone familiar with Clojure development before you spend too much time trying to figure things out on your own. Find a local Cursive user for guidance if you like IntelliJ. Rex Wang and Saman Ehsan know how to use it. Cursive licences are available here . The steps for getting this project set up with very recent versions of IntelliJ differ from Cursive's docs: 1. Outside of IntelliJ , clone the repo and run boot at the top-level to generate the project.clj (see below) 2. Now inside of IntelliJ , import the project by specifically targeting the project.clj file (it should offer to import the entire project, and targeting the project.clj will make use of Leiningen to work with Cursive) 3. Use the Project Structure window (Help -> Find Action -> Project Structure) to set a JDK as the Project SDK There is also a Calva plugin for Visual Studio Code . I hack Clojure in Emacs using CIDER and nREPL . CIDER is not trivial to set up, but not especially difficult if you are used to Emacs. (I can help if CIDER gives you trouble.) Every time boot runs, it generates a project.clj file to support lein , Cursive, and Calva users. Running boot build will not only build a fat jar ( uberjar ) for the WFL project, but will add an executable symbolic link zero to conveniently execute the Clojure code as a script. Testing \u00b6 If you've never run boot before, you may have to run it twice: first to bootstrap Clojure and boot itself, and again to download their and WFL's dependencies. The first boot build run will create a ./wfl link to the build.boot file ./wfl starter dev $USER @broadinstitute.org You should eventually receive an humongous email from wfl@broadinstitute.org containing evidence of Zero's adventures. Of course, after boot build , you can also run WFL from its JAR file. Exomes in the Cloud Resources \u00b6 From Hybrid Selection in the Cloud V1 Clients Google Cloud Storage Client Library (Java) Google Cloud Client Library for Java Diagrams Zamboni Overview Sources /Users/tbl/Broad/zamboni/Client/src/scala/org/broadinstitute/zamboni/client/lightning/clp/Lightning.scala /Users/tbl/Broad/picard-private/src/java/edu/mit/broad/picard/lightning /Users/tbl/Broad/gppipeline-devtools/release client /Users/tbl/Broad/gppipeline-devtools/starter control /picard02:/seq/pipeline/gppipeline-devtools/current/defs/prod.defs","title":"Welcome to WorkFlow Launcher"},{"location":"#welcome-to-workflow-launcher","text":"","title":"Welcome to WorkFlow Launcher"},{"location":"#build-board","text":"Type Master Branch Dev Prod Unit Test N/A N/A Integration","title":"Build Board"},{"location":"#overview","text":"WorkFlow Launcher (WFL) is a workload manager. For example, a workload could be a set of WGS samples to be reprocessed in a given project/bucket, the workflow is the processing of an individual sample in that workload running WGS reprocessing. It runs as you, with your credentials, from your laptop, and communicates with other services as necessary to manage a workload. It can also be deployed to run as a service in the cloud. For more on Workflow Launcher's role in the Terra infrastructure see Workflow Launcher's role in Terra .","title":"Overview"},{"location":"#set-up","text":"Run boot build at the top of a wfl.git repo to build an uberjar. The resulting jar is in target/wfl-*.jar relative to the wfl.git clone. With some start-up and performance penalty, you can also run Workflow Launcher as a script. See below for details.","title":"Set up"},{"location":"#versioning","text":"Workflow Launcher needs to manage its version and the versions of dsde-pipelines.git which contribute the WDL files. There may be as many dsde-pipelines.git versions as there are workflow wdls. The wfl jar includes a manifest with at least that information in it, and a version command that returns it.","title":"Versioning"},{"location":"#capabilities","text":"When Workflow Launcher is on-premises or in the cloud, it can currently talk to the following services: service on premises in cloud Cloud SQL x x Cromwell x x Google App Engine x x Google Cloud Platform Admin x x Google Cloud Pub/Sub x x Google Cloud Storage x x Oracle DB x Vault x x Wfl x Workflow Launcher has a diagnostic mode, dx , for debugging problems. Run zero dx to get a list of the diagnostics available. wm28d-f87:wfl yanc$ java -jar ./target/wfl-2020-03-13t17-29-12z.jar dx zero dx: tools to help debug workflow problems. Usage: zero dx <tool> [ <arg> ... ] Where: <tool> is the name of some diagnostic tool. <arg> ... are optional arguments to <tool>. The <tool>s and their <arg>s are named here. all-metadata environment & ids All workflow metadata for IDS from Cromwell in ENVIRONMENT. event-timing environment id Time per event type for workflow with ID in ENVIRONMENT. ... Error: Must specify a dx <tool> to run. BTW: You ran: zero dx wm28d-f87:wfl yanc$","title":"Capabilities"},{"location":"#implementation","text":"For frontend details, check Frontend Section","title":"Implementation"},{"location":"#top-level-files","text":"After cloning a new WFL repo, the top-level files are. README.md is this file, which is just a symlink to the actual doc file under docs/md/ . boot.properties overrides some defaults in boot-clj . ( boot.properties is something like build.properties for sbt .) build.boot is a Clojure script to bootstrap WFL with boot-clj . build.txt holds a monotonically increasing integer for build versioning. database/ holds database scheme migration changelog and changeset files for liquibase. docs/ has ancillary documentation. It's compiled as a static doc website. ops/ is a directory of standard scripts to support operations. It includes scripts to deploy the server in Google App Engine, and to run it locally for easier debugging. (See server.md for more information.) resources/ contains the simplelogger properties and files staged from other repositories that need to be on the Java classpath when running from the .jar file. src/ contains the Workflow Launcher source code. test/ contains a mixture of unit and integration tests and supplementary test tooling. After building and working with WFL a while, you may notice a couple of other top-level files and directories. project.clj is a lein project file to support IntelliJ. zero is a link to build.boot that runs WFL as a script. Run boot build at least once after cloning the repo to make sure all the necessary files are in place.","title":"Top-level files"},{"location":"#source-code","text":"The Clojure source code is in the src/ directory. The entry point for the WFL executable is the -main function in main.clj . It takes the command line arguments as strings, validates the arguments, then launches the appropriate process. The server.clj file implements the WFL server. The server_debug.clj file adds some tools to aid in debugging the server. Some hacks specific to WFL are in zero.clj . The boot.clj offloads code from the build.boot file for easier development and debugging. The debug.clj file defines some macros useful when debugging or logging. The util.clj file contains a few functions and macros used in WFL that are not specific to its function. The environments.clj file defines configuration parameters for different execution contexts. It's a placeholder in this repo but will be loaded in build/deploy time from a private repo. The module/ukb.clj file implements a command-line starter for the White Album , Pharma5 , or UK Biobank project. The module/xx.clj file implements a command-line starter for reprocessing eXternal eXomes . The module/wgs.clj file helps implements a command-line starter for reprocessing Whole GenomeS . The module/all.clj file hosts some utilities shared across modules. The metadata.clj file implements a tool to extract metadata from Cromwell that can be archived with the outputs generated by a workflow. The dx.clj file implements miscellaneous pipeline debugging tools. The once.clj file defines some initialization functions mostly supporting authentication. The api/handlers.clj file defines the handler functions used by server. The api/routes.clj file defines the routing strategy for server. Each of the other source files implement an interface to one of the services WFL talks to, and are named accordingly. File Service cromwell.clj Cromwell workflow runner datarepo.clj DSP DataRepo db.clj On-prem and Cloud SQL databases gcs.clj Google Cloud Storage jms.clj Java Message Service queues postgres.clj Cloud SQL postgres databases pubsub.clj Google Cloud Pub/Sub server.clj the WFL server itself wdl.clj parse WDL and manage dependencies","title":"Source code"},{"location":"#test-code","text":"Test code lives under the project test/ root. At present, wfl has two kinds of test, unit and integration . These can be run via the deps.edn , optionally specifying the kind: clojure -A:test [ unit | integration ] Note that the integration tests currently require a little more configuration before they can be run, namely, they require a wfl server running locally: ./ops/server.sh See the development guide for more information.","title":"Test code"},{"location":"#development","text":"WFL is implemented in Clojure and uses a tool named boot or boot-clj to manage dependencies and so on. The boot tool is a Clojure bootstrapper: it's job is to turn a standard Linux, MacOS, or Windows process into something that can host a Clojure program. WFL uses a gcloud auth command line to authenticate the user. You need to be authenticated to Google Cloud and have a recent version of google-cloud-sdk in your path to run zero or its jar successfully. I verified that Google Cloud SDK 161.0.0 works. That or any later version should be OK.","title":"Development"},{"location":"#installation","text":"See this link to install boot-clj . Running boot is enough to \"install\" Clojure. The build.boot file is equivalent to the build.sbt file for SBT in Scala projects. It specified project dependencies and the build and release pipeline. It also functions as a script for running and testing the project without a separate compilation step. There is another tool like boot named lein , which is short for \" Leiningen \". You currently need lein to develop with IntelliJ using its Clojure plugin Cursive .","title":"Installation"},{"location":"#macos","text":"On MacOS, I suggest installing Homebrew and then running this. # brew install boot-clj leiningen You can brew install maven , and java too if necessary.","title":"MacOS"},{"location":"#arch-linux","text":"install clojure and leiningen from the official repositories. Install boot and google-cloud-sdk from the AUR.","title":"Arch Linux"},{"location":"#hacking","text":"Clojure development feels very different from Scala and Java development. It even differs markedly from development in other dynamic languages such as Python or Ruby. Get a demonstration from someone familiar with Clojure development before you spend too much time trying to figure things out on your own. Find a local Cursive user for guidance if you like IntelliJ. Rex Wang and Saman Ehsan know how to use it. Cursive licences are available here . The steps for getting this project set up with very recent versions of IntelliJ differ from Cursive's docs: 1. Outside of IntelliJ , clone the repo and run boot at the top-level to generate the project.clj (see below) 2. Now inside of IntelliJ , import the project by specifically targeting the project.clj file (it should offer to import the entire project, and targeting the project.clj will make use of Leiningen to work with Cursive) 3. Use the Project Structure window (Help -> Find Action -> Project Structure) to set a JDK as the Project SDK There is also a Calva plugin for Visual Studio Code . I hack Clojure in Emacs using CIDER and nREPL . CIDER is not trivial to set up, but not especially difficult if you are used to Emacs. (I can help if CIDER gives you trouble.) Every time boot runs, it generates a project.clj file to support lein , Cursive, and Calva users. Running boot build will not only build a fat jar ( uberjar ) for the WFL project, but will add an executable symbolic link zero to conveniently execute the Clojure code as a script.","title":"Hacking"},{"location":"#testing","text":"If you've never run boot before, you may have to run it twice: first to bootstrap Clojure and boot itself, and again to download their and WFL's dependencies. The first boot build run will create a ./wfl link to the build.boot file ./wfl starter dev $USER @broadinstitute.org You should eventually receive an humongous email from wfl@broadinstitute.org containing evidence of Zero's adventures. Of course, after boot build , you can also run WFL from its JAR file.","title":"Testing"},{"location":"#exomes-in-the-cloud-resources","text":"From Hybrid Selection in the Cloud V1 Clients Google Cloud Storage Client Library (Java) Google Cloud Client Library for Java Diagrams Zamboni Overview Sources /Users/tbl/Broad/zamboni/Client/src/scala/org/broadinstitute/zamboni/client/lightning/clp/Lightning.scala /Users/tbl/Broad/picard-private/src/java/edu/mit/broad/picard/lightning /Users/tbl/Broad/gppipeline-devtools/release client /Users/tbl/Broad/gppipeline-devtools/starter control /picard02:/seq/pipeline/gppipeline-devtools/current/defs/prod.defs","title":"Exomes in the Cloud Resources"},{"location":"deployment/","text":"Deployment of WorkFlow Launcher \u00b6 Make a deployment \u00b6 The WorkFlow Launcher is currently running on a Kubernetes cluster, and the deployment is managed by Helm . In order to make a deployment or upgrade the current deployment manually, you have to make the following preparations: Setup Kubernetes \u00b6 Make sure you have the kubectl command available, while in the Broad network or using the VPN, run a command like the following to set up the connection to the desired cluster: gcloud container clusters get-credentials \\ gotc-dev-shared-us-central1-a --zone us-central1-a \\ --project broad-gotc-dev Run kubectl config get-contexts to make sure you are connected to the right cluster. Later you could run kubectl config use-context $CONTEXT_NAME to switch (back) to other contexts. Setup Helm \u00b6 Install Helm, please follow the Helm instructions or simply try brew install helm , assuming you have HomeBrew installed on your MacOS. Run: helm repo add gotc-charts https://broadinstitute.github.io/gotc-helm-repo/ to add gotc\u2019s Helm repo to your Helm. Note gotc-charts is just an alias, you could give it any name you want. Run: helm repo update to make the local cached charts update-to-date to the remote repo and also run: helm repo list to check the list of repo you have connected to anytime you want. In the Broad network or on VPN and your kubectl is setup to connect to the right cluster, run: helm list to check the current deployments that are managed by Helm. Build \u00b6 Build a new WFL jar it you want one. boot build Note the :version printed by the build, or run this to recover the :version string. java -jar ./target/wfl-*.tar version The version string should look something like this. 2020 -06-24t16-41-44z Compose an docker image tag for later use. IMAGE = 2020 -06-24t16-41-44z- $USER Dockerize \u00b6 Compose a new docker image if you want one. Tag it with some helpful name, then push it to DockerHub so Kubernetes can find it. docker build -t broadinstitute/workflow-launcher-api: $IMAGE . docker build -t broadinstitute/workflow-launcher-ui: $IMAGE ui/. docker push broadinstitute/workflow-launcher-api: $IMAGE docker push broadinstitute/workflow-launcher-ui: $IMAGE You should see it listed here. https://hub.docker.com/repository/docker/broadinstitute/workflow-launcher-api Clone the gotc-deploy repository. \u00b6 git clone --branch <the branch you want> \\ https://github.com/broadinstitute/gotc-deploy.git Render the chart \u00b6 Render the wfl-values.yaml file. docker run -i --rm -v \" $( pwd ) \" :/working \\ -v \" $HOME \" /.vault-token:/root/.vault-token \\ -e WFL_VERSION = latest \\ broadinstitute/dsde-toolbox:dev \\ /usr/local/bin/render-ctmpls.sh \\ -k ./gotc-deploy/deploy/gotc-dev/helm/wfl-values.yaml.ctmpl Note: That command always fails, so look at ./gotc-deploy/deploy/gotc-dev/helm/wfl-values.yaml to verify that the values substituted into the template are correct. Note: Some of the values in these YML files contain credentials or sensitive information, so DO NOT check them in to your version control system or make them public!!! Deploy \u00b6 Run this to upgrade a deployment release. helm upgrade wfl-k8s gotc-charts/wfl --install \\ -f ./gotc-deploy/deploy/gotc-dev/helm/wfl-values.yaml That doesn't recreate ingress resources and so on, but it will --install everything necessary if a release is not deployed already. Run kubectl get pods to review your deployments. # kubectl get pods NAME READY STATUS AGE cromwell-auth-gotc-dev-authproxy-6f76598bb4-snpp6 1 /1 Running 11d hjf-test-authproxy-78758bbb6b-7lnfg 2 /2 Running 42d wfl-k8s-6948f6566d-2bdcp 2 /2 Running 2d5h # Run kubectl logs wfl-k8s-6948f6566d-2bdcp workflow-launcher-api to view the logs for the WFL-API server. Run kubectl logs wfl-k8s-6948f6566d-2bdcp workflow-launcher-ui to view the logs for the WFL-UI front end. Testing the deployment locally \u00b6 Similar to the above process, but in addition to the above preparations, you also have to: Make sure you have a relatively new version of Docker client installed, which has Docker -> Preferences -> Kubernetes -> Enable Kubernetes. Turn on that options and restart your Docker client. Run kubectl config use-context $CONTEXT_NAME to the Docker-built-in Kubernetes context, usually it should be called something like docker-for-mac . Similar to how you have setup the Helm charts, you could run: helm install gotc-dev gotc-charts/authproxy -f custom-authvals.yaml Or helm upgrade gotc-dev gotc-charts/authproxy -f custom-authvals.yaml To install or upgrade the deployments on your local cluster. One thing to note is that you CANNOT create an ingress resource locally, so it\u2019s important to disable the ingress creation in your custom values YML files.","title":"Deployment with Kubernetes"},{"location":"deployment/#deployment-of-workflow-launcher","text":"","title":"Deployment of WorkFlow Launcher"},{"location":"deployment/#make-a-deployment","text":"The WorkFlow Launcher is currently running on a Kubernetes cluster, and the deployment is managed by Helm . In order to make a deployment or upgrade the current deployment manually, you have to make the following preparations:","title":"Make a deployment"},{"location":"deployment/#setup-kubernetes","text":"Make sure you have the kubectl command available, while in the Broad network or using the VPN, run a command like the following to set up the connection to the desired cluster: gcloud container clusters get-credentials \\ gotc-dev-shared-us-central1-a --zone us-central1-a \\ --project broad-gotc-dev Run kubectl config get-contexts to make sure you are connected to the right cluster. Later you could run kubectl config use-context $CONTEXT_NAME to switch (back) to other contexts.","title":"Setup Kubernetes"},{"location":"deployment/#setup-helm","text":"Install Helm, please follow the Helm instructions or simply try brew install helm , assuming you have HomeBrew installed on your MacOS. Run: helm repo add gotc-charts https://broadinstitute.github.io/gotc-helm-repo/ to add gotc\u2019s Helm repo to your Helm. Note gotc-charts is just an alias, you could give it any name you want. Run: helm repo update to make the local cached charts update-to-date to the remote repo and also run: helm repo list to check the list of repo you have connected to anytime you want. In the Broad network or on VPN and your kubectl is setup to connect to the right cluster, run: helm list to check the current deployments that are managed by Helm.","title":"Setup Helm"},{"location":"deployment/#build","text":"Build a new WFL jar it you want one. boot build Note the :version printed by the build, or run this to recover the :version string. java -jar ./target/wfl-*.tar version The version string should look something like this. 2020 -06-24t16-41-44z Compose an docker image tag for later use. IMAGE = 2020 -06-24t16-41-44z- $USER","title":"Build"},{"location":"deployment/#dockerize","text":"Compose a new docker image if you want one. Tag it with some helpful name, then push it to DockerHub so Kubernetes can find it. docker build -t broadinstitute/workflow-launcher-api: $IMAGE . docker build -t broadinstitute/workflow-launcher-ui: $IMAGE ui/. docker push broadinstitute/workflow-launcher-api: $IMAGE docker push broadinstitute/workflow-launcher-ui: $IMAGE You should see it listed here. https://hub.docker.com/repository/docker/broadinstitute/workflow-launcher-api","title":"Dockerize"},{"location":"deployment/#clone-the-gotc-deploy-repository","text":"git clone --branch <the branch you want> \\ https://github.com/broadinstitute/gotc-deploy.git","title":"Clone the gotc-deploy repository."},{"location":"deployment/#render-the-chart","text":"Render the wfl-values.yaml file. docker run -i --rm -v \" $( pwd ) \" :/working \\ -v \" $HOME \" /.vault-token:/root/.vault-token \\ -e WFL_VERSION = latest \\ broadinstitute/dsde-toolbox:dev \\ /usr/local/bin/render-ctmpls.sh \\ -k ./gotc-deploy/deploy/gotc-dev/helm/wfl-values.yaml.ctmpl Note: That command always fails, so look at ./gotc-deploy/deploy/gotc-dev/helm/wfl-values.yaml to verify that the values substituted into the template are correct. Note: Some of the values in these YML files contain credentials or sensitive information, so DO NOT check them in to your version control system or make them public!!!","title":"Render the chart"},{"location":"deployment/#deploy","text":"Run this to upgrade a deployment release. helm upgrade wfl-k8s gotc-charts/wfl --install \\ -f ./gotc-deploy/deploy/gotc-dev/helm/wfl-values.yaml That doesn't recreate ingress resources and so on, but it will --install everything necessary if a release is not deployed already. Run kubectl get pods to review your deployments. # kubectl get pods NAME READY STATUS AGE cromwell-auth-gotc-dev-authproxy-6f76598bb4-snpp6 1 /1 Running 11d hjf-test-authproxy-78758bbb6b-7lnfg 2 /2 Running 42d wfl-k8s-6948f6566d-2bdcp 2 /2 Running 2d5h # Run kubectl logs wfl-k8s-6948f6566d-2bdcp workflow-launcher-api to view the logs for the WFL-API server. Run kubectl logs wfl-k8s-6948f6566d-2bdcp workflow-launcher-ui to view the logs for the WFL-UI front end.","title":"Deploy"},{"location":"deployment/#testing-the-deployment-locally","text":"Similar to the above process, but in addition to the above preparations, you also have to: Make sure you have a relatively new version of Docker client installed, which has Docker -> Preferences -> Kubernetes -> Enable Kubernetes. Turn on that options and restart your Docker client. Run kubectl config use-context $CONTEXT_NAME to the Docker-built-in Kubernetes context, usually it should be called something like docker-for-mac . Similar to how you have setup the Helm charts, you could run: helm install gotc-dev gotc-charts/authproxy -f custom-authvals.yaml Or helm upgrade gotc-dev gotc-charts/authproxy -f custom-authvals.yaml To install or upgrade the deployments on your local cluster. One thing to note is that you CANNOT create an ingress resource locally, so it\u2019s important to disable the ingress creation in your custom values YML files.","title":"Testing the deployment locally"},{"location":"dev-process/","text":"Development Process \u00b6 This is a development process we are tying to standardize within the team, and encourage ourselves to follow in most cases. Summary \u00b6 We always make feature branches from master , make pull requests, ask for reviews and merge back to master on Github. Currently we always deploy the latest master to the development environment after merge, but in the future we might need to cut off releases on master and deploy the released versions to the server only. It's not decided yet. Steps \u00b6 Clone the repo git@github.com:broadinstitute/wfl.git Start from the latest copy of the remote master git checkout master git pull origin master Create a feature branch It is highly recommend that you follow the naming convention shown below so JIRA could pick up the branch and link it to our JIRA board. git checkout -b tbl/GH-666-feature-branch-something Start your work, add and commit your changes git add \"README.md\" git commit -m \"Update the readme file.\" [Optional] Rebase onto lastet master: only if you want to get updates from the master git checkout master git pull origin master git checkout tbl/GH-666-feature-branch-something git rebase master alternatively, you could use the following commands without switching branches: git checkout tbl/GH-666-feature-branch-something git fetch origin master git merge master Push branch to Github in the early stage of your development (recommended): git push --set-upstream origin tbl/GH-666-feature-branch-something Create the pull request on Github UI. Be sure to fill out the PR description following the PR template instructions. If the PR is still in development, make sure use the dropdown menu and choose Create draft pull request If the PR is ready for review, click Create pull request . Look for a reviewer in the team. Address reviewer comments with more commits. Receive approval from reviewers. Make sure build the backend code at least once with: boot build Merge the PR. The feature branch will be automatically cleaned up. [Temporary] Fetch the lastest master branch again and deploy it to dev server. git checkout master git pull origin master boot deploy you might need to login to vault and google by the following commands before you want to deploy: vault auth -method=github token=$(cat ~/.github-token) gcloud auth login Note: this action might interfere other people's work that is under QA, please always coordinate before you do this! Tips \u00b6 Here are some tips for WFL development. Some of this advice might help when testing Liquibase migration or other changes that affect WFL's Postgres database. migrating a database \u00b6 To change WFL's Postgres database schema, add a changeset XML file in the database/changesets directory. Name the file for a recent or the current date followed by something describing the change. That will ensure that the changesets list in the order in which they apply. Note that the id and logicalFilePath attributes are derived from the changeset's file name. Then add the changeset file to the database/changlog.xml file. Test the changes against a local scratch database . See the next section for suggestions. debugging JDBC SQL \u00b6 Something seems to swallow SQL exceptions raised by Postgres and the JDBC library. Wrap suspect clojure.java.jdbc calls in zero.util/do-or-nil to ensure that any exceptions show up in the server logs. debugging API specs \u00b6 If an API references an undefined spec, HTTP requests and responses might silently fail or the Swagger page will fail to render. Check the clojure.spec.alpha/def s in zero.api.routes for typos before tearing your hair out. hacking a scratch database \u00b6 You can test against a local Postgres before running Liquibase or SQL against a shared database in gotc-dev or gasp production. First install Postgres locally. brew install postgresql@11 You need version 11 because that is what Google's hosted service supports, and there are differences in the SQL syntax. Set \"ZERO_POSTGRES_URL\" to (postgres/zero-db-url :debug) in zero.server/env_variables to redirect the WFL server's database to a local Postgres server. With that hack in place, running ./ops/server.sh (or however you launch a local WFL server) will connect the server to a local Postgres. Now any changes to WFL state will affect only your local database. That includes running Liquibase, so don't forget to reset :debug to env before deploying your changes after merging a PR. Rich comment in zero.service.postgres \u00b6 There is also a \"Rich comment\" at the end of zero.service.postgres with some useful hacks for debugging Postgres and Liquibase operations during development. The 0-arity zero.service.postgres/run-liquibase function runs the standard liquibase scripts against the local :debug database. Similarly, zero.service.postgres/reset-debug-db resets the local :debug Postgres server to its initial state. Do not run something like reset-debug-db against a Postgres server deployed in the cloud or with an actual environment set. Test \u00b6 We implement tests under the test/ root directory and use the kaocha test runner. Test suites use a -test namespace suffix. You can pass extra command line arguments to kaocha . For example, to run a specific test point: clojure -A:test --focus my.integration-test/test-foo-works You can see the full list of options with the following: clojure -A:integration --help","title":"Development Process"},{"location":"dev-process/#development-process","text":"This is a development process we are tying to standardize within the team, and encourage ourselves to follow in most cases.","title":"Development Process"},{"location":"dev-process/#summary","text":"We always make feature branches from master , make pull requests, ask for reviews and merge back to master on Github. Currently we always deploy the latest master to the development environment after merge, but in the future we might need to cut off releases on master and deploy the released versions to the server only. It's not decided yet.","title":"Summary"},{"location":"dev-process/#steps","text":"Clone the repo git@github.com:broadinstitute/wfl.git Start from the latest copy of the remote master git checkout master git pull origin master Create a feature branch It is highly recommend that you follow the naming convention shown below so JIRA could pick up the branch and link it to our JIRA board. git checkout -b tbl/GH-666-feature-branch-something Start your work, add and commit your changes git add \"README.md\" git commit -m \"Update the readme file.\" [Optional] Rebase onto lastet master: only if you want to get updates from the master git checkout master git pull origin master git checkout tbl/GH-666-feature-branch-something git rebase master alternatively, you could use the following commands without switching branches: git checkout tbl/GH-666-feature-branch-something git fetch origin master git merge master Push branch to Github in the early stage of your development (recommended): git push --set-upstream origin tbl/GH-666-feature-branch-something Create the pull request on Github UI. Be sure to fill out the PR description following the PR template instructions. If the PR is still in development, make sure use the dropdown menu and choose Create draft pull request If the PR is ready for review, click Create pull request . Look for a reviewer in the team. Address reviewer comments with more commits. Receive approval from reviewers. Make sure build the backend code at least once with: boot build Merge the PR. The feature branch will be automatically cleaned up. [Temporary] Fetch the lastest master branch again and deploy it to dev server. git checkout master git pull origin master boot deploy you might need to login to vault and google by the following commands before you want to deploy: vault auth -method=github token=$(cat ~/.github-token) gcloud auth login Note: this action might interfere other people's work that is under QA, please always coordinate before you do this!","title":"Steps"},{"location":"dev-process/#tips","text":"Here are some tips for WFL development. Some of this advice might help when testing Liquibase migration or other changes that affect WFL's Postgres database.","title":"Tips"},{"location":"dev-process/#migrating-a-database","text":"To change WFL's Postgres database schema, add a changeset XML file in the database/changesets directory. Name the file for a recent or the current date followed by something describing the change. That will ensure that the changesets list in the order in which they apply. Note that the id and logicalFilePath attributes are derived from the changeset's file name. Then add the changeset file to the database/changlog.xml file. Test the changes against a local scratch database . See the next section for suggestions.","title":"migrating a database"},{"location":"dev-process/#debugging-jdbc-sql","text":"Something seems to swallow SQL exceptions raised by Postgres and the JDBC library. Wrap suspect clojure.java.jdbc calls in zero.util/do-or-nil to ensure that any exceptions show up in the server logs.","title":"debugging JDBC SQL"},{"location":"dev-process/#debugging-api-specs","text":"If an API references an undefined spec, HTTP requests and responses might silently fail or the Swagger page will fail to render. Check the clojure.spec.alpha/def s in zero.api.routes for typos before tearing your hair out.","title":"debugging API specs"},{"location":"dev-process/#hacking-a-scratch-database","text":"You can test against a local Postgres before running Liquibase or SQL against a shared database in gotc-dev or gasp production. First install Postgres locally. brew install postgresql@11 You need version 11 because that is what Google's hosted service supports, and there are differences in the SQL syntax. Set \"ZERO_POSTGRES_URL\" to (postgres/zero-db-url :debug) in zero.server/env_variables to redirect the WFL server's database to a local Postgres server. With that hack in place, running ./ops/server.sh (or however you launch a local WFL server) will connect the server to a local Postgres. Now any changes to WFL state will affect only your local database. That includes running Liquibase, so don't forget to reset :debug to env before deploying your changes after merging a PR.","title":"hacking a scratch database"},{"location":"dev-process/#rich-comment-in-zeroservicepostgres","text":"There is also a \"Rich comment\" at the end of zero.service.postgres with some useful hacks for debugging Postgres and Liquibase operations during development. The 0-arity zero.service.postgres/run-liquibase function runs the standard liquibase scripts against the local :debug database. Similarly, zero.service.postgres/reset-debug-db resets the local :debug Postgres server to its initial state. Do not run something like reset-debug-db against a Postgres server deployed in the cloud or with an actual environment set.","title":"Rich comment in zero.service.postgres"},{"location":"dev-process/#test","text":"We implement tests under the test/ root directory and use the kaocha test runner. Test suites use a -test namespace suffix. You can pass extra command line arguments to kaocha . For example, to run a specific test point: clojure -A:test --focus my.integration-test/test-foo-works You can see the full list of options with the following: clojure -A:integration --help","title":"Test"},{"location":"frontend/","text":"Workflow Launcher UI \u00b6 This is the front-end interface of the Workflow Launcher. It is a VueJS based SPA (Single-Page Application) that works as an ordinary client of the Workflow Launcher Server. You could find its position in the following Diagram: Structure \u00b6 ui \u251c\u2500\u2500 README.md \u251c\u2500\u2500 babel.config.js \u251c\u2500\u2500 dist/ \u251c\u2500\u2500 node_modules/ \u251c\u2500\u2500 package-lock.json \u251c\u2500\u2500 package.json \u251c\u2500\u2500 public/ \u251c\u2500\u2500 src/ \u2502 \u251c\u2500\u2500 App.vue \u2502 \u251c\u2500\u2500 assets/ \u2502 \u251c\u2500\u2500 components/ \u2502 \u251c\u2500\u2500 main.js \u2502 \u251c\u2500\u2500 plugins/ \u2502 \u251c\u2500\u2500 router/ \u2502 \u251c\u2500\u2500 store/ \u2502 \u2514\u2500\u2500 views/ \u2514\u2500\u2500 vue.config.js In the above structure: dist/ folder hosts the built target of the application. node_modules/ hosts the installed JS dependencies and libraries. They are ignored by git. public/ hosts the template static index HTML file that will be injected. package-*.json files hold various metadata relevant to the project. This file is used to give information to npm that allows it to identify the project as well as handle the project's dependencies. src/ folder hosts the source code of the UI application: App.vue is the main Vue component and it glues all other components together. components/ hosts all reusable Vue components. main.js helps inject some project-wide tools and plugins such as vue-router or vuetify and make them available to all sub components. plugins/ holds plugin components' settgins files. router contains files that register the internal routing table for UI. store/ hosts state files and functions that used by vuex . views/ holds different views or \"pages\" for the single-page application. The views consume the re-usable components here. vue.config.js contains settings for the Vue applicationm, such as the proxy table for local development. Project setup \u00b6 Note: for any of the following commands that uses npm , if you prefer to run from the root directory of the WFL repo instead of running from within zero/ui , please be sure to append --prefix=ui to the npm command you run. Install dependencies \u00b6 When you first clone the repo, run: npm install to install the necessary dependencies. Compiles and hot-reloads for development \u00b6 npm run serve Compiles and minifies for production \u00b6 npm run build Lints and fixes files \u00b6 npm run lint Development \u00b6 It makes your life easier if you start the local server while developing on the ui, since you could preview the live changes in your browser. Styles \u00b6 This project follows and uses Material Design, especilly the Vue implementation of Material Design framework: Vuetify. Please check their docs before adding anything to the front-end. Add new components or views \u00b6 The development process is pretty straightforward as the above structure diagram shows. Usually you just need to create a new re-usable component under ui/src/components , which follows the Vue file format: <template> <!-- your HTML and template code --> </template> <script> // your JavaScript code following Vue- // component standards </script> <style> /* your CSS styles */ </style> You could either put the component you created in the App.vue directly, or use it in the views under views/ . Note the views files are also components, except they are designed to be specific not reusable. UI states \u00b6 Sometimes it's inevitable to store some states for components of UI to better control their behaviors, the state files should be added to store/modules/ and get registered in store/index.js . Single Page Routing \u00b6 The SPA application is achieved by an internal routing in UI. This is controlled by the routing tables in router/ . More refernces \u00b6 VueJS: https://vuejs.org/v2/guide/ Vuetify: https://vuetifyjs.com/en/ Vue-router: https://router.vuejs.org/","title":"Frontend"},{"location":"frontend/#workflow-launcher-ui","text":"This is the front-end interface of the Workflow Launcher. It is a VueJS based SPA (Single-Page Application) that works as an ordinary client of the Workflow Launcher Server. You could find its position in the following Diagram:","title":"Workflow Launcher UI"},{"location":"frontend/#structure","text":"ui \u251c\u2500\u2500 README.md \u251c\u2500\u2500 babel.config.js \u251c\u2500\u2500 dist/ \u251c\u2500\u2500 node_modules/ \u251c\u2500\u2500 package-lock.json \u251c\u2500\u2500 package.json \u251c\u2500\u2500 public/ \u251c\u2500\u2500 src/ \u2502 \u251c\u2500\u2500 App.vue \u2502 \u251c\u2500\u2500 assets/ \u2502 \u251c\u2500\u2500 components/ \u2502 \u251c\u2500\u2500 main.js \u2502 \u251c\u2500\u2500 plugins/ \u2502 \u251c\u2500\u2500 router/ \u2502 \u251c\u2500\u2500 store/ \u2502 \u2514\u2500\u2500 views/ \u2514\u2500\u2500 vue.config.js In the above structure: dist/ folder hosts the built target of the application. node_modules/ hosts the installed JS dependencies and libraries. They are ignored by git. public/ hosts the template static index HTML file that will be injected. package-*.json files hold various metadata relevant to the project. This file is used to give information to npm that allows it to identify the project as well as handle the project's dependencies. src/ folder hosts the source code of the UI application: App.vue is the main Vue component and it glues all other components together. components/ hosts all reusable Vue components. main.js helps inject some project-wide tools and plugins such as vue-router or vuetify and make them available to all sub components. plugins/ holds plugin components' settgins files. router contains files that register the internal routing table for UI. store/ hosts state files and functions that used by vuex . views/ holds different views or \"pages\" for the single-page application. The views consume the re-usable components here. vue.config.js contains settings for the Vue applicationm, such as the proxy table for local development.","title":"Structure"},{"location":"frontend/#project-setup","text":"Note: for any of the following commands that uses npm , if you prefer to run from the root directory of the WFL repo instead of running from within zero/ui , please be sure to append --prefix=ui to the npm command you run.","title":"Project setup"},{"location":"frontend/#install-dependencies","text":"When you first clone the repo, run: npm install to install the necessary dependencies.","title":"Install dependencies"},{"location":"frontend/#compiles-and-hot-reloads-for-development","text":"npm run serve","title":"Compiles and hot-reloads for development"},{"location":"frontend/#compiles-and-minifies-for-production","text":"npm run build","title":"Compiles and minifies for production"},{"location":"frontend/#lints-and-fixes-files","text":"npm run lint","title":"Lints and fixes files"},{"location":"frontend/#development","text":"It makes your life easier if you start the local server while developing on the ui, since you could preview the live changes in your browser.","title":"Development"},{"location":"frontend/#styles","text":"This project follows and uses Material Design, especilly the Vue implementation of Material Design framework: Vuetify. Please check their docs before adding anything to the front-end.","title":"Styles"},{"location":"frontend/#add-new-components-or-views","text":"The development process is pretty straightforward as the above structure diagram shows. Usually you just need to create a new re-usable component under ui/src/components , which follows the Vue file format: <template> <!-- your HTML and template code --> </template> <script> // your JavaScript code following Vue- // component standards </script> <style> /* your CSS styles */ </style> You could either put the component you created in the App.vue directly, or use it in the views under views/ . Note the views files are also components, except they are designed to be specific not reusable.","title":"Add new components or views"},{"location":"frontend/#ui-states","text":"Sometimes it's inevitable to store some states for components of UI to better control their behaviors, the state files should be added to store/modules/ and get registered in store/index.js .","title":"UI states"},{"location":"frontend/#single-page-routing","text":"The SPA application is achieved by an internal routing in UI. This is controlled by the routing tables in router/ .","title":"Single Page Routing"},{"location":"frontend/#more-refernces","text":"VueJS: https://vuejs.org/v2/guide/ Vuetify: https://vuetifyjs.com/en/ Vue-router: https://router.vuejs.org/","title":"More refernces"},{"location":"server/","text":"WorkFlow Launcher Server \u00b6 We now have the basics of WorkFlow Launcher running as a server in Google App Engine (GAE). Deploy to Google App Engine \u00b6 To build and deploy WFL, run ./ops/deploy.sh . It's Google Credentials page is here. https://console.developers.google.com/apis/credentials?project=broad-gotc-dev WFL server features \u00b6 The WFL server doesn't do much now. It can configure its secrets and deploy itself. It can authenticate to Google using OAuth2. It can serve authorized and unauthorized routes. This is the application server's home URL. https://zero-dot-broad-gotc-dev.appspot.com/ Create a workload \u00b6 Defining a workload requires these top-level parameters. Parameter Type project text cromwell URL pipeline pipeline input URL prefix output URL prefix The parameters are used this way. The project is just some text to identify a researcher, billing entity, or cost object responsible for the workload. The cromwell URL specifies the Cromwell instance to service the workload . The pipeline enumeration implicitly identifies a data schema for the inputs to and outputs from the workload. You can think of it as the kind of workflow specified for the workload. People sometimes refer to this as the tag in that it is a well-known name for a Cromwell pipeline defined in WDL. You might also think of pipeline as the external or official name of a WFL processing module. ExternalWholeGenomeReprocessing is the only pipeline currently defined. Pipelines differ in their processing and results, so each pipeline requires a different set of inputs for each workflow in a workload. All input files for the workload share the input URL prefix, and all output files from the workload share the output URL prefix. These should be the longest common prefix shared by all the files to keep discovery and monitoring efficient. An ExternalWholeGenomeReprocessing workload \u00b6 An ExternalWholeGenomeReprocessing workload requires the following inputs for each workflow in the workload. input_cram sample_name base_file_name final_gvcf_base_name unmapped_bam_suffix The input_cram is the path to a file relative to the input URL prefix in the workload definition above. The input_cram contains the sample_name but we break it out into a separate input here to avoid parsing every (often large) CRAM file. The base_file_name is used to name result files. The workflow uses the base_file_name together with one or more filetype suffixes to name intermediate and output files. It is usually just the leaf name of input_cram with the .cram extension removed. The final_gvcf_base_name is the root of the leaf name of the pathname of the final VCF. The final VCF will have some variant of a .vcf suffix added by the workflow WDL. It is common for base_file_name and final_gvcf_base_name to be identical to sample_name . If no base_file_name is specified for any workflow in a workload, base_file_name defaults to sample_name . Likewise, if no final_gvcf_base_name is specified for any workflow in a workload, then final_gvcf_base_name also defaults to sample_name . It is used to recover the filename resulting from re-aligning a reverted CRAM file. The unmapped_bam_suffix is almost always .unmapped.bam , so that is its default value unless it is specified. URIs served \u00b6 The following URIs work now. Home ( / ) : Home replies with Authorized! when authorized. Otherwise it redirects to the Status page. Status ( /status ) : Status is an uauthorized endpoint that responds with \"OK\". Version ( /version ) : Version is an uauthorized endpoint that responds with the version currently deployed. OAuth Launch ( /auth/google ) : Launch begins the OAuth2 call chain to authenticate using your Google credentials. Environments ( /api/v1/environments ) : Environments returns WFL's environment tree as JSON when authorized. Environments redirects to Status when unauthorized. Starting WFL server for local development \u00b6 Run ./ops/server.sh from the command line. There is a wrap-reload-for-development-only handler wrapper commented out on the app defined in the server.clj file. When it is compiled in, source code changes that you make will be reloaded into the running server. As its name implies, you should comment it out before deploying WFL.","title":"Server"},{"location":"server/#workflow-launcher-server","text":"We now have the basics of WorkFlow Launcher running as a server in Google App Engine (GAE).","title":"WorkFlow Launcher Server"},{"location":"server/#deploy-to-google-app-engine","text":"To build and deploy WFL, run ./ops/deploy.sh . It's Google Credentials page is here. https://console.developers.google.com/apis/credentials?project=broad-gotc-dev","title":"Deploy to Google App Engine"},{"location":"server/#wfl-server-features","text":"The WFL server doesn't do much now. It can configure its secrets and deploy itself. It can authenticate to Google using OAuth2. It can serve authorized and unauthorized routes. This is the application server's home URL. https://zero-dot-broad-gotc-dev.appspot.com/","title":"WFL server features"},{"location":"server/#create-a-workload","text":"Defining a workload requires these top-level parameters. Parameter Type project text cromwell URL pipeline pipeline input URL prefix output URL prefix The parameters are used this way. The project is just some text to identify a researcher, billing entity, or cost object responsible for the workload. The cromwell URL specifies the Cromwell instance to service the workload . The pipeline enumeration implicitly identifies a data schema for the inputs to and outputs from the workload. You can think of it as the kind of workflow specified for the workload. People sometimes refer to this as the tag in that it is a well-known name for a Cromwell pipeline defined in WDL. You might also think of pipeline as the external or official name of a WFL processing module. ExternalWholeGenomeReprocessing is the only pipeline currently defined. Pipelines differ in their processing and results, so each pipeline requires a different set of inputs for each workflow in a workload. All input files for the workload share the input URL prefix, and all output files from the workload share the output URL prefix. These should be the longest common prefix shared by all the files to keep discovery and monitoring efficient.","title":"Create a workload"},{"location":"server/#an-externalwholegenomereprocessing-workload","text":"An ExternalWholeGenomeReprocessing workload requires the following inputs for each workflow in the workload. input_cram sample_name base_file_name final_gvcf_base_name unmapped_bam_suffix The input_cram is the path to a file relative to the input URL prefix in the workload definition above. The input_cram contains the sample_name but we break it out into a separate input here to avoid parsing every (often large) CRAM file. The base_file_name is used to name result files. The workflow uses the base_file_name together with one or more filetype suffixes to name intermediate and output files. It is usually just the leaf name of input_cram with the .cram extension removed. The final_gvcf_base_name is the root of the leaf name of the pathname of the final VCF. The final VCF will have some variant of a .vcf suffix added by the workflow WDL. It is common for base_file_name and final_gvcf_base_name to be identical to sample_name . If no base_file_name is specified for any workflow in a workload, base_file_name defaults to sample_name . Likewise, if no final_gvcf_base_name is specified for any workflow in a workload, then final_gvcf_base_name also defaults to sample_name . It is used to recover the filename resulting from re-aligning a reverted CRAM file. The unmapped_bam_suffix is almost always .unmapped.bam , so that is its default value unless it is specified.","title":"An ExternalWholeGenomeReprocessing workload"},{"location":"server/#uris-served","text":"The following URIs work now. Home ( / ) : Home replies with Authorized! when authorized. Otherwise it redirects to the Status page. Status ( /status ) : Status is an uauthorized endpoint that responds with \"OK\". Version ( /version ) : Version is an uauthorized endpoint that responds with the version currently deployed. OAuth Launch ( /auth/google ) : Launch begins the OAuth2 call chain to authenticate using your Google credentials. Environments ( /api/v1/environments ) : Environments returns WFL's environment tree as JSON when authorized. Environments redirects to Status when unauthorized.","title":"URIs served"},{"location":"server/#starting-wfl-server-for-local-development","text":"Run ./ops/server.sh from the command line. There is a wrap-reload-for-development-only handler wrapper commented out on the app defined in the server.clj file. When it is compiled in, source code changes that you make will be reloaded into the running server. As its name implies, you should comment it out before deploying WFL.","title":"Starting WFL server for local development"},{"location":"terra/","text":"WorkFlow Launcher's Role in Terra \u00b6 Summary \u00b6 The Data Sciences Platform (DSP) is building a new system (around Terra ) for storing and processing biological data. The system design includes a Data Repository where data is stored, and a Methods Repository that executably describes transformations on that data. In the new system, the DSP needs something to fulfill the role that Zamboni currently plays in DSP's current infrastructure to support the Genomics Platform (GP). Zamboni watches various queues for messages describing new data and how to process it. Zamboni interprets those messages to dispatch work to workflow engines (running on the premises or in the cloud) and monitors the progress of those workflows. The Zamboni web UI allows users to track the progress of workflows, and enables Ops engineers to debug problems and resume or restart failed workflows. Zamboni can run workflows on both a local Sun Grid Engine (SGE), and on Cromwell on premises and in the cloud. We think that WFL can fill the role of Zamboni in the new data storage and processing system that DSP is developing now. History \u00b6 WFL began as a project to replace a Zamboni starter , with the old name \"Zero\". A starter is a Zamboni component that brokers messages among the queues that Zamboni watches. It interprets messages queued from a Laboratory Information Management System (LIMS), such as the Mercury web service, and demultiplexes them to other Zamboni queues. Zero was later adapted to manage the reprocessing of the first batch of UK Biobank exomes. It has since been adapted to drive workflows for other projects at the Broad. Zero is unusual in that it usually runs as a stateless command line program without special system privilege, and interfaces with services running both on premises and in Google Cloud. It also manages a processing workload as a set of inputs mapped to outputs instead of tracking the progress of individual sample workflows. A Zero user need only specify a source of inputs, a workflow to run, an execution environment, and an output location. Then each time it is invoked, Zero ensures that workflows are started and retried as needed until an output exists for every input. Zero has recently been adapted again to deploy as a web service under Google App Engine (GAE) though most of the value of Zero is still not available to the server. And now it has the new name WFL. The role of WFL in Terra \u00b6 Diagrams of the new DSP processing system show a WFL service subscribed to event streams from the Data Repository (DR), with interfaces to both the Data and the Method Repositories. The implication is that something notifies WFL of new data in the Data Repository and WFL determines how to process it somehow. WFL then looks up whatever is required from the Method Repository, calls on other services as necessary to process the data and writes the results back to the DR. There is also, presumably, a web UI to track and debug the workflows managed by WFL. Many details are yet to be worked out. WFL Concepts \u00b6 WFL is designed around several novel concepts. Manage workloads instead of workflows. This is the biggest difference between Zamboni and Zero (WFL). Zamboni manages workflows whereas WFL manages workloads . Zamboni's unit of work is the workflow . Zamboni manages each workflow separately. A workflow is a transformation specified in WDL or Scala code that succeeds or fails to produce a result. The input to a workflow and its result may consist of multiple files, but they represent a single unit of work managed by a workflow engine such as Cromwell. Zamboni prepares a new workflow for each message it receives by packaging up the input and submitting it to a workflow engine. It then monitors that workflow and reports on its success or failure. WFL manages a workload , which indirectly comprises multiple workflows. Each workflow maps an input to some output, but WFL generally tracks only the inputs and outputs instead of the workflows themselves. Think of a workload as a set of inputs transformed via a workflow engine into a set of outputs. Call that set of outputs the result set . WFL generally does not care whether any individual workflow succeeds or fails. It merely considers all possible inputs specified by the workload, and looks for inputs whose outputs are missing from the result set. If some input lacks an output in the result set, WFL starts a new workflow to process that input. Note: This characterization is unfair to Zamboni. Zamboni also had to manage multiple workflows before the advent of Cromwell and still does when running workflows on SGE. But WFL can take advantage of Cromwell's job management to simplify its implementation. Specify inputs and outputs by general predicates. Each Zamboni message explicitly specifies an input to be processed. Zamboni then starts a workflow for that input and reports its status. Zamboni reports failure so a user can debug and manually succeed , reconsider, or restart the workflow. The output of a successful workflow is not Zamboni's concern. WFL finds inputs by applying a predicate specified by the user subject to some run-time constraint. Then WFL applies a function to each input to find how that input maps to the result set. Another predicate applied to the input, and its output in the result set, determines whether WFL will launch a workflow on that input. Those predicates and function can be anything expressed in a programming language. The run-time constraint is some strings passed on the command line. Minimize user input and decisions at run time. WFL gathers the predicates and mapping functions described above into a module that also knows how to generate everything a workflow engine needs to launch the workflow to process an input into a result output. That module name is one of a few run-time constraints specified by strings in a web form or on a command line. Further constraints are usually one or two of the following: - a processing environment ( `dev` `prod` `pharma5` ), - a file system directory ( `/seq/tng/tbl/` `file://home/tbl/` ), - a cloud object prefix ( `gs://bucket/folder/` `s3://bucket/` ), - a pathname suffix ( `.cram` `.vcf` ), - a spreadsheet ( or JSON , TSV , CSV , XML ) filename - or a count to limit the scope of a predicate . The module interprets the other constraints, determines which processing environments are allowed, and parses any files named accordingly. Maintain provenance. WFL runs out of a single JARfile built entirely from sources pulled from Git repositories. WFL records the Git commit hashes in the JARfile and adds them to every Cromwell workflow it starts. WFL can also preserve the Cromwell metadata alongside any result files generated by the workflow. Run with minimal privilege. Zamboni runs as a service with system account credentials such as picard . WFL is designed to run as whoever invokes it, such as tbl@broadinstitute.org . WFL fetches the users credentials from the environment when invoked from the command line. WFL requires authentication when running as a server, and constructs a JSON Web Token (JWT) to authorize other services as needed. Limit dependencies. WFL depends on a Java runtime, boot-clj to manage dependencies, Google Cloud SDK to deploy to Google App Engine (GAE). Of course, it also pulls in numerous Clojure and Java libraries at build time, and sources WDL files from the dsde-pipelines repository. A programmer needs only clone the wfl Git repositories, and run boot-clj to bootstrap WFL from source. And boot-clj is a single file: its own installer. Similarly, boot build builds WFL, and boot deploy deploys it to GAE. WFL attempts to be self-describing and self-documenting. It includes monitoring and diagnostic modules for tracking workload progress and debugging failures. WFL server \u00b6 The WFL client is a command-line batch program that a person runs intermittently on a laptop or virtual machine (VM). We are working to port the client functions of WFL to a ubiquitous web service (WFL server) running in Google Cloud. That port requires we solve several problems. State The WFL client is a stateless program that relies on consistent command line arguments to provide the constraints needed to drive the input discovery predicates and so on. Each user runs a separate process that lasts only as long as necessary to complete some stage of a workload. The WFL server is shared among all its users and runs continually. Therefore it requires some kind of data store (a database) to maintain the state of each workload across successive connections from web browsers. We intend to use the hosted Postgres service available to GAE applications for this. This work is already underway (GH-573). Authorization The WFL client assumes it runs in an authenticated context. It can pull credentials from the environment on every invocation that requires authorization to a service. The WFL server will also need to authorize services to run as some authenticated user, but cannot assume the credentials are always available, nor that there is a user present to provide them. WFL can already use OAuth2.0 to authenticate users against an identity provider and use the resulting credentials to build a JWT. It can also derive the bearer token required by most of our authorized services from a JWT. But WFL also needs some secure JWT store, so tokens are available to authorize services even when there is no active user connection. It also needs some mechanism to refresh tokens as they expire to support long-running workloads. Workload specification The user of a WFL service needs some way to specify a workload. A workload may be some set of inputs and the kind of workflow to run on them. A WFL client user now specifies a workload with a module name and a constraint . For example, ukb pharma5 110000 gs://broad-ukb/in/ gs://broad-ukb/out/ means find up to 110000 cloud objects with names prefixed with gs://broad-ukb/in/ , process them in the Cromwell set up for pharma5 , and store their outputs under gs://broad-ukb/out/ somewhere. The ukb module knows how to find .aligned.cram files under the gs://broad-ukb/in/ cloud path and set up the WDL and Cromwell dependencies and options necessary to reprocess them into .cram output files. The ukb module also knows how to find the Cromwell deployed to support pharma5 workloads, how to authorize the user to that Cromwell, and how to read any supporting data from other services. And finally, the ukb module knows how to determine which inputs do not yet have outputs under the gs://broad-ukb/out/ cloud path, and do not have workflows running in the pharma5 Cromwell. In an ideal design, this workload specification would integrate conveniently with the Data Repository's subscription or eventing service. In any case though, WFL needs some interface through which a user can specify what needs to be done. Workload management Workloads need to be started, stopped, and monitored somehow. This implies that there is some way to find active or suspended workloads, and affordances for acting on them. Users need some way to monitor the progress of a workload, and to find and debug workloads encountering unacceptable workflow failures. Monitoring and diagnostic code already exists in various WFL modules, but there is no easy way to use them from a web browser. Service interface WFL should be useful to programs other than web browsers. It is easy to imagine Terra users wanting to query WFL for the status of workloads directly without buggy and tedious screen scraping. WFL should at least export a query endpoint for use by other reporting services as well as its own browser interface. It would be nice to provide a familiar JSON or GraphQL query syntax to other services. Browser interface A browser interface should require little in addition to WFL's service interface. Ideally, one should be able to adapt WFL to new workloads via a browser interface without requiring a redeployment.","title":"WorkFlow Launcher's Role in Terra"},{"location":"terra/#workflow-launchers-role-in-terra","text":"","title":"WorkFlow Launcher's Role in Terra"},{"location":"terra/#summary","text":"The Data Sciences Platform (DSP) is building a new system (around Terra ) for storing and processing biological data. The system design includes a Data Repository where data is stored, and a Methods Repository that executably describes transformations on that data. In the new system, the DSP needs something to fulfill the role that Zamboni currently plays in DSP's current infrastructure to support the Genomics Platform (GP). Zamboni watches various queues for messages describing new data and how to process it. Zamboni interprets those messages to dispatch work to workflow engines (running on the premises or in the cloud) and monitors the progress of those workflows. The Zamboni web UI allows users to track the progress of workflows, and enables Ops engineers to debug problems and resume or restart failed workflows. Zamboni can run workflows on both a local Sun Grid Engine (SGE), and on Cromwell on premises and in the cloud. We think that WFL can fill the role of Zamboni in the new data storage and processing system that DSP is developing now.","title":"Summary"},{"location":"terra/#history","text":"WFL began as a project to replace a Zamboni starter , with the old name \"Zero\". A starter is a Zamboni component that brokers messages among the queues that Zamboni watches. It interprets messages queued from a Laboratory Information Management System (LIMS), such as the Mercury web service, and demultiplexes them to other Zamboni queues. Zero was later adapted to manage the reprocessing of the first batch of UK Biobank exomes. It has since been adapted to drive workflows for other projects at the Broad. Zero is unusual in that it usually runs as a stateless command line program without special system privilege, and interfaces with services running both on premises and in Google Cloud. It also manages a processing workload as a set of inputs mapped to outputs instead of tracking the progress of individual sample workflows. A Zero user need only specify a source of inputs, a workflow to run, an execution environment, and an output location. Then each time it is invoked, Zero ensures that workflows are started and retried as needed until an output exists for every input. Zero has recently been adapted again to deploy as a web service under Google App Engine (GAE) though most of the value of Zero is still not available to the server. And now it has the new name WFL.","title":"History"},{"location":"terra/#the-role-of-wfl-in-terra","text":"Diagrams of the new DSP processing system show a WFL service subscribed to event streams from the Data Repository (DR), with interfaces to both the Data and the Method Repositories. The implication is that something notifies WFL of new data in the Data Repository and WFL determines how to process it somehow. WFL then looks up whatever is required from the Method Repository, calls on other services as necessary to process the data and writes the results back to the DR. There is also, presumably, a web UI to track and debug the workflows managed by WFL. Many details are yet to be worked out.","title":"The role of WFL in Terra"},{"location":"terra/#wfl-concepts","text":"WFL is designed around several novel concepts. Manage workloads instead of workflows. This is the biggest difference between Zamboni and Zero (WFL). Zamboni manages workflows whereas WFL manages workloads . Zamboni's unit of work is the workflow . Zamboni manages each workflow separately. A workflow is a transformation specified in WDL or Scala code that succeeds or fails to produce a result. The input to a workflow and its result may consist of multiple files, but they represent a single unit of work managed by a workflow engine such as Cromwell. Zamboni prepares a new workflow for each message it receives by packaging up the input and submitting it to a workflow engine. It then monitors that workflow and reports on its success or failure. WFL manages a workload , which indirectly comprises multiple workflows. Each workflow maps an input to some output, but WFL generally tracks only the inputs and outputs instead of the workflows themselves. Think of a workload as a set of inputs transformed via a workflow engine into a set of outputs. Call that set of outputs the result set . WFL generally does not care whether any individual workflow succeeds or fails. It merely considers all possible inputs specified by the workload, and looks for inputs whose outputs are missing from the result set. If some input lacks an output in the result set, WFL starts a new workflow to process that input. Note: This characterization is unfair to Zamboni. Zamboni also had to manage multiple workflows before the advent of Cromwell and still does when running workflows on SGE. But WFL can take advantage of Cromwell's job management to simplify its implementation. Specify inputs and outputs by general predicates. Each Zamboni message explicitly specifies an input to be processed. Zamboni then starts a workflow for that input and reports its status. Zamboni reports failure so a user can debug and manually succeed , reconsider, or restart the workflow. The output of a successful workflow is not Zamboni's concern. WFL finds inputs by applying a predicate specified by the user subject to some run-time constraint. Then WFL applies a function to each input to find how that input maps to the result set. Another predicate applied to the input, and its output in the result set, determines whether WFL will launch a workflow on that input. Those predicates and function can be anything expressed in a programming language. The run-time constraint is some strings passed on the command line. Minimize user input and decisions at run time. WFL gathers the predicates and mapping functions described above into a module that also knows how to generate everything a workflow engine needs to launch the workflow to process an input into a result output. That module name is one of a few run-time constraints specified by strings in a web form or on a command line. Further constraints are usually one or two of the following: - a processing environment ( `dev` `prod` `pharma5` ), - a file system directory ( `/seq/tng/tbl/` `file://home/tbl/` ), - a cloud object prefix ( `gs://bucket/folder/` `s3://bucket/` ), - a pathname suffix ( `.cram` `.vcf` ), - a spreadsheet ( or JSON , TSV , CSV , XML ) filename - or a count to limit the scope of a predicate . The module interprets the other constraints, determines which processing environments are allowed, and parses any files named accordingly. Maintain provenance. WFL runs out of a single JARfile built entirely from sources pulled from Git repositories. WFL records the Git commit hashes in the JARfile and adds them to every Cromwell workflow it starts. WFL can also preserve the Cromwell metadata alongside any result files generated by the workflow. Run with minimal privilege. Zamboni runs as a service with system account credentials such as picard . WFL is designed to run as whoever invokes it, such as tbl@broadinstitute.org . WFL fetches the users credentials from the environment when invoked from the command line. WFL requires authentication when running as a server, and constructs a JSON Web Token (JWT) to authorize other services as needed. Limit dependencies. WFL depends on a Java runtime, boot-clj to manage dependencies, Google Cloud SDK to deploy to Google App Engine (GAE). Of course, it also pulls in numerous Clojure and Java libraries at build time, and sources WDL files from the dsde-pipelines repository. A programmer needs only clone the wfl Git repositories, and run boot-clj to bootstrap WFL from source. And boot-clj is a single file: its own installer. Similarly, boot build builds WFL, and boot deploy deploys it to GAE. WFL attempts to be self-describing and self-documenting. It includes monitoring and diagnostic modules for tracking workload progress and debugging failures.","title":"WFL Concepts"},{"location":"terra/#wfl-server","text":"The WFL client is a command-line batch program that a person runs intermittently on a laptop or virtual machine (VM). We are working to port the client functions of WFL to a ubiquitous web service (WFL server) running in Google Cloud. That port requires we solve several problems. State The WFL client is a stateless program that relies on consistent command line arguments to provide the constraints needed to drive the input discovery predicates and so on. Each user runs a separate process that lasts only as long as necessary to complete some stage of a workload. The WFL server is shared among all its users and runs continually. Therefore it requires some kind of data store (a database) to maintain the state of each workload across successive connections from web browsers. We intend to use the hosted Postgres service available to GAE applications for this. This work is already underway (GH-573). Authorization The WFL client assumes it runs in an authenticated context. It can pull credentials from the environment on every invocation that requires authorization to a service. The WFL server will also need to authorize services to run as some authenticated user, but cannot assume the credentials are always available, nor that there is a user present to provide them. WFL can already use OAuth2.0 to authenticate users against an identity provider and use the resulting credentials to build a JWT. It can also derive the bearer token required by most of our authorized services from a JWT. But WFL also needs some secure JWT store, so tokens are available to authorize services even when there is no active user connection. It also needs some mechanism to refresh tokens as they expire to support long-running workloads. Workload specification The user of a WFL service needs some way to specify a workload. A workload may be some set of inputs and the kind of workflow to run on them. A WFL client user now specifies a workload with a module name and a constraint . For example, ukb pharma5 110000 gs://broad-ukb/in/ gs://broad-ukb/out/ means find up to 110000 cloud objects with names prefixed with gs://broad-ukb/in/ , process them in the Cromwell set up for pharma5 , and store their outputs under gs://broad-ukb/out/ somewhere. The ukb module knows how to find .aligned.cram files under the gs://broad-ukb/in/ cloud path and set up the WDL and Cromwell dependencies and options necessary to reprocess them into .cram output files. The ukb module also knows how to find the Cromwell deployed to support pharma5 workloads, how to authorize the user to that Cromwell, and how to read any supporting data from other services. And finally, the ukb module knows how to determine which inputs do not yet have outputs under the gs://broad-ukb/out/ cloud path, and do not have workflows running in the pharma5 Cromwell. In an ideal design, this workload specification would integrate conveniently with the Data Repository's subscription or eventing service. In any case though, WFL needs some interface through which a user can specify what needs to be done. Workload management Workloads need to be started, stopped, and monitored somehow. This implies that there is some way to find active or suspended workloads, and affordances for acting on them. Users need some way to monitor the progress of a workload, and to find and debug workloads encountering unacceptable workflow failures. Monitoring and diagnostic code already exists in various WFL modules, but there is no easy way to use them from a web browser. Service interface WFL should be useful to programs other than web browsers. It is easy to imagine Terra users wanting to query WFL for the status of workloads directly without buggy and tedious screen scraping. WFL should at least export a query endpoint for use by other reporting services as well as its own browser interface. It would be nice to provide a familiar JSON or GraphQL query syntax to other services. Browser interface A browser interface should require little in addition to WFL's service interface. Ideally, one should be able to adapt WFL to new workloads via a browser interface without requiring a redeployment.","title":"WFL server"}]}