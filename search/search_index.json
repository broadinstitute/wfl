{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to WorkFlow Launcher \u00b6 Overview \u00b6 WorkFlow Launcher (WFL) is a workload manager. For example, a workload could be a set of WGS samples to be reprocessed in a given project/bucket, the workflow is the processing of an individual sample in that workload running WGS reprocessing. It runs as you, with your credentials, from your laptop, and communicates with other services as necessary to manage a workload. It can also be deployed to run as a service in the cloud. For more on Workflow Launcher's role in the Terra infrastructure see Workflow Launcher's role in Terra (./docs/md/terra.md) . Set up \u00b6 Run boot build at the top of a wfl.git repo to build an uberjar. The resulting jar is in ./target/wfl-*.jar relative to the wfl.git clone. With some start-up and performance penalty, you can also run Workflow Launcher as a script. See below for details. Versioning \u00b6 Workflow Launcher needs to manage its version and the versions of dsde-pipelines.git which contribute the WDL files. There may be as many dsde-pipelines.git versions as there are workflow wdls. The wfl jar includes a manifest with at least that information in it, and a version command that returns it. Capabilities \u00b6 When Workflow Launcher is on-premises or in the cloud, it can currently talk to the following services: service on premises in cloud Cloud SQL x x Cromwell x x Google App Engine x x Google Cloud Platform Admin x x Google Cloud Pub/Sub x x Google Cloud Storage x x Oracle DB x Vault x x Wfl x Workflow Launcher has a diagnostic mode, dx , for debugging problems. Run zero dx to get a list of the diagnostics available. wm28d-f87:wfl yanc$ java -jar ./target/wfl-2020-03-13t17-29-12z.jar dx zero dx: tools to help debug workflow problems. Usage: zero dx <tool> [ <arg> ... ] Where: <tool> is the name of some diagnostic tool. <arg> ... are optional arguments to <tool>. The <tool>s and their <arg>s are named here. all-metadata environment & ids All workflow metadata for IDS from Cromwell in ENVIRONMENT. event-timing environment id Time per event type for workflow with ID in ENVIRONMENT. ... Error: Must specify a dx <tool> to run. BTW: You ran: zero dx wm28d-f87:wfl yanc$ Implementation \u00b6 For frontend details, check Frontend Section The initial file structure looks like this. $ tree . . \u251c\u2500\u2500 LICENSE.txt \u251c\u2500\u2500 README.md -> ./docs/md/README.md \u251c\u2500\u2500 boot.properties \u251c\u2500\u2500 build.boot \u251c\u2500\u2500 build.txt \u251c\u2500\u2500 database \u2502 \u251c\u2500\u2500 changelog.xml \u2502 \u2514\u2500\u2500 changesets \u2502 \u2514\u2500\u2500 01_db_schema.xml \u251c\u2500\u2500 deps.edn \u251c\u2500\u2500 docs \u2502 \u251c\u2500\u2500 md \u2502 \u2502 \u251c\u2500\u2500 README.md \u2502 \u2502 \u251c\u2500\u2500 frontend.md \u2502 \u2502 \u251c\u2500\u2500 server.md \u2502 \u2502 \u251c\u2500\u2500 terra.md \u2502 \u2502 \u2514\u2500\u2500 terra.org \u2502 \u251c\u2500\u2500 mkdocs.yml \u2502 \u2514\u2500\u2500 requirements.txt \u251c\u2500\u2500 ops \u2502 \u251c\u2500\u2500 README.org \u2502 \u251c\u2500\u2500 deploy.sh \u2502 \u251c\u2500\u2500 index.md \u2502 \u251c\u2500\u2500 server.sh \u2502 \u2514\u2500\u2500 terra.md -> ./docs/md/terra.md \u251c\u2500\u2500 resources \u2502 \u2514\u2500\u2500 simplelogger.properties \u251c\u2500\u2500 src \u2502 \u2514\u2500\u2500 zero \u2502 \u251c\u2500\u2500 api \u2502 \u2502 \u251c\u2500\u2500 handlers.clj \u2502 \u2502 \u2514\u2500\u2500 routes.clj \u2502 \u251c\u2500\u2500 boot.clj \u2502 \u251c\u2500\u2500 debug.clj \u2502 \u251c\u2500\u2500 dx.clj \u2502 \u251c\u2500\u2500 environments.clj \u2502 \u251c\u2500\u2500 main.clj \u2502 \u251c\u2500\u2500 metadata.clj \u2502 \u251c\u2500\u2500 module \u2502 \u2502 \u251c\u2500\u2500 all.clj \u2502 \u2502 \u251c\u2500\u2500 ukb.clj \u2502 \u2502 \u251c\u2500\u2500 wgs.clj \u2502 \u2502 \u2514\u2500\u2500 xx.clj \u2502 \u251c\u2500\u2500 once.clj \u2502 \u251c\u2500\u2500 references.clj \u2502 \u251c\u2500\u2500 server.clj \u2502 \u251c\u2500\u2500 server_debug.clj \u2502 \u251c\u2500\u2500 service \u2502 \u2502 \u251c\u2500\u2500 cromwell.clj \u2502 \u2502 \u251c\u2500\u2500 datarepo.clj \u2502 \u2502 \u251c\u2500\u2500 gcs.clj \u2502 \u2502 \u251c\u2500\u2500 postgres.clj \u2502 \u2502 \u2514\u2500\u2500 pubsub.clj \u2502 \u251c\u2500\u2500 util.clj \u2502 \u251c\u2500\u2500 wdl.clj \u2502 \u2514\u2500\u2500 zero.clj \u251c\u2500\u2500 test \u2502 \u2514\u2500\u2500 zero \u2502 \u251c\u2500\u2500 datarepo_test.clj \u2502 \u251c\u2500\u2500 gcs_test.clj \u2502 \u2514\u2500\u2500 pubsub_test.clj \u251c\u2500\u2500 ui/ \u2514\u2500\u2500 wfl.iml Top-level files \u00b6 After cloning a new WFL repo, the top-level files are. ./README.md is this file, which is just a symlink to the actual doc file under docs/md/ . ./boot.properties overrides some defaults in boot-clj . ( boot.properties is something like build.properties for sbt .) ./build.boot is a Clojure script to bootstrap WFL with boot-clj . ./build.txt holds a monotonically increasing integer for build versioning. ./.github holds Github related files, such as PR templates and Github Actions files. ./database holds database scheme migration changelog and changeset files for liquibase. ./docs has ancillary documentation. It's compiled as a static doc website. ./ops is a directory of standard scripts to support operations. It includes scripts to deploy the server in Google App Engine, and to run it locally for easier debugging. (See ./docs/md/server.md for more information.) ./resources contains the simplelogger properties and files staged from other repositories that need to be on the Java classpath when running from the .jar file. ./src/zero contains the Workflow Launcher source code. ./test/zero contains some unit tests. After building and working with WFL a while, you may notice a couple of other top-level files and directories. ./project.clj is a lein project file to support IntelliJ. ./zero is a link to build.boot that runs WFL as a script. Run boot build at least once after cloning the repo to make sure all the necessary files are in place. Source code \u00b6 The Clojure source code is in the ./src/zero directory. The entry point for the WFL executable is the -main function in main.clj . It takes the command line arguments as strings, validates the arguments, then launches the appropriate process. The server.clj file implements the WFL server. The server_debug.clj file adds some tools to aid in debugging the server. Some hacks specific to WFL are in zero.clj . The boot.clj offloads code from the build.boot file for easier development and debugging. The debug.clj file defines some macros useful when debugging or logging. The util.clj file contains a few function and macros used in WFL that are not specific to its function. The environments.clj file defines configuration parameters for different execution contexts. It's a placeholder in this repo but will be loaded in build/deploy time from a private repo. The module/ukb.clj file implements a command-line starter for the White Album , Pharma5 , or UK Biobank project. The module/xx.clj file implements a command-line starter for reprocessing eXternal eXomes . The module/wgs.clj file helps implements a command-line starter for reprocessing Whole GenomeS . The module/all.clj file hosts some utilities shared across modules. The metadata.clj file implements a tool to extract metadata from Cromwell that can be archived with the outputs generated by a workflow. The dx.clj file implements miscellaneous pipeline debugging tools. The once.clj file defines some initialization functions mostly supporting authentication. The api/handlers.clj file defines the handler functions used by server. The api/routes.clj file defines the routing strategy for server. Each of the other source files implement an interface to one of the services WFL talks to, and are named accordingly. File Service cromwell.clj Cromwell workflow runner datarepo.clj DSP DataRepo db.clj On-prem and Cloud SQL databases gcs.clj Google Cloud Storage jms.clj Java Message Service queues postgres.clj Cloud SQL postgres databases pubsub.clj Google Cloud Pub/Sub server.clj the WFL server itself wdl.clj parse WDL and manage dependencies Test code \u00b6 There are some unit tests under ./test/zero/ . File Test the namespace gcs test .clj zero.gcs in gcs.clj pubsub test .clj zero.pubsub in pubsub.clj Run them with boot test . There are also some integration tests under ./integration . Run them using aliases defined in the ./deps.edn file. clojure -A:integration clojure -A:test-create-workload With a little hacking, they can run against a local ./ops/server.sh or a server deployed to Google App Engine. Development \u00b6 WFL is implemented in Clojure and uses a tool named boot or boot-clj to manage dependencies and so on. The boot tool is a Clojure bootstrapper: it's job is to turn a standard Linux, MacOS, or Windows process into something that can host a Clojure program. WFL uses a gcloud auth command line to authenticate the user. You need to be authenticated to Google Cloud and have a recent version of google-cloud-sdk in your path to run zero or its jar successfully. I verified that Google Cloud SDK 161.0.0 works. That or any later version should be OK. Installation See this link to install boot-clj . Running boot is enough to \"install\" Clojure. There is another tool like boot named lein , which is short for \" Leiningen \". You currently need lein to develop with IntelliJ using its Clojure plugin Cursive . On MacOS, I suggest installing Homebrew and then running this. wfl # brew install boot-clj leiningen == > Using the sandbox == > Downloading https://github.com/boot-clj/boot-bin/releases/download/2.5.2/boot \ud83c\udf7a /usr/local/Cellar/boot-clj/2.5.2: 3 files, 7 .7K, built in 2 seconds ... wfl # You can brew install maven , and java too if necessary. There are boot-clj and lein distributions for all the common OS platforms. Each tool is just a file. Copy them into your PATH , run them once to bootstrap them, and you're done. (The first run of each tool downloads dependencies and so on.) The build.boot file is equivalent to the build.sbt file for SBT in Scala projects. It specified project dependencies and the build and release pipeline. It also functions as a script for running and testing the project without a separate compilation step. Hacking Clojure development feels very different from Scala and Java development. It even differs markedly from development in other dynamic languages such as Python or Ruby. Get a demonstration from someone familiar with Clojure development before you spend too much time trying to figure things out on your own. Find a local Cursive user for guidance if you like IntelliJ. Rex Wang and Saman Ehsan know how to use it. There are Cursive licenses here . There is also a Calva plugin for Visual Studio Code . I hack Clojure in Emacs using CIDER and nREPL . CIDER is not trivial to set up, but not especially difficult if you are used to Emacs. (I can help if CIDER gives you trouble.) Every time boot runs, it generates a project.clj file to support lein , Cursive, and Calva users. Running boot build will not only build a fat jar ( uberjar ) for the WFL project, but will add an executable symbolic link zero to conveniently execute the Clojure code as a script. Testing If you've never run boot before, you may have to run it twice: first to bootstrap Clojure and boot itself, and again to download their and WFL's dependencies. The first boot build run will create a ./zero link to the build.boot file ./zero starter dev $USER @broadinstitute.org You should eventually receive an humongous email from zero@broadinstitute.org containing evidence of Zero's adventures. The result should look something like this. tbl@wm97a-c2b ~/Tmp # brew install boot-clj Warning: boot-clj 2 .7.2 is already installed tbl@wm97a-c2b ~/Tmp # which boot /usr/local/bin/boot tbl@wm97a-c2b ~/Tmp # ls tbl@wm97a-c2b ~/Tmp # git clone https://github.com/broadinstitute/wfl.git Cloning into 'wfl' ... remote: Counting objects: 456 , done . remote: Compressing objects: 100 % ( 59 /59 ) , done . remote: Total 456 ( delta 62 ) , reused 98 ( delta 44 ) , pack-reused 337 Receiving objects: 100 % ( 456 /456 ) , 71 .27 KiB | 663 .00 KiB/s, done . Resolving deltas: 100 % ( 214 /214 ) , done . tbl@wm97a-c2b ~/Tmp # ls wfl tbl@wm97a-c2b ~/Tmp # cd ./wfl tbl@wm97a-c2b ~/Tmp/wfl # ls README.org build.boot src tbl@wm97a-c2b ~/Tmp/wfl # boot build Compiling 1 /1 zero.main... Adding uberjar entries... Writing pom.xml and pom.properties... Writing wfl-2020-03-13t17-29-12z.jar... Writing target dir ( s ) ... tbl@wm97a-c2b ~/Tmp/wfl # ls README.org build.boot project.clj src target zero tbl@wm97a-c2b ~/Tmp/wfl # ./zero starter zero: Error: Must specify an environment. zero: The valid environments are: cromwellv36 Test Cromwell v36 for PAPIv2 requester pays cromwellv38 Test Cromwell v38 for PAPIv2 requester pays dev Development hca HCA/DCP Lira and Falcon for the Mint team pharma5 Pharma5 WhiteAlbum UK Biobank, UKB, whatever for ukb.clj prod Production ( gotc-prod ) staging Staging Zero: zero Email a report from all systems. Usage: zero starter <env> [ <to> ... ] Where: <env> is the runtime environment. <to> ... are email addresses of recipients. zero: Error: Must specify an environment. BTW: You ran: zero starter tbl@wm97a-c2b ~/Tmp/wfl # ./zero starter dev $USER@broadinstitute.org SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\" . SLF4J: Defaulting to no-operation ( NOP ) logger implementation SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details. ... just inSANE spilling of debug logs ... tbl@wm97a-c2b ~/Tmp/wfl # Of course, after boot build , you can also run WFL from its JAR file. tbl@wm97a-c2b ~/Broad/wfl # boot build Compiling 1/1 zero.main... Adding uberjar entries... Writing pom.xml and pom.properties... Writing wfl-2020-03-13t17-29-12z.jar... Writing target dir(s)... tbl@wm97a-c2b ~/Broad/wfl # java -jar ./target/wfl-2020-03-13t17-29-12z.jar ... tbl@wm97a-c2b ~/Broad/wfl 1# Exomes in the Cloud Resources From Hybrid Selection in the Cloud V1 Clients Google Cloud Storage Client Library (Java) Google Cloud Client Library for Java Diagrams Zamboni Overview Sources /Users/tbl/Broad/zamboni/Client/src/scala/org/broadinstitute/zamboni/client/lightning/clp/Lightning.scala /Users/tbl/Broad/picard-private/src/java/edu/mit/broad/picard/lightning /Users/tbl/Broad/gppipeline-devtools/release client /Users/tbl/Broad/gppipeline-devtools/starter control /picard02:/seq/pipeline/gppipeline-devtools/current/defs/prod.defs","title":"Welcome to WorkFlow Launcher"},{"location":"#welcome-to-workflow-launcher","text":"","title":"Welcome to WorkFlow Launcher"},{"location":"#overview","text":"WorkFlow Launcher (WFL) is a workload manager. For example, a workload could be a set of WGS samples to be reprocessed in a given project/bucket, the workflow is the processing of an individual sample in that workload running WGS reprocessing. It runs as you, with your credentials, from your laptop, and communicates with other services as necessary to manage a workload. It can also be deployed to run as a service in the cloud. For more on Workflow Launcher's role in the Terra infrastructure see Workflow Launcher's role in Terra (./docs/md/terra.md) .","title":"Overview"},{"location":"#set-up","text":"Run boot build at the top of a wfl.git repo to build an uberjar. The resulting jar is in ./target/wfl-*.jar relative to the wfl.git clone. With some start-up and performance penalty, you can also run Workflow Launcher as a script. See below for details.","title":"Set up"},{"location":"#versioning","text":"Workflow Launcher needs to manage its version and the versions of dsde-pipelines.git which contribute the WDL files. There may be as many dsde-pipelines.git versions as there are workflow wdls. The wfl jar includes a manifest with at least that information in it, and a version command that returns it.","title":"Versioning"},{"location":"#capabilities","text":"When Workflow Launcher is on-premises or in the cloud, it can currently talk to the following services: service on premises in cloud Cloud SQL x x Cromwell x x Google App Engine x x Google Cloud Platform Admin x x Google Cloud Pub/Sub x x Google Cloud Storage x x Oracle DB x Vault x x Wfl x Workflow Launcher has a diagnostic mode, dx , for debugging problems. Run zero dx to get a list of the diagnostics available. wm28d-f87:wfl yanc$ java -jar ./target/wfl-2020-03-13t17-29-12z.jar dx zero dx: tools to help debug workflow problems. Usage: zero dx <tool> [ <arg> ... ] Where: <tool> is the name of some diagnostic tool. <arg> ... are optional arguments to <tool>. The <tool>s and their <arg>s are named here. all-metadata environment & ids All workflow metadata for IDS from Cromwell in ENVIRONMENT. event-timing environment id Time per event type for workflow with ID in ENVIRONMENT. ... Error: Must specify a dx <tool> to run. BTW: You ran: zero dx wm28d-f87:wfl yanc$","title":"Capabilities"},{"location":"#implementation","text":"For frontend details, check Frontend Section The initial file structure looks like this. $ tree . . \u251c\u2500\u2500 LICENSE.txt \u251c\u2500\u2500 README.md -> ./docs/md/README.md \u251c\u2500\u2500 boot.properties \u251c\u2500\u2500 build.boot \u251c\u2500\u2500 build.txt \u251c\u2500\u2500 database \u2502 \u251c\u2500\u2500 changelog.xml \u2502 \u2514\u2500\u2500 changesets \u2502 \u2514\u2500\u2500 01_db_schema.xml \u251c\u2500\u2500 deps.edn \u251c\u2500\u2500 docs \u2502 \u251c\u2500\u2500 md \u2502 \u2502 \u251c\u2500\u2500 README.md \u2502 \u2502 \u251c\u2500\u2500 frontend.md \u2502 \u2502 \u251c\u2500\u2500 server.md \u2502 \u2502 \u251c\u2500\u2500 terra.md \u2502 \u2502 \u2514\u2500\u2500 terra.org \u2502 \u251c\u2500\u2500 mkdocs.yml \u2502 \u2514\u2500\u2500 requirements.txt \u251c\u2500\u2500 ops \u2502 \u251c\u2500\u2500 README.org \u2502 \u251c\u2500\u2500 deploy.sh \u2502 \u251c\u2500\u2500 index.md \u2502 \u251c\u2500\u2500 server.sh \u2502 \u2514\u2500\u2500 terra.md -> ./docs/md/terra.md \u251c\u2500\u2500 resources \u2502 \u2514\u2500\u2500 simplelogger.properties \u251c\u2500\u2500 src \u2502 \u2514\u2500\u2500 zero \u2502 \u251c\u2500\u2500 api \u2502 \u2502 \u251c\u2500\u2500 handlers.clj \u2502 \u2502 \u2514\u2500\u2500 routes.clj \u2502 \u251c\u2500\u2500 boot.clj \u2502 \u251c\u2500\u2500 debug.clj \u2502 \u251c\u2500\u2500 dx.clj \u2502 \u251c\u2500\u2500 environments.clj \u2502 \u251c\u2500\u2500 main.clj \u2502 \u251c\u2500\u2500 metadata.clj \u2502 \u251c\u2500\u2500 module \u2502 \u2502 \u251c\u2500\u2500 all.clj \u2502 \u2502 \u251c\u2500\u2500 ukb.clj \u2502 \u2502 \u251c\u2500\u2500 wgs.clj \u2502 \u2502 \u2514\u2500\u2500 xx.clj \u2502 \u251c\u2500\u2500 once.clj \u2502 \u251c\u2500\u2500 references.clj \u2502 \u251c\u2500\u2500 server.clj \u2502 \u251c\u2500\u2500 server_debug.clj \u2502 \u251c\u2500\u2500 service \u2502 \u2502 \u251c\u2500\u2500 cromwell.clj \u2502 \u2502 \u251c\u2500\u2500 datarepo.clj \u2502 \u2502 \u251c\u2500\u2500 gcs.clj \u2502 \u2502 \u251c\u2500\u2500 postgres.clj \u2502 \u2502 \u2514\u2500\u2500 pubsub.clj \u2502 \u251c\u2500\u2500 util.clj \u2502 \u251c\u2500\u2500 wdl.clj \u2502 \u2514\u2500\u2500 zero.clj \u251c\u2500\u2500 test \u2502 \u2514\u2500\u2500 zero \u2502 \u251c\u2500\u2500 datarepo_test.clj \u2502 \u251c\u2500\u2500 gcs_test.clj \u2502 \u2514\u2500\u2500 pubsub_test.clj \u251c\u2500\u2500 ui/ \u2514\u2500\u2500 wfl.iml","title":"Implementation"},{"location":"#top-level-files","text":"After cloning a new WFL repo, the top-level files are. ./README.md is this file, which is just a symlink to the actual doc file under docs/md/ . ./boot.properties overrides some defaults in boot-clj . ( boot.properties is something like build.properties for sbt .) ./build.boot is a Clojure script to bootstrap WFL with boot-clj . ./build.txt holds a monotonically increasing integer for build versioning. ./.github holds Github related files, such as PR templates and Github Actions files. ./database holds database scheme migration changelog and changeset files for liquibase. ./docs has ancillary documentation. It's compiled as a static doc website. ./ops is a directory of standard scripts to support operations. It includes scripts to deploy the server in Google App Engine, and to run it locally for easier debugging. (See ./docs/md/server.md for more information.) ./resources contains the simplelogger properties and files staged from other repositories that need to be on the Java classpath when running from the .jar file. ./src/zero contains the Workflow Launcher source code. ./test/zero contains some unit tests. After building and working with WFL a while, you may notice a couple of other top-level files and directories. ./project.clj is a lein project file to support IntelliJ. ./zero is a link to build.boot that runs WFL as a script. Run boot build at least once after cloning the repo to make sure all the necessary files are in place.","title":"Top-level files"},{"location":"#source-code","text":"The Clojure source code is in the ./src/zero directory. The entry point for the WFL executable is the -main function in main.clj . It takes the command line arguments as strings, validates the arguments, then launches the appropriate process. The server.clj file implements the WFL server. The server_debug.clj file adds some tools to aid in debugging the server. Some hacks specific to WFL are in zero.clj . The boot.clj offloads code from the build.boot file for easier development and debugging. The debug.clj file defines some macros useful when debugging or logging. The util.clj file contains a few function and macros used in WFL that are not specific to its function. The environments.clj file defines configuration parameters for different execution contexts. It's a placeholder in this repo but will be loaded in build/deploy time from a private repo. The module/ukb.clj file implements a command-line starter for the White Album , Pharma5 , or UK Biobank project. The module/xx.clj file implements a command-line starter for reprocessing eXternal eXomes . The module/wgs.clj file helps implements a command-line starter for reprocessing Whole GenomeS . The module/all.clj file hosts some utilities shared across modules. The metadata.clj file implements a tool to extract metadata from Cromwell that can be archived with the outputs generated by a workflow. The dx.clj file implements miscellaneous pipeline debugging tools. The once.clj file defines some initialization functions mostly supporting authentication. The api/handlers.clj file defines the handler functions used by server. The api/routes.clj file defines the routing strategy for server. Each of the other source files implement an interface to one of the services WFL talks to, and are named accordingly. File Service cromwell.clj Cromwell workflow runner datarepo.clj DSP DataRepo db.clj On-prem and Cloud SQL databases gcs.clj Google Cloud Storage jms.clj Java Message Service queues postgres.clj Cloud SQL postgres databases pubsub.clj Google Cloud Pub/Sub server.clj the WFL server itself wdl.clj parse WDL and manage dependencies","title":"Source code"},{"location":"#test-code","text":"There are some unit tests under ./test/zero/ . File Test the namespace gcs test .clj zero.gcs in gcs.clj pubsub test .clj zero.pubsub in pubsub.clj Run them with boot test . There are also some integration tests under ./integration . Run them using aliases defined in the ./deps.edn file. clojure -A:integration clojure -A:test-create-workload With a little hacking, they can run against a local ./ops/server.sh or a server deployed to Google App Engine.","title":"Test code"},{"location":"#development","text":"WFL is implemented in Clojure and uses a tool named boot or boot-clj to manage dependencies and so on. The boot tool is a Clojure bootstrapper: it's job is to turn a standard Linux, MacOS, or Windows process into something that can host a Clojure program. WFL uses a gcloud auth command line to authenticate the user. You need to be authenticated to Google Cloud and have a recent version of google-cloud-sdk in your path to run zero or its jar successfully. I verified that Google Cloud SDK 161.0.0 works. That or any later version should be OK. Installation See this link to install boot-clj . Running boot is enough to \"install\" Clojure. There is another tool like boot named lein , which is short for \" Leiningen \". You currently need lein to develop with IntelliJ using its Clojure plugin Cursive . On MacOS, I suggest installing Homebrew and then running this. wfl # brew install boot-clj leiningen == > Using the sandbox == > Downloading https://github.com/boot-clj/boot-bin/releases/download/2.5.2/boot \ud83c\udf7a /usr/local/Cellar/boot-clj/2.5.2: 3 files, 7 .7K, built in 2 seconds ... wfl # You can brew install maven , and java too if necessary. There are boot-clj and lein distributions for all the common OS platforms. Each tool is just a file. Copy them into your PATH , run them once to bootstrap them, and you're done. (The first run of each tool downloads dependencies and so on.) The build.boot file is equivalent to the build.sbt file for SBT in Scala projects. It specified project dependencies and the build and release pipeline. It also functions as a script for running and testing the project without a separate compilation step. Hacking Clojure development feels very different from Scala and Java development. It even differs markedly from development in other dynamic languages such as Python or Ruby. Get a demonstration from someone familiar with Clojure development before you spend too much time trying to figure things out on your own. Find a local Cursive user for guidance if you like IntelliJ. Rex Wang and Saman Ehsan know how to use it. There are Cursive licenses here . There is also a Calva plugin for Visual Studio Code . I hack Clojure in Emacs using CIDER and nREPL . CIDER is not trivial to set up, but not especially difficult if you are used to Emacs. (I can help if CIDER gives you trouble.) Every time boot runs, it generates a project.clj file to support lein , Cursive, and Calva users. Running boot build will not only build a fat jar ( uberjar ) for the WFL project, but will add an executable symbolic link zero to conveniently execute the Clojure code as a script. Testing If you've never run boot before, you may have to run it twice: first to bootstrap Clojure and boot itself, and again to download their and WFL's dependencies. The first boot build run will create a ./zero link to the build.boot file ./zero starter dev $USER @broadinstitute.org You should eventually receive an humongous email from zero@broadinstitute.org containing evidence of Zero's adventures. The result should look something like this. tbl@wm97a-c2b ~/Tmp # brew install boot-clj Warning: boot-clj 2 .7.2 is already installed tbl@wm97a-c2b ~/Tmp # which boot /usr/local/bin/boot tbl@wm97a-c2b ~/Tmp # ls tbl@wm97a-c2b ~/Tmp # git clone https://github.com/broadinstitute/wfl.git Cloning into 'wfl' ... remote: Counting objects: 456 , done . remote: Compressing objects: 100 % ( 59 /59 ) , done . remote: Total 456 ( delta 62 ) , reused 98 ( delta 44 ) , pack-reused 337 Receiving objects: 100 % ( 456 /456 ) , 71 .27 KiB | 663 .00 KiB/s, done . Resolving deltas: 100 % ( 214 /214 ) , done . tbl@wm97a-c2b ~/Tmp # ls wfl tbl@wm97a-c2b ~/Tmp # cd ./wfl tbl@wm97a-c2b ~/Tmp/wfl # ls README.org build.boot src tbl@wm97a-c2b ~/Tmp/wfl # boot build Compiling 1 /1 zero.main... Adding uberjar entries... Writing pom.xml and pom.properties... Writing wfl-2020-03-13t17-29-12z.jar... Writing target dir ( s ) ... tbl@wm97a-c2b ~/Tmp/wfl # ls README.org build.boot project.clj src target zero tbl@wm97a-c2b ~/Tmp/wfl # ./zero starter zero: Error: Must specify an environment. zero: The valid environments are: cromwellv36 Test Cromwell v36 for PAPIv2 requester pays cromwellv38 Test Cromwell v38 for PAPIv2 requester pays dev Development hca HCA/DCP Lira and Falcon for the Mint team pharma5 Pharma5 WhiteAlbum UK Biobank, UKB, whatever for ukb.clj prod Production ( gotc-prod ) staging Staging Zero: zero Email a report from all systems. Usage: zero starter <env> [ <to> ... ] Where: <env> is the runtime environment. <to> ... are email addresses of recipients. zero: Error: Must specify an environment. BTW: You ran: zero starter tbl@wm97a-c2b ~/Tmp/wfl # ./zero starter dev $USER@broadinstitute.org SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\" . SLF4J: Defaulting to no-operation ( NOP ) logger implementation SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details. ... just inSANE spilling of debug logs ... tbl@wm97a-c2b ~/Tmp/wfl # Of course, after boot build , you can also run WFL from its JAR file. tbl@wm97a-c2b ~/Broad/wfl # boot build Compiling 1/1 zero.main... Adding uberjar entries... Writing pom.xml and pom.properties... Writing wfl-2020-03-13t17-29-12z.jar... Writing target dir(s)... tbl@wm97a-c2b ~/Broad/wfl # java -jar ./target/wfl-2020-03-13t17-29-12z.jar ... tbl@wm97a-c2b ~/Broad/wfl 1# Exomes in the Cloud Resources From Hybrid Selection in the Cloud V1 Clients Google Cloud Storage Client Library (Java) Google Cloud Client Library for Java Diagrams Zamboni Overview Sources /Users/tbl/Broad/zamboni/Client/src/scala/org/broadinstitute/zamboni/client/lightning/clp/Lightning.scala /Users/tbl/Broad/picard-private/src/java/edu/mit/broad/picard/lightning /Users/tbl/Broad/gppipeline-devtools/release client /Users/tbl/Broad/gppipeline-devtools/starter control /picard02:/seq/pipeline/gppipeline-devtools/current/defs/prod.defs","title":"Development"},{"location":"dev-process/","text":"Development Process \u00b6 This is a development process we are tying to standardize within the team, and encourage ourselves to follow in most cases. Summary \u00b6 We always make feature branches from master , make pull requests, ask for reviews and merge back to master on Github. Currently we always deploy the latest master to the development environment after merge, but in the future, we might need to cut off releases on master and deployed the released versions to the server only. It's not decided yet. Steps \u00b6 Clone the repo git@github.com:broadinstitute/wfl.git Start from the latest copy of the remote master git checkout master git pull origin master Create a feature branch It is highly recommend that you follow the naming convention shown below so JIRA could pick up the branch and link it to our JIRA board. git checkout -b tbl/GH-666-feature-branch-something Start your work, add and commit your changes git add \"README.md\" git commit -m \"Update the readme file.\" [Optional] Rebase onto lastet master: only if you want to get updates from the master git checkout master git pull origin master git checkout tbl/GH-666-feature-branch-something git rebase master alternatively, you could use the following commands without switching branches: git checkout tbl/GH-666-feature-branch-something git fetch origin master git merge master Push branch to Github in the early stage of your development (recommended): git push --set-upstream origin tbl/GH-666-feature-branch-something Create the pull request on Github UI. Be sure to fill out the PR description following the PR template instructions. If the PR is still in development, make sure use the dropdown menu and choose Create draft pull request If the PR is ready for review, click Create pull request . Look for a reviewer in the team. Address reviewer comments with more commits. Receive approval from reviewers. Make sure build the backend code at least once with: boot build Merge the PR. The feature branch will be automatically cleaned up. [Temporary] Fetch the lastest master branch again and deploy it to dev server. git checkout master git pull origin master boot deploy you might need to login to vault and google by the following commands before you want to deploy: vault auth -method=github token=$(cat ~/.github-token) gcloud auth login Note: this action might interfere other people's work that is under QA, please always coordinate before you do this!","title":"Development Process"},{"location":"dev-process/#development-process","text":"This is a development process we are tying to standardize within the team, and encourage ourselves to follow in most cases.","title":"Development Process"},{"location":"dev-process/#summary","text":"We always make feature branches from master , make pull requests, ask for reviews and merge back to master on Github. Currently we always deploy the latest master to the development environment after merge, but in the future, we might need to cut off releases on master and deployed the released versions to the server only. It's not decided yet.","title":"Summary"},{"location":"dev-process/#steps","text":"Clone the repo git@github.com:broadinstitute/wfl.git Start from the latest copy of the remote master git checkout master git pull origin master Create a feature branch It is highly recommend that you follow the naming convention shown below so JIRA could pick up the branch and link it to our JIRA board. git checkout -b tbl/GH-666-feature-branch-something Start your work, add and commit your changes git add \"README.md\" git commit -m \"Update the readme file.\" [Optional] Rebase onto lastet master: only if you want to get updates from the master git checkout master git pull origin master git checkout tbl/GH-666-feature-branch-something git rebase master alternatively, you could use the following commands without switching branches: git checkout tbl/GH-666-feature-branch-something git fetch origin master git merge master Push branch to Github in the early stage of your development (recommended): git push --set-upstream origin tbl/GH-666-feature-branch-something Create the pull request on Github UI. Be sure to fill out the PR description following the PR template instructions. If the PR is still in development, make sure use the dropdown menu and choose Create draft pull request If the PR is ready for review, click Create pull request . Look for a reviewer in the team. Address reviewer comments with more commits. Receive approval from reviewers. Make sure build the backend code at least once with: boot build Merge the PR. The feature branch will be automatically cleaned up. [Temporary] Fetch the lastest master branch again and deploy it to dev server. git checkout master git pull origin master boot deploy you might need to login to vault and google by the following commands before you want to deploy: vault auth -method=github token=$(cat ~/.github-token) gcloud auth login Note: this action might interfere other people's work that is under QA, please always coordinate before you do this!","title":"Steps"},{"location":"frontend/","text":"Workflow Launcher UI \u00b6 This is the front-end interface of the Workflow Launcher. It is a VueJS based SPA (Single-Page Application) that works as an ordinary client of the Workflow Launcher Server. You could find its position in the following Diagram: Structure \u00b6 ui \u251c\u2500\u2500 README.md \u251c\u2500\u2500 babel.config.js \u251c\u2500\u2500 dist/ \u251c\u2500\u2500 node_modules/ \u251c\u2500\u2500 package-lock.json \u251c\u2500\u2500 package.json \u251c\u2500\u2500 public/ \u251c\u2500\u2500 src/ \u2502 \u251c\u2500\u2500 App.vue \u2502 \u251c\u2500\u2500 assets/ \u2502 \u251c\u2500\u2500 components/ \u2502 \u251c\u2500\u2500 main.js \u2502 \u251c\u2500\u2500 plugins/ \u2502 \u251c\u2500\u2500 router/ \u2502 \u251c\u2500\u2500 store/ \u2502 \u2514\u2500\u2500 views/ \u2514\u2500\u2500 vue.config.js In the above structure: dist/ folder hosts the built target of the application. node_modules/ hosts the installed JS dependencies and libraries. They are ignored by git. public/ hosts the template static index HTML file that will be injected. package-*.json files hold various metadata relevant to the project. This file is used to give information to npm that allows it to identify the project as well as handle the project's dependencies. src/ folder hosts the source code of the UI application: App.vue is the main Vue component and it glues all other components together. components/ hosts all reusable Vue components. main.js helps inject some project-wide tools and plugins such as vue-router or vuetify and make them available to all sub components. plugins/ holds plugin components' settgins files. router contains files that register the internal routing table for UI. store/ hosts state files and functions that used by vuex . views/ holds different views or \"pages\" for the single-page application. The views consume the re-usable components here. vue.config.js contains settings for the Vue applicationm, such as the proxy table for local development. Project setup \u00b6 Note: for any of the following commands that uses npm , if you prefer to run from the root directory of the WFL repo instead of running from within zero/ui , please be sure to append --prefix=ui to the npm command you run. Install dependencies \u00b6 When you first clone the repo, run: npm install to install the necessary dependencies. Compiles and hot-reloads for development \u00b6 npm run serve Compiles and minifies for production \u00b6 npm run build Lints and fixes files \u00b6 npm run lint Development \u00b6 It makes your life easier if you start the local server while developing on the ui, since you could preview the live changes in your browser. Styles \u00b6 This project follows and uses Material Design, especilly the Vue implementation of Material Design framework: Vuetify. Please check their docs before adding anything to the front-end. Add new components or views \u00b6 The development process is pretty straightforward as the above structure diagram shows. Usually you just need to create a new re-usable component under ui/src/components , which follows the Vue file format: <template> <!-- your HTML and template code --> </template> <script> // your JavaScript code following Vue- // component standards </script> <style> /* your CSS styles */ </style> You could either put the component you created in the App.vue directly, or use it in the views under views/ . Note the views files are also components, except they are designed to be specific not reusable. UI states \u00b6 Sometimes it's inevitable to store some states for components of UI to better control their behaviors, the state files should be added to store/modules/ and get registered in store/index.js . Single Page Routing \u00b6 The SPA application is achieved by an internal routing in UI. This is controlled by the routing tables in router/ . More refernces \u00b6 VueJS: https://vuejs.org/v2/guide/ Vuetify: https://vuetifyjs.com/en/ Vue-router: https://router.vuejs.org/","title":"Frontend"},{"location":"frontend/#workflow-launcher-ui","text":"This is the front-end interface of the Workflow Launcher. It is a VueJS based SPA (Single-Page Application) that works as an ordinary client of the Workflow Launcher Server. You could find its position in the following Diagram:","title":"Workflow Launcher UI"},{"location":"frontend/#structure","text":"ui \u251c\u2500\u2500 README.md \u251c\u2500\u2500 babel.config.js \u251c\u2500\u2500 dist/ \u251c\u2500\u2500 node_modules/ \u251c\u2500\u2500 package-lock.json \u251c\u2500\u2500 package.json \u251c\u2500\u2500 public/ \u251c\u2500\u2500 src/ \u2502 \u251c\u2500\u2500 App.vue \u2502 \u251c\u2500\u2500 assets/ \u2502 \u251c\u2500\u2500 components/ \u2502 \u251c\u2500\u2500 main.js \u2502 \u251c\u2500\u2500 plugins/ \u2502 \u251c\u2500\u2500 router/ \u2502 \u251c\u2500\u2500 store/ \u2502 \u2514\u2500\u2500 views/ \u2514\u2500\u2500 vue.config.js In the above structure: dist/ folder hosts the built target of the application. node_modules/ hosts the installed JS dependencies and libraries. They are ignored by git. public/ hosts the template static index HTML file that will be injected. package-*.json files hold various metadata relevant to the project. This file is used to give information to npm that allows it to identify the project as well as handle the project's dependencies. src/ folder hosts the source code of the UI application: App.vue is the main Vue component and it glues all other components together. components/ hosts all reusable Vue components. main.js helps inject some project-wide tools and plugins such as vue-router or vuetify and make them available to all sub components. plugins/ holds plugin components' settgins files. router contains files that register the internal routing table for UI. store/ hosts state files and functions that used by vuex . views/ holds different views or \"pages\" for the single-page application. The views consume the re-usable components here. vue.config.js contains settings for the Vue applicationm, such as the proxy table for local development.","title":"Structure"},{"location":"frontend/#project-setup","text":"Note: for any of the following commands that uses npm , if you prefer to run from the root directory of the WFL repo instead of running from within zero/ui , please be sure to append --prefix=ui to the npm command you run.","title":"Project setup"},{"location":"frontend/#install-dependencies","text":"When you first clone the repo, run: npm install to install the necessary dependencies.","title":"Install dependencies"},{"location":"frontend/#compiles-and-hot-reloads-for-development","text":"npm run serve","title":"Compiles and hot-reloads for development"},{"location":"frontend/#compiles-and-minifies-for-production","text":"npm run build","title":"Compiles and minifies for production"},{"location":"frontend/#lints-and-fixes-files","text":"npm run lint","title":"Lints and fixes files"},{"location":"frontend/#development","text":"It makes your life easier if you start the local server while developing on the ui, since you could preview the live changes in your browser.","title":"Development"},{"location":"frontend/#styles","text":"This project follows and uses Material Design, especilly the Vue implementation of Material Design framework: Vuetify. Please check their docs before adding anything to the front-end.","title":"Styles"},{"location":"frontend/#add-new-components-or-views","text":"The development process is pretty straightforward as the above structure diagram shows. Usually you just need to create a new re-usable component under ui/src/components , which follows the Vue file format: <template> <!-- your HTML and template code --> </template> <script> // your JavaScript code following Vue- // component standards </script> <style> /* your CSS styles */ </style> You could either put the component you created in the App.vue directly, or use it in the views under views/ . Note the views files are also components, except they are designed to be specific not reusable.","title":"Add new components or views"},{"location":"frontend/#ui-states","text":"Sometimes it's inevitable to store some states for components of UI to better control their behaviors, the state files should be added to store/modules/ and get registered in store/index.js .","title":"UI states"},{"location":"frontend/#single-page-routing","text":"The SPA application is achieved by an internal routing in UI. This is controlled by the routing tables in router/ .","title":"Single Page Routing"},{"location":"frontend/#more-refernces","text":"VueJS: https://vuejs.org/v2/guide/ Vuetify: https://vuetifyjs.com/en/ Vue-router: https://router.vuejs.org/","title":"More refernces"},{"location":"server/","text":"WorkFlow Launcher Server \u00b6 We now have the basics of WorkFlow Launcher running as a server in Google App Engine (GAE). Deploy to Google App Engine \u00b6 To build and deploy WFL, run ./ops/deploy.sh . It's Google Credentials page is here. https://console.developers.google.com/apis/credentials?project=broad-gotc-dev WFL server features \u00b6 The WFL server doesn't do much now. It can configure its secrets and deploy itself. It can authenticate to Google using OAuth2. It can serve authorized and unauthorized routes. This is the application server's home URL. https://zero-dot-broad-gotc-dev.appspot.com/ Create a workload \u00b6 Defining a workload requires these top-level parameters. Parameter Type project text cromwell URL pipeline pipeline input URL prefix output URL prefix The parameters are used this way. The project is just some text to identify a researcher, billing entity, or cost object responsible for the workload. The cromwell URL specifies the Cromwell instance to service the workload . The pipeline enumeration implicitly identifies a data schema for the inputs to and outputs from the workload. You can think of it as the kind of workflow specified for the workload. People sometimes refer to this as the tag in that it is a well-known name for a Cromwell pipeline defined in WDL. You might also think of pipeline as the external or official name of a WFL processing module. ExternalWholeGenomeReprocessing is the only pipeline currently defined. Pipelines differ in their processing and results, so each pipeline requires a different set of inputs for each workflow in a workload. All input files for the workload share the input URL prefix, and all output files from the workload share the output URL prefix. These should be the longest common prefix shared by all the files to keep discovery and monitoring efficient. An ExternalWholeGenomeReprocessing workload \u00b6 An ExternalWholeGenomeReprocessing workload requires the following inputs for each workflow in the workload. input_cram sample_name base_file_name final_gvcf_base_name unmapped_bam_suffix The input_cram is the path to a file relative to the input URL prefix in the workload definition above. The input_cram contains the sample_name but we break it out into a separate input here to avoid parsing every (often large) CRAM file. The base_file_name is used to name result files. The workflow uses the base_file_name together with one or more filetype suffixes to name intermediate and output files. It is usually just the leaf name of input_cram with the .cram extension removed. The final_gvcf_base_name is the root of the leaf name of the pathname of the final VCF. The final VCF will have some variant of a .vcf suffix added by the workflow WDL. It is common for base_file_name and final_gvcf_base_name to be identical to sample_name . If no base_file_name is specified for any workflow in a workload, base_file_name defaults to sample_name . Likewise, if no final_gvcf_base_name is specified for any workflow in a workload, then final_gvcf_base_name also defaults to sample_name . It is used to recover the filename resulting from re-aligning a reverted CRAM file. The unmapped_bam_suffix is almost always .unmapped.bam , so that is its default value unless it is specified. URIs served \u00b6 The following URIs work now. Home ( / ) : Home replies with Authorized! when authorized. Otherwise it redirects to the Status page. Status ( /status ) : Status is an uauthorized endpoint that responds with \"OK\". Version ( /version ) : Version is an uauthorized endpoint that responds with the version currently deployed. OAuth Launch ( /auth/google ) : Launch begins the OAuth2 call chain to authenticate using your Google credentials. Environments ( /api/v1/environments ) : Environments returns WFL's environment tree as JSON when authorized. Environments redirects to Status when unauthorized. Starting WFL server for local development \u00b6 Run ./ops/server.sh from the command line. There is a wrap-reload-for-development-only handler wrapper commented out on the app defined in the server.clj file. When it is compiled in, source code changes that you make will be reloaded into the running server. As its name implies, you should comment it out before deploying WFL.","title":"Server"},{"location":"server/#workflow-launcher-server","text":"We now have the basics of WorkFlow Launcher running as a server in Google App Engine (GAE).","title":"WorkFlow Launcher Server"},{"location":"server/#deploy-to-google-app-engine","text":"To build and deploy WFL, run ./ops/deploy.sh . It's Google Credentials page is here. https://console.developers.google.com/apis/credentials?project=broad-gotc-dev","title":"Deploy to Google App Engine"},{"location":"server/#wfl-server-features","text":"The WFL server doesn't do much now. It can configure its secrets and deploy itself. It can authenticate to Google using OAuth2. It can serve authorized and unauthorized routes. This is the application server's home URL. https://zero-dot-broad-gotc-dev.appspot.com/","title":"WFL server features"},{"location":"server/#create-a-workload","text":"Defining a workload requires these top-level parameters. Parameter Type project text cromwell URL pipeline pipeline input URL prefix output URL prefix The parameters are used this way. The project is just some text to identify a researcher, billing entity, or cost object responsible for the workload. The cromwell URL specifies the Cromwell instance to service the workload . The pipeline enumeration implicitly identifies a data schema for the inputs to and outputs from the workload. You can think of it as the kind of workflow specified for the workload. People sometimes refer to this as the tag in that it is a well-known name for a Cromwell pipeline defined in WDL. You might also think of pipeline as the external or official name of a WFL processing module. ExternalWholeGenomeReprocessing is the only pipeline currently defined. Pipelines differ in their processing and results, so each pipeline requires a different set of inputs for each workflow in a workload. All input files for the workload share the input URL prefix, and all output files from the workload share the output URL prefix. These should be the longest common prefix shared by all the files to keep discovery and monitoring efficient.","title":"Create a workload"},{"location":"server/#an-externalwholegenomereprocessing-workload","text":"An ExternalWholeGenomeReprocessing workload requires the following inputs for each workflow in the workload. input_cram sample_name base_file_name final_gvcf_base_name unmapped_bam_suffix The input_cram is the path to a file relative to the input URL prefix in the workload definition above. The input_cram contains the sample_name but we break it out into a separate input here to avoid parsing every (often large) CRAM file. The base_file_name is used to name result files. The workflow uses the base_file_name together with one or more filetype suffixes to name intermediate and output files. It is usually just the leaf name of input_cram with the .cram extension removed. The final_gvcf_base_name is the root of the leaf name of the pathname of the final VCF. The final VCF will have some variant of a .vcf suffix added by the workflow WDL. It is common for base_file_name and final_gvcf_base_name to be identical to sample_name . If no base_file_name is specified for any workflow in a workload, base_file_name defaults to sample_name . Likewise, if no final_gvcf_base_name is specified for any workflow in a workload, then final_gvcf_base_name also defaults to sample_name . It is used to recover the filename resulting from re-aligning a reverted CRAM file. The unmapped_bam_suffix is almost always .unmapped.bam , so that is its default value unless it is specified.","title":"An ExternalWholeGenomeReprocessing workload"},{"location":"server/#uris-served","text":"The following URIs work now. Home ( / ) : Home replies with Authorized! when authorized. Otherwise it redirects to the Status page. Status ( /status ) : Status is an uauthorized endpoint that responds with \"OK\". Version ( /version ) : Version is an uauthorized endpoint that responds with the version currently deployed. OAuth Launch ( /auth/google ) : Launch begins the OAuth2 call chain to authenticate using your Google credentials. Environments ( /api/v1/environments ) : Environments returns WFL's environment tree as JSON when authorized. Environments redirects to Status when unauthorized.","title":"URIs served"},{"location":"server/#starting-wfl-server-for-local-development","text":"Run ./ops/server.sh from the command line. There is a wrap-reload-for-development-only handler wrapper commented out on the app defined in the server.clj file. When it is compiled in, source code changes that you make will be reloaded into the running server. As its name implies, you should comment it out before deploying WFL.","title":"Starting WFL server for local development"},{"location":"terra/","text":"WorkFlow Launcher's Role in Terra \u00b6 Summary \u00b6 The Data Sciences Platform (DSP) is building a new system (around Terra ) for storing and processing biological data. The system design includes a Data Repository where data is stored, and a Methods Repository that executably describes transformations on that data. In the new system, the DSP needs something to fulfill the role that Zamboni currently plays in DSP's current infrastructure to support the Genomics Platform (GP). Zamboni watches various queues for messages describing new data and how to process it. Zamboni interprets those messages to dispatch work to workflow engines (running on the premises or in the cloud) and monitors the progress of those workflows. The Zamboni web UI allows users to track the progress of workflows, and enables Ops engineers to debug problems and resume or restart failed workflows. Zamboni can run workflows on both a local Sun Grid Engine (SGE), and on Cromwell on premises and in the cloud. We think that WFL can fill the role of Zamboni in the new data storage and processing system that DSP is developing now. History \u00b6 WFL began as a project to replace a Zamboni starter , with the old name \"Zero\". A starter is a Zamboni component that brokers messages among the queues that Zamboni watches. It interprets messages queued from a Laboratory Information Management System (LIMS), such as the Mercury web service, and demultiplexes them to other Zamboni queues. Zero was later adapted to manage the reprocessing of the first batch of UK Biobank exomes. It has since been adapted to drive workflows for other projects at the Broad. Zero is unusual in that it usually runs as a stateless command line program without special system privilege, and interfaces with services running both on premises and in Google Cloud. It also manages a processing workload as a set of inputs mapped to outputs instead of tracking the progress of individual sample workflows. A Zero user need only specify a source of inputs, a workflow to run, an execution environment, and an output location. Then each time it is invoked, Zero ensures that workflows are started and retried as needed until an output exists for every input. Zero has recently been adapted again to deploy as a web service under Google App Engine (GAE) though most of the value of Zero is still not available to the server. And now it has the new name WFL. The role of WFL in Terra \u00b6 Diagrams of the new DSP processing system show a WFL service subscribed to event streams from the Data Repository (DR), with interfaces to both the Data and the Method Repositories. The implication is that something notifies WFL of new data in the Data Repository and WFL determines how to process it somehow. WFL then looks up whatever is required from the Method Repository, calls on other services as necessary to process the data and writes the results back to the DR. There is also, presumably, a web UI to track and debug the workflows managed by WFL. Many details are yet to be worked out. WFL Concepts \u00b6 WFL is designed around several novel concepts. Manage workloads instead of workflows. This is the biggest difference between Zamboni and Zero (WFL). Zamboni manages workflows whereas WFL manages workloads . Zamboni's unit of work is the workflow . Zamboni manages each workflow separately. A workflow is a transformation specified in WDL or Scala code that succeeds or fails to produce a result. The input to a workflow and its result may consist of multiple files, but they represent a single unit of work managed by a workflow engine such as Cromwell. Zamboni prepares a new workflow for each message it receives by packaging up the input and submitting it to a workflow engine. It then monitors that workflow and reports on its success or failure. WFL manages a workload , which indirectly comprises multiple workflows. Each workflow maps an input to some output, but WFL generally tracks only the inputs and outputs instead of the workflows themselves. Think of a workload as a set of inputs transformed via a workflow engine into a set of outputs. Call that set of outputs the result set . WFL generally does not care whether any individual workflow succeeds or fails. It merely considers all possible inputs specified by the workload, and looks for inputs whose outputs are missing from the result set. If some input lacks an output in the result set, WFL starts a new workflow to process that input. Note: This characterization is unfair to Zamboni. Zamboni also had to manage multiple workflows before the advent of Cromwell and still does when running workflows on SGE. But WFL can take advantage of Cromwell's job management to simplify its implementation. Specify inputs and outputs by general predicates. Each Zamboni message explicitly specifies an input to be processed. Zamboni then starts a workflow for that input and reports its status. Zamboni reports failure so a user can debug and manually succeed , reconsider, or restart the workflow. The output of a successful workflow is not Zamboni's concern. WFL finds inputs by applying a predicate specified by the user subject to some run-time constraint. Then WFL applies a function to each input to find how that input maps to the result set. Another predicate applied to the input, and its output in the result set, determines whether WFL will launch a workflow on that input. Those predicates and function can be anything expressed in a programming language. The run-time constraint is some strings passed on the command line. Minimize user input and decisions at run time. WFL gathers the predicates and mapping functions described above into a module that also knows how to generate everything a workflow engine needs to launch the workflow to process an input into a result output. That module name is one of a few run-time constraints specified by strings in a web form or on a command line. Further constraints are usually one or two of the following: - a processing environment ( `dev` `prod` `pharma5` ), - a file system directory ( `/seq/tng/tbl/` `file://home/tbl/` ), - a cloud object prefix ( `gs://bucket/folder/` `s3://bucket/` ), - a pathname suffix ( `.cram` `.vcf` ), - a spreadsheet ( or JSON , TSV , CSV , XML ) filename - or a count to limit the scope of a predicate . The module interprets the other constraints, determines which processing environments are allowed, and parses any files named accordingly. Maintain provenance. WFL runs out of a single JARfile built entirely from sources pulled from Git repositories. WFL records the Git commit hashes in the JARfile and adds them to every Cromwell workflow it starts. WFL can also preserve the Cromwell metadata alongside any result files generated by the workflow. Run with minimal privilege. Zamboni runs as a service with system account credentials such as picard . WFL is designed to run as whoever invokes it, such as tbl@broadinstitute.org . WFL fetches the users credentials from the environment when invoked from the command line. WFL requires authentication when running as a server, and constructs a JSON Web Token (JWT) to authorize other services as needed. Limit dependencies. WFL depends on a Java runtime, boot-clj to manage dependencies, Google Cloud SDK to deploy to Google App Engine (GAE). Of course, it also pulls in numerous Clojure and Java libraries at build time, and sources WDL files from the dsde-pipelines repository. A programmer needs only clone the wfl Git repositories, and run boot-clj to bootstrap WFL from source. And boot-clj is a single file: its own installer. Similarly, boot build builds WFL, and boot deploy deploys it to GAE. WFL attempts to be self-describing and self-documenting. It includes monitoring and diagnostic modules for tracking workload progress and debugging failures. WFL server \u00b6 The WFL client is a command-line batch program that a person runs intermittently on a laptop or virtual machine (VM). We are working to port the client functions of WFL to a ubiquitous web service (WFL server) running in Google Cloud. That port requires we solve several problems. State The WFL client is a stateless program that relies on consistent command line arguments to provide the constraints needed to drive the input discovery predicates and so on. Each user runs a separate process that lasts only as long as necessary to complete some stage of a workload. The WFL server is shared among all its users and runs continually. Therefore it requires some kind of data store (a database) to maintain the state of each workload across successive connections from web browsers. We intend to use the hosted Postgres service available to GAE applications for this. This work is already underway (GH-573). Authorization The WFL client assumes it runs in an authenticated context. It can pull credentials from the environment on every invocation that requires authorization to a service. The WFL server will also need to authorize services to run as some authenticated user, but cannot assume the credentials are always available, nor that there is a user present to provide them. WFL can already use OAuth2.0 to authenticate users against an identity provider and use the resulting credentials to build a JWT. It can also derive the bearer token required by most of our authorized services from a JWT. But WFL also needs some secure JWT store, so tokens are available to authorize services even when there is no active user connection. It also needs some mechanism to refresh tokens as they expire to support long-running workloads. Workload specification The user of a WFL service needs some way to specify a workload. A workload may be some set of inputs and the kind of workflow to run on them. A WFL client user now specifies a workload with a module name and a constraint . For example, ukb pharma5 110000 gs://broad-ukb/in/ gs://broad-ukb/out/ means find up to 110000 cloud objects with names prefixed with gs://broad-ukb/in/ , process them in the Cromwell set up for pharma5 , and store their outputs under gs://broad-ukb/out/ somewhere. The ukb module knows how to find .aligned.cram files under the gs://broad-ukb/in/ cloud path and set up the WDL and Cromwell dependencies and options necessary to reprocess them into .cram output files. The ukb module also knows how to find the Cromwell deployed to support pharma5 workloads, how to authorize the user to that Cromwell, and how to read any supporting data from other services. And finally, the ukb module knows how to determine which inputs do not yet have outputs under the gs://broad-ukb/out/ cloud path, and do not have workflows running in the pharma5 Cromwell. In an ideal design, this workload specification would integrate conveniently with the Data Repository's subscription or eventing service. In any case though, WFL needs some interface through which a user can specify what needs to be done. Workload management Workloads need to be started, stopped, and monitored somehow. This implies that there is some way to find active or suspended workloads, and affordances for acting on them. Users need some way to monitor the progress of a workload, and to find and debug workloads encountering unacceptable workflow failures. Monitoring and diagnostic code already exists in various WFL modules, but there is no easy way to use them from a web browser. Service interface WFL should be useful to programs other than web browsers. It is easy to imagine Terra users wanting to query WFL for the status of workloads directly without buggy and tedious screen scraping. WFL should at least export a query endpoint for use by other reporting services as well as its own browser interface. It would be nice to provide a familiar JSON or GraphQL query syntax to other services. Browser interface A browser interface should require little in addition to WFL's service interface. Ideally, one should be able to adapt WFL to new workloads via a browser interface without requiring a redeployment.","title":"WorkFlow Launcher's Role in Terra"},{"location":"terra/#workflow-launchers-role-in-terra","text":"","title":"WorkFlow Launcher's Role in Terra"},{"location":"terra/#summary","text":"The Data Sciences Platform (DSP) is building a new system (around Terra ) for storing and processing biological data. The system design includes a Data Repository where data is stored, and a Methods Repository that executably describes transformations on that data. In the new system, the DSP needs something to fulfill the role that Zamboni currently plays in DSP's current infrastructure to support the Genomics Platform (GP). Zamboni watches various queues for messages describing new data and how to process it. Zamboni interprets those messages to dispatch work to workflow engines (running on the premises or in the cloud) and monitors the progress of those workflows. The Zamboni web UI allows users to track the progress of workflows, and enables Ops engineers to debug problems and resume or restart failed workflows. Zamboni can run workflows on both a local Sun Grid Engine (SGE), and on Cromwell on premises and in the cloud. We think that WFL can fill the role of Zamboni in the new data storage and processing system that DSP is developing now.","title":"Summary"},{"location":"terra/#history","text":"WFL began as a project to replace a Zamboni starter , with the old name \"Zero\". A starter is a Zamboni component that brokers messages among the queues that Zamboni watches. It interprets messages queued from a Laboratory Information Management System (LIMS), such as the Mercury web service, and demultiplexes them to other Zamboni queues. Zero was later adapted to manage the reprocessing of the first batch of UK Biobank exomes. It has since been adapted to drive workflows for other projects at the Broad. Zero is unusual in that it usually runs as a stateless command line program without special system privilege, and interfaces with services running both on premises and in Google Cloud. It also manages a processing workload as a set of inputs mapped to outputs instead of tracking the progress of individual sample workflows. A Zero user need only specify a source of inputs, a workflow to run, an execution environment, and an output location. Then each time it is invoked, Zero ensures that workflows are started and retried as needed until an output exists for every input. Zero has recently been adapted again to deploy as a web service under Google App Engine (GAE) though most of the value of Zero is still not available to the server. And now it has the new name WFL.","title":"History"},{"location":"terra/#the-role-of-wfl-in-terra","text":"Diagrams of the new DSP processing system show a WFL service subscribed to event streams from the Data Repository (DR), with interfaces to both the Data and the Method Repositories. The implication is that something notifies WFL of new data in the Data Repository and WFL determines how to process it somehow. WFL then looks up whatever is required from the Method Repository, calls on other services as necessary to process the data and writes the results back to the DR. There is also, presumably, a web UI to track and debug the workflows managed by WFL. Many details are yet to be worked out.","title":"The role of WFL in Terra"},{"location":"terra/#wfl-concepts","text":"WFL is designed around several novel concepts. Manage workloads instead of workflows. This is the biggest difference between Zamboni and Zero (WFL). Zamboni manages workflows whereas WFL manages workloads . Zamboni's unit of work is the workflow . Zamboni manages each workflow separately. A workflow is a transformation specified in WDL or Scala code that succeeds or fails to produce a result. The input to a workflow and its result may consist of multiple files, but they represent a single unit of work managed by a workflow engine such as Cromwell. Zamboni prepares a new workflow for each message it receives by packaging up the input and submitting it to a workflow engine. It then monitors that workflow and reports on its success or failure. WFL manages a workload , which indirectly comprises multiple workflows. Each workflow maps an input to some output, but WFL generally tracks only the inputs and outputs instead of the workflows themselves. Think of a workload as a set of inputs transformed via a workflow engine into a set of outputs. Call that set of outputs the result set . WFL generally does not care whether any individual workflow succeeds or fails. It merely considers all possible inputs specified by the workload, and looks for inputs whose outputs are missing from the result set. If some input lacks an output in the result set, WFL starts a new workflow to process that input. Note: This characterization is unfair to Zamboni. Zamboni also had to manage multiple workflows before the advent of Cromwell and still does when running workflows on SGE. But WFL can take advantage of Cromwell's job management to simplify its implementation. Specify inputs and outputs by general predicates. Each Zamboni message explicitly specifies an input to be processed. Zamboni then starts a workflow for that input and reports its status. Zamboni reports failure so a user can debug and manually succeed , reconsider, or restart the workflow. The output of a successful workflow is not Zamboni's concern. WFL finds inputs by applying a predicate specified by the user subject to some run-time constraint. Then WFL applies a function to each input to find how that input maps to the result set. Another predicate applied to the input, and its output in the result set, determines whether WFL will launch a workflow on that input. Those predicates and function can be anything expressed in a programming language. The run-time constraint is some strings passed on the command line. Minimize user input and decisions at run time. WFL gathers the predicates and mapping functions described above into a module that also knows how to generate everything a workflow engine needs to launch the workflow to process an input into a result output. That module name is one of a few run-time constraints specified by strings in a web form or on a command line. Further constraints are usually one or two of the following: - a processing environment ( `dev` `prod` `pharma5` ), - a file system directory ( `/seq/tng/tbl/` `file://home/tbl/` ), - a cloud object prefix ( `gs://bucket/folder/` `s3://bucket/` ), - a pathname suffix ( `.cram` `.vcf` ), - a spreadsheet ( or JSON , TSV , CSV , XML ) filename - or a count to limit the scope of a predicate . The module interprets the other constraints, determines which processing environments are allowed, and parses any files named accordingly. Maintain provenance. WFL runs out of a single JARfile built entirely from sources pulled from Git repositories. WFL records the Git commit hashes in the JARfile and adds them to every Cromwell workflow it starts. WFL can also preserve the Cromwell metadata alongside any result files generated by the workflow. Run with minimal privilege. Zamboni runs as a service with system account credentials such as picard . WFL is designed to run as whoever invokes it, such as tbl@broadinstitute.org . WFL fetches the users credentials from the environment when invoked from the command line. WFL requires authentication when running as a server, and constructs a JSON Web Token (JWT) to authorize other services as needed. Limit dependencies. WFL depends on a Java runtime, boot-clj to manage dependencies, Google Cloud SDK to deploy to Google App Engine (GAE). Of course, it also pulls in numerous Clojure and Java libraries at build time, and sources WDL files from the dsde-pipelines repository. A programmer needs only clone the wfl Git repositories, and run boot-clj to bootstrap WFL from source. And boot-clj is a single file: its own installer. Similarly, boot build builds WFL, and boot deploy deploys it to GAE. WFL attempts to be self-describing and self-documenting. It includes monitoring and diagnostic modules for tracking workload progress and debugging failures.","title":"WFL Concepts"},{"location":"terra/#wfl-server","text":"The WFL client is a command-line batch program that a person runs intermittently on a laptop or virtual machine (VM). We are working to port the client functions of WFL to a ubiquitous web service (WFL server) running in Google Cloud. That port requires we solve several problems. State The WFL client is a stateless program that relies on consistent command line arguments to provide the constraints needed to drive the input discovery predicates and so on. Each user runs a separate process that lasts only as long as necessary to complete some stage of a workload. The WFL server is shared among all its users and runs continually. Therefore it requires some kind of data store (a database) to maintain the state of each workload across successive connections from web browsers. We intend to use the hosted Postgres service available to GAE applications for this. This work is already underway (GH-573). Authorization The WFL client assumes it runs in an authenticated context. It can pull credentials from the environment on every invocation that requires authorization to a service. The WFL server will also need to authorize services to run as some authenticated user, but cannot assume the credentials are always available, nor that there is a user present to provide them. WFL can already use OAuth2.0 to authenticate users against an identity provider and use the resulting credentials to build a JWT. It can also derive the bearer token required by most of our authorized services from a JWT. But WFL also needs some secure JWT store, so tokens are available to authorize services even when there is no active user connection. It also needs some mechanism to refresh tokens as they expire to support long-running workloads. Workload specification The user of a WFL service needs some way to specify a workload. A workload may be some set of inputs and the kind of workflow to run on them. A WFL client user now specifies a workload with a module name and a constraint . For example, ukb pharma5 110000 gs://broad-ukb/in/ gs://broad-ukb/out/ means find up to 110000 cloud objects with names prefixed with gs://broad-ukb/in/ , process them in the Cromwell set up for pharma5 , and store their outputs under gs://broad-ukb/out/ somewhere. The ukb module knows how to find .aligned.cram files under the gs://broad-ukb/in/ cloud path and set up the WDL and Cromwell dependencies and options necessary to reprocess them into .cram output files. The ukb module also knows how to find the Cromwell deployed to support pharma5 workloads, how to authorize the user to that Cromwell, and how to read any supporting data from other services. And finally, the ukb module knows how to determine which inputs do not yet have outputs under the gs://broad-ukb/out/ cloud path, and do not have workflows running in the pharma5 Cromwell. In an ideal design, this workload specification would integrate conveniently with the Data Repository's subscription or eventing service. In any case though, WFL needs some interface through which a user can specify what needs to be done. Workload management Workloads need to be started, stopped, and monitored somehow. This implies that there is some way to find active or suspended workloads, and affordances for acting on them. Users need some way to monitor the progress of a workload, and to find and debug workloads encountering unacceptable workflow failures. Monitoring and diagnostic code already exists in various WFL modules, but there is no easy way to use them from a web browser. Service interface WFL should be useful to programs other than web browsers. It is easy to imagine Terra users wanting to query WFL for the status of workloads directly without buggy and tedious screen scraping. WFL should at least export a query endpoint for use by other reporting services as well as its own browser interface. It would be nice to provide a familiar JSON or GraphQL query syntax to other services. Browser interface A browser interface should require little in addition to WFL's service interface. Ideally, one should be able to adapt WFL to new workloads via a browser interface without requiring a redeployment.","title":"WFL server"}]}