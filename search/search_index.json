{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to WorkFlow Launcher \u00b6 To the Build Board Overview \u00b6 WorkFlow Launcher (WFL) is a workload manager. For example, a workload could be a set of Whole Genome samples to be reprocessed in a given project/bucket, the workflow is the processing of an individual sample in that workload running WGS reprocessing; a workload could also be a queue of incoming notifications that describe all of the required inputs to launch Arrays scientific pipelines in Cromwell. WFL is designed to be deployed to run as a service in the cloud, primarily on Kubernetes clusters. For more on Workflow Launcher's role in the Terra infrastructure see Workflow Launcher's role in Terra . Quickstart \u00b6 Tip This is the Quickstart section, which should cover the most frequent uses cases that interact with WFL. For more detailed information, please check other sections such as the development guide or modules design principles . Build \u00b6 The easiest way to build WFL is via make (or gmake on macOS), in addition, the following prerequisites are needed: The Docker daemon Clojure ( brew install clojure on macOS) Boot ( brew install boot-clj on macOS) Python3 ( brew install python@3.8 on macOS) NodeJS ( brew install node on macOS) Arch Linux tips Install clojure and leiningen from the official repositories. Install boot and google-cloud-sdk from the AUR. You could then invoke make at the project level to test and build all workflow-launcher modules: $ make -j8 where 8 can be replaced by any number that represents the concurrent jobs you wish to run. Info If the version of your make is above GNU Make 4.0 (you could check by running make --version ), it's highly recommended to use --output-sync along with -j so the standard outputs are sorted, i.e. $ make -j8 --output-sync make will build each module in workflow-launcher , run tests and generate Docker images. All generated files go into a derived directory under the project root. You can also invoke make on a module from the top level directory by $ make [ MODULE ] TARGET ={ prebuild | build | check | images | clean | distclean } where currently available MODULE s are {api cloud_function docs helm ui} For most of the time, you would want to run something like: $ make distclean to clean up the built modules such as the docker images or the derived folder first, and then run: $ make ui api TARGET = images -j8 to only build the WFL and its docker images without running tests. Test \u00b6 If you only want to run tests on specific modules, you could run: $ make [ MODULE ] TARGET = check such as make api TARGET=check or make cloud_function TARGET=check . Clojure Test \u00b6 When it comes to clojure tests, sometimes it's useful to only run a subset of tests to save time and filter out noise. You can do this by directly invoke clojure cli from within the api directory, for example, cd api and: $ clojure -A:test integration --focus wfl.integration.v1-endpoint-test In general, we implement Clojure tests under the test/ root directory and use the kaocha test runner. Test suites use a -test namespace suffix. You can pass extra command line arguments to kaocha , such as the above --focus flag. You can see the full list of options with the following: clojure -A:integration --help At present, wfl api has two kinds of test, unit and integration . These can be run via the deps.edn , optionally specifying the kind: clojure -A:test [ unit | integration ] Note that the integration tests currently require a little more configuration before they can be run, namely, they require a wfl server running locally: ./ops/server.sh Deploy \u00b6 Currently, we mainly deploy WFL to gotc-dev and gotc-prod projects. When it's time to deploy WFL, developers need to bump the version string in the version file at the root of repo, which could be done either in a standalone PR or along with a feature PR. After having done that, the Github Action Release Latest Version will get triggered to build and push the tagged docker images of WFL to DockerHub . From here, the developers who have broad VPN connected can go to the Jenkins Page to deploy applicable versions of WFL to various available cloud projects. Warning In addition to its own version, Workflow Launcher also needs to manage the verions of dsde-pipelines.git which contribute the WDL files. Currently, that version is controlled by the commit hash string in function stage-some-files in api/src/boot.clj , for instance: ( util/shell-io! \"git\" \"-C\" ( .getParent environments ) \"checkout\" \"ad2a1b6b0f16d0e732dd08abcb79eccf4913c8d8\" ) In the long term, this is likely to change. Learn more about the deployment details in Deployment of WorkFlow Launcher . Diagnosis \u00b6 Workflow Launcher has a diagnostic command, dx , for debugging problems. Run wfl dx to get a list of the diagnostics available. $ java -jar derived/api/target/wfl.jar dx wfl dx: tools to help debug workflow problems. Usage: wfl dx <tool> [ <arg> ... ] Where: <tool> is the name of some diagnostic tool. <arg> ... are optional arguments to <tool>. The <tool>s and their <arg>s are named here. all-metadata environment & ids All workflow metadata for IDS from Cromwell in ENVIRONMENT. event-timing environment id Time per event type for workflow with ID in ENVIRONMENT. ... Error: Must specify a dx <tool> to run. BTW: You ran: wfl dx wm28d-f87:wfl yanc$ Implementation \u00b6 For frontend details, check Frontend Section Top-level files \u00b6 After cloning a new WFL repo, the top-level files are: . \u251c\u2500\u2500 api/ - `workflow-launcher` backend \u251c\u2500\u2500 cloud_function/ - functions deployed separately \u251c\u2500\u2500 database/ - database scheme migration changelog and changeset \u251c\u2500\u2500 derived/ - generated artifacts \u251c\u2500\u2500 docs/ - ancillary documentation \u251c\u2500\u2500 helm/ - helm-managed k8s configuration \u251c\u2500\u2500 LICENSE.txt \u251c\u2500\u2500 Makefile - project level` Makefile` \u251c\u2500\u2500 makerules/ - common `Makefile` functionality \u251c\u2500\u2500 ops/ - scripts to support Operations \u251c\u2500\u2500 README.md - symbolic link to docs/md/README.md \u251c\u2500\u2500 ui/ - `workflow-launcher` frontend \u2514\u2500\u2500 version - holds the current semantic version Tip: Run make at least once after cloning the repo to make sure all the necessary files are in place. api Module \u00b6 Source code \u00b6 The Clojure source code is in the api/src/ directory. The entry point for the WFL executable is the -main function in main.clj . It takes the command line arguments as strings, validates the arguments, then launches the appropriate process. The server.clj file implements the WFL server. The server_debug.clj file adds some tools to aid in debugging the server. Some hacks specific to WFL are in wfl.clj . The boot.clj offloads code from the build.boot file for easier development and debugging. The debug.clj file defines some macros useful when debugging or logging. The util.clj file contains a few functions and macros used in WFL that are not specific to its function. The environments.clj file defines configuration parameters for different execution contexts. It's a placeholder in this repo but will be loaded in build/deploy time from a private repo. The module/ukb.clj file implements a command-line starter for the White Album , Pharma5 , or UK Biobank project. The module/xx.clj file implements a command-line starter for reprocessing eXternal eXomes . The module/wgs.clj file helps implements a command-line starter for reprocessing Whole GenomeS . The module/all.clj file hosts some utilities shared across modules. The metadata.clj file implements a tool to extract metadata from Cromwell that can be archived with the outputs generated by a workflow. The dx.clj file implements miscellaneous pipeline debugging tools. The once.clj file defines some initialization functions mostly supporting authentication. The api/handlers.clj file defines the handler functions used by server. The api/routes.clj file defines the routing strategy for server. Each of the other source files implement an interface to one of the services WFL talks to, and are named accordingly. File Service cromwell.clj Cromwell workflow runner datarepo.clj DSP DataRepo db.clj On-prem and Cloud SQL databases gcs.clj Google Cloud Storage jms.clj Java Message Service queues postgres.clj Cloud SQL postgres databases server.clj the WFL server itself wdl.clj parse WDL and manage dependencies Exomes in the Cloud Resources \u00b6 From Hybrid Selection in the Cloud V1 Clients Google Cloud Storage Client Library (Java) Google Cloud Client Library for Java Diagrams Zamboni Overview Sources /Users/tbl/Broad/zamboni/Client/src/scala/org/broadinstitute/zamboni/client/lightning/clp/Lightning.scala /Users/tbl/Broad/picard-private/src/java/edu/mit/broad/picard/lightning /Users/tbl/Broad/gppipeline-devtools/release client /Users/tbl/Broad/gppipeline-devtools/starter control /picard02:/seq/pipeline/gppipeline-devtools/current/defs/prod.defs","title":"Get Started"},{"location":"#welcome-to-workflow-launcher","text":"To the Build Board","title":"Welcome to WorkFlow Launcher"},{"location":"#overview","text":"WorkFlow Launcher (WFL) is a workload manager. For example, a workload could be a set of Whole Genome samples to be reprocessed in a given project/bucket, the workflow is the processing of an individual sample in that workload running WGS reprocessing; a workload could also be a queue of incoming notifications that describe all of the required inputs to launch Arrays scientific pipelines in Cromwell. WFL is designed to be deployed to run as a service in the cloud, primarily on Kubernetes clusters. For more on Workflow Launcher's role in the Terra infrastructure see Workflow Launcher's role in Terra .","title":"Overview"},{"location":"#quickstart","text":"Tip This is the Quickstart section, which should cover the most frequent uses cases that interact with WFL. For more detailed information, please check other sections such as the development guide or modules design principles .","title":"Quickstart"},{"location":"#build","text":"The easiest way to build WFL is via make (or gmake on macOS), in addition, the following prerequisites are needed: The Docker daemon Clojure ( brew install clojure on macOS) Boot ( brew install boot-clj on macOS) Python3 ( brew install python@3.8 on macOS) NodeJS ( brew install node on macOS) Arch Linux tips Install clojure and leiningen from the official repositories. Install boot and google-cloud-sdk from the AUR. You could then invoke make at the project level to test and build all workflow-launcher modules: $ make -j8 where 8 can be replaced by any number that represents the concurrent jobs you wish to run. Info If the version of your make is above GNU Make 4.0 (you could check by running make --version ), it's highly recommended to use --output-sync along with -j so the standard outputs are sorted, i.e. $ make -j8 --output-sync make will build each module in workflow-launcher , run tests and generate Docker images. All generated files go into a derived directory under the project root. You can also invoke make on a module from the top level directory by $ make [ MODULE ] TARGET ={ prebuild | build | check | images | clean | distclean } where currently available MODULE s are {api cloud_function docs helm ui} For most of the time, you would want to run something like: $ make distclean to clean up the built modules such as the docker images or the derived folder first, and then run: $ make ui api TARGET = images -j8 to only build the WFL and its docker images without running tests.","title":"Build"},{"location":"#test","text":"If you only want to run tests on specific modules, you could run: $ make [ MODULE ] TARGET = check such as make api TARGET=check or make cloud_function TARGET=check .","title":"Test"},{"location":"#clojure-test","text":"When it comes to clojure tests, sometimes it's useful to only run a subset of tests to save time and filter out noise. You can do this by directly invoke clojure cli from within the api directory, for example, cd api and: $ clojure -A:test integration --focus wfl.integration.v1-endpoint-test In general, we implement Clojure tests under the test/ root directory and use the kaocha test runner. Test suites use a -test namespace suffix. You can pass extra command line arguments to kaocha , such as the above --focus flag. You can see the full list of options with the following: clojure -A:integration --help At present, wfl api has two kinds of test, unit and integration . These can be run via the deps.edn , optionally specifying the kind: clojure -A:test [ unit | integration ] Note that the integration tests currently require a little more configuration before they can be run, namely, they require a wfl server running locally: ./ops/server.sh","title":"Clojure Test"},{"location":"#deploy","text":"Currently, we mainly deploy WFL to gotc-dev and gotc-prod projects. When it's time to deploy WFL, developers need to bump the version string in the version file at the root of repo, which could be done either in a standalone PR or along with a feature PR. After having done that, the Github Action Release Latest Version will get triggered to build and push the tagged docker images of WFL to DockerHub . From here, the developers who have broad VPN connected can go to the Jenkins Page to deploy applicable versions of WFL to various available cloud projects. Warning In addition to its own version, Workflow Launcher also needs to manage the verions of dsde-pipelines.git which contribute the WDL files. Currently, that version is controlled by the commit hash string in function stage-some-files in api/src/boot.clj , for instance: ( util/shell-io! \"git\" \"-C\" ( .getParent environments ) \"checkout\" \"ad2a1b6b0f16d0e732dd08abcb79eccf4913c8d8\" ) In the long term, this is likely to change. Learn more about the deployment details in Deployment of WorkFlow Launcher .","title":"Deploy"},{"location":"#diagnosis","text":"Workflow Launcher has a diagnostic command, dx , for debugging problems. Run wfl dx to get a list of the diagnostics available. $ java -jar derived/api/target/wfl.jar dx wfl dx: tools to help debug workflow problems. Usage: wfl dx <tool> [ <arg> ... ] Where: <tool> is the name of some diagnostic tool. <arg> ... are optional arguments to <tool>. The <tool>s and their <arg>s are named here. all-metadata environment & ids All workflow metadata for IDS from Cromwell in ENVIRONMENT. event-timing environment id Time per event type for workflow with ID in ENVIRONMENT. ... Error: Must specify a dx <tool> to run. BTW: You ran: wfl dx wm28d-f87:wfl yanc$","title":"Diagnosis"},{"location":"#implementation","text":"For frontend details, check Frontend Section","title":"Implementation"},{"location":"#top-level-files","text":"After cloning a new WFL repo, the top-level files are: . \u251c\u2500\u2500 api/ - `workflow-launcher` backend \u251c\u2500\u2500 cloud_function/ - functions deployed separately \u251c\u2500\u2500 database/ - database scheme migration changelog and changeset \u251c\u2500\u2500 derived/ - generated artifacts \u251c\u2500\u2500 docs/ - ancillary documentation \u251c\u2500\u2500 helm/ - helm-managed k8s configuration \u251c\u2500\u2500 LICENSE.txt \u251c\u2500\u2500 Makefile - project level` Makefile` \u251c\u2500\u2500 makerules/ - common `Makefile` functionality \u251c\u2500\u2500 ops/ - scripts to support Operations \u251c\u2500\u2500 README.md - symbolic link to docs/md/README.md \u251c\u2500\u2500 ui/ - `workflow-launcher` frontend \u2514\u2500\u2500 version - holds the current semantic version Tip: Run make at least once after cloning the repo to make sure all the necessary files are in place.","title":"Top-level files"},{"location":"#api-module","text":"","title":"api Module"},{"location":"#source-code","text":"The Clojure source code is in the api/src/ directory. The entry point for the WFL executable is the -main function in main.clj . It takes the command line arguments as strings, validates the arguments, then launches the appropriate process. The server.clj file implements the WFL server. The server_debug.clj file adds some tools to aid in debugging the server. Some hacks specific to WFL are in wfl.clj . The boot.clj offloads code from the build.boot file for easier development and debugging. The debug.clj file defines some macros useful when debugging or logging. The util.clj file contains a few functions and macros used in WFL that are not specific to its function. The environments.clj file defines configuration parameters for different execution contexts. It's a placeholder in this repo but will be loaded in build/deploy time from a private repo. The module/ukb.clj file implements a command-line starter for the White Album , Pharma5 , or UK Biobank project. The module/xx.clj file implements a command-line starter for reprocessing eXternal eXomes . The module/wgs.clj file helps implements a command-line starter for reprocessing Whole GenomeS . The module/all.clj file hosts some utilities shared across modules. The metadata.clj file implements a tool to extract metadata from Cromwell that can be archived with the outputs generated by a workflow. The dx.clj file implements miscellaneous pipeline debugging tools. The once.clj file defines some initialization functions mostly supporting authentication. The api/handlers.clj file defines the handler functions used by server. The api/routes.clj file defines the routing strategy for server. Each of the other source files implement an interface to one of the services WFL talks to, and are named accordingly. File Service cromwell.clj Cromwell workflow runner datarepo.clj DSP DataRepo db.clj On-prem and Cloud SQL databases gcs.clj Google Cloud Storage jms.clj Java Message Service queues postgres.clj Cloud SQL postgres databases server.clj the WFL server itself wdl.clj parse WDL and manage dependencies","title":"Source code"},{"location":"#exomes-in-the-cloud-resources","text":"From Hybrid Selection in the Cloud V1 Clients Google Cloud Storage Client Library (Java) Google Cloud Client Library for Java Diagrams Zamboni Overview Sources /Users/tbl/Broad/zamboni/Client/src/scala/org/broadinstitute/zamboni/client/lightning/clp/Lightning.scala /Users/tbl/Broad/picard-private/src/java/edu/mit/broad/picard/lightning /Users/tbl/Broad/gppipeline-devtools/release client /Users/tbl/Broad/gppipeline-devtools/starter control /picard02:/seq/pipeline/gppipeline-devtools/current/defs/prod.defs","title":"Exomes in the Cloud Resources"},{"location":"dev-deployment/","text":"Deployment of WorkFlow Launcher \u00b6 Make a deployment \u00b6 The WorkFlow Launcher is currently running on a Kubernetes cluster, and the deployment is managed by Helm . In order to make a deployment or upgrade the current deployment manually without using the automations, you have to make the following preparations: Setup Kubernetes \u00b6 Make sure you have the kubectl command available, while in the Broad network or using the VPN, run a command like the following to set up the connection to the desired cluster: gcloud container clusters get-credentials \\ gotc-dev-shared-us-central1-a --zone us-central1-a \\ --project broad-gotc-dev Run kubectl config get-contexts to make sure you are connected to the right cluster. Later you could run kubectl config use-context $CONTEXT_NAME to switch (back) to other contexts. Setup Helm \u00b6 Install Helm, please follow the Helm instructions or simply try brew install helm , assuming you have HomeBrew installed on your MacOS. Run: helm repo add gotc-charts https://broadinstitute.github.io/gotc-helm-repo/ to add gotc\u2019s Helm repo to your Helm. Note gotc-charts is just an alias, you could give it any name you want. Run: helm repo update to make the local cached charts update-to-date to the remote repo and also run: helm repo list to check the list of repo you have connected to anytime you want. In the Broad network or on VPN and your kubectl is setup to connect to the right cluster, run: helm list to check the current deployments that are managed by Helm. Build and Dockerize \u00b6 Build the WFL with make as it's described in Quickstart : $ make ui api TARGET = images -j8 Info To manually compose a new docker image if you want one. Tag it with some helpful name, then push it to DockerHub so Kubernetes can find it, run: docker build -t broadinstitute/workflow-launcher-api: $IMAGE . docker build -t broadinstitute/workflow-launcher-ui: $IMAGE ui/. docker push broadinstitute/workflow-launcher-api: $IMAGE docker push broadinstitute/workflow-launcher-ui: $IMAGE You should see it listed here: https://hub.docker.com/repository/docker/broadinstitute/workflow-launcher-api Clone the gotc-deploy repository. \u00b6 git clone --branch <the branch you want> \\ https://github.com/broadinstitute/gotc-deploy.git Render the chart \u00b6 Render the wfl-values.yaml file. docker run -i --rm -v \" $( pwd ) \" :/working \\ -v \" $HOME \" /.vault-token:/root/.vault-token \\ -e WFL_VERSION = latest \\ broadinstitute/dsde-toolbox:dev \\ /usr/local/bin/render-ctmpls.sh \\ -k ./gotc-deploy/deploy/gotc-dev/helm/wfl-values.yaml.ctmpl Note: That command always fails, so look at ./gotc-deploy/deploy/gotc-dev/helm/wfl-values.yaml to verify that the values substituted into the template are correct. Note: Some of the values in these YML files contain credentials or sensitive information, so DO NOT check them in to your version control system or make them public!!! Deploy \u00b6 Run this to upgrade a deployment release. helm upgrade wfl-k8s gotc-charts/wfl --install \\ -f ./gotc-deploy/deploy/gotc-dev/helm/wfl-values.yaml That doesn't recreate ingress resources and so on, but it will --install everything necessary if a release is not deployed already. Run kubectl get pods to review your deployments. # kubectl get pods NAME READY STATUS AGE cromwell-auth-gotc-dev-authproxy-6f76598bb4-snpp6 1 /1 Running 11d hjf-test-authproxy-78758bbb6b-7lnfg 2 /2 Running 42d wfl-k8s-6948f6566d-2bdcp 2 /2 Running 2d5h # Run kubectl logs wfl-k8s-6948f6566d-2bdcp workflow-launcher-api to view the logs for the WFL-API server. Run kubectl logs wfl-k8s-6948f6566d-2bdcp workflow-launcher-ui to view the logs for the WFL-UI front end. Testing the deployment locally \u00b6 Similar to the above process, but in addition to the above preparations, you also have to: Make sure you have a relatively new version of Docker client installed, which has Docker -> Preferences -> Kubernetes -> Enable Kubernetes. Turn on that options and restart your Docker client. Run kubectl config use-context $CONTEXT_NAME to the Docker-built-in Kubernetes context, usually it should be called something like docker-for-mac . Similar to how you have setup the Helm charts, you could run: helm upgrade gotc-dev gotc-charts/wfl -f custom-authvals.yaml --install To install or upgrade the deployments on your local cluster. One thing to note is that you CANNOT create an ingress resource locally, so it\u2019s important to disable the ingress creation in your custom values YML files.","title":"Deployment"},{"location":"dev-deployment/#deployment-of-workflow-launcher","text":"","title":"Deployment of WorkFlow Launcher"},{"location":"dev-deployment/#make-a-deployment","text":"The WorkFlow Launcher is currently running on a Kubernetes cluster, and the deployment is managed by Helm . In order to make a deployment or upgrade the current deployment manually without using the automations, you have to make the following preparations:","title":"Make a deployment"},{"location":"dev-deployment/#setup-kubernetes","text":"Make sure you have the kubectl command available, while in the Broad network or using the VPN, run a command like the following to set up the connection to the desired cluster: gcloud container clusters get-credentials \\ gotc-dev-shared-us-central1-a --zone us-central1-a \\ --project broad-gotc-dev Run kubectl config get-contexts to make sure you are connected to the right cluster. Later you could run kubectl config use-context $CONTEXT_NAME to switch (back) to other contexts.","title":"Setup Kubernetes"},{"location":"dev-deployment/#setup-helm","text":"Install Helm, please follow the Helm instructions or simply try brew install helm , assuming you have HomeBrew installed on your MacOS. Run: helm repo add gotc-charts https://broadinstitute.github.io/gotc-helm-repo/ to add gotc\u2019s Helm repo to your Helm. Note gotc-charts is just an alias, you could give it any name you want. Run: helm repo update to make the local cached charts update-to-date to the remote repo and also run: helm repo list to check the list of repo you have connected to anytime you want. In the Broad network or on VPN and your kubectl is setup to connect to the right cluster, run: helm list to check the current deployments that are managed by Helm.","title":"Setup Helm"},{"location":"dev-deployment/#build-and-dockerize","text":"Build the WFL with make as it's described in Quickstart : $ make ui api TARGET = images -j8 Info To manually compose a new docker image if you want one. Tag it with some helpful name, then push it to DockerHub so Kubernetes can find it, run: docker build -t broadinstitute/workflow-launcher-api: $IMAGE . docker build -t broadinstitute/workflow-launcher-ui: $IMAGE ui/. docker push broadinstitute/workflow-launcher-api: $IMAGE docker push broadinstitute/workflow-launcher-ui: $IMAGE You should see it listed here: https://hub.docker.com/repository/docker/broadinstitute/workflow-launcher-api","title":"Build and Dockerize"},{"location":"dev-deployment/#clone-the-gotc-deploy-repository","text":"git clone --branch <the branch you want> \\ https://github.com/broadinstitute/gotc-deploy.git","title":"Clone the gotc-deploy repository."},{"location":"dev-deployment/#render-the-chart","text":"Render the wfl-values.yaml file. docker run -i --rm -v \" $( pwd ) \" :/working \\ -v \" $HOME \" /.vault-token:/root/.vault-token \\ -e WFL_VERSION = latest \\ broadinstitute/dsde-toolbox:dev \\ /usr/local/bin/render-ctmpls.sh \\ -k ./gotc-deploy/deploy/gotc-dev/helm/wfl-values.yaml.ctmpl Note: That command always fails, so look at ./gotc-deploy/deploy/gotc-dev/helm/wfl-values.yaml to verify that the values substituted into the template are correct. Note: Some of the values in these YML files contain credentials or sensitive information, so DO NOT check them in to your version control system or make them public!!!","title":"Render the chart"},{"location":"dev-deployment/#deploy","text":"Run this to upgrade a deployment release. helm upgrade wfl-k8s gotc-charts/wfl --install \\ -f ./gotc-deploy/deploy/gotc-dev/helm/wfl-values.yaml That doesn't recreate ingress resources and so on, but it will --install everything necessary if a release is not deployed already. Run kubectl get pods to review your deployments. # kubectl get pods NAME READY STATUS AGE cromwell-auth-gotc-dev-authproxy-6f76598bb4-snpp6 1 /1 Running 11d hjf-test-authproxy-78758bbb6b-7lnfg 2 /2 Running 42d wfl-k8s-6948f6566d-2bdcp 2 /2 Running 2d5h # Run kubectl logs wfl-k8s-6948f6566d-2bdcp workflow-launcher-api to view the logs for the WFL-API server. Run kubectl logs wfl-k8s-6948f6566d-2bdcp workflow-launcher-ui to view the logs for the WFL-UI front end.","title":"Deploy"},{"location":"dev-deployment/#testing-the-deployment-locally","text":"Similar to the above process, but in addition to the above preparations, you also have to: Make sure you have a relatively new version of Docker client installed, which has Docker -> Preferences -> Kubernetes -> Enable Kubernetes. Turn on that options and restart your Docker client. Run kubectl config use-context $CONTEXT_NAME to the Docker-built-in Kubernetes context, usually it should be called something like docker-for-mac . Similar to how you have setup the Helm charts, you could run: helm upgrade gotc-dev gotc-charts/wfl -f custom-authvals.yaml --install To install or upgrade the deployments on your local cluster. One thing to note is that you CANNOT create an ingress resource locally, so it\u2019s important to disable the ingress creation in your custom values YML files.","title":"Testing the deployment locally"},{"location":"dev-frontend/","text":"Workflow Launcher UI \u00b6 This is the front-end interface of the Workflow Launcher. It is a VueJS based SPA (Single-Page Application) that works as an ordinary client of the Workflow Launcher Server. You could find its position in the following Diagram: Structure \u00b6 ui \u251c\u2500\u2500 README.md \u251c\u2500\u2500 babel.config.js \u251c\u2500\u2500 package-lock.json \u251c\u2500\u2500 package.json \u251c\u2500\u2500 public/ \u251c\u2500\u2500 src/ \u2502 \u251c\u2500\u2500 App.vue \u2502 \u251c\u2500\u2500 assets/ \u2502 \u251c\u2500\u2500 components/ \u2502 \u251c\u2500\u2500 main.js \u2502 \u251c\u2500\u2500 plugins/ \u2502 \u251c\u2500\u2500 router/ \u2502 \u251c\u2500\u2500 store/ \u2502 \u2514\u2500\u2500 views/ \u2514\u2500\u2500 vue.config.js In the above structure: public/ hosts the template static index HTML file that will be injected. package-*.json files hold various metadata relevant to the project. This file is used to give information to npm that allows it to identify the project as well as handle the project's dependencies. src/ folder hosts the source code of the UI application: App.vue is the main Vue component and it glues all other components together. components/ hosts all reusable Vue components. main.js helps inject some project-wide tools and plugins such as vue-router or vuetify and make them available to all sub components. plugins/ holds plugin components' settgins files. router contains files that register the internal routing table for UI. store/ hosts state files and functions that used by vuex . views/ holds different views or \"pages\" for the single-page application. The views consume the re-usable components here. vue.config.js contains settings for the Vue applicationm, such as the proxy table for local development. Project setup \u00b6 Quick Start \u00b6 Run the following command from the workflow-launcher root directory: $ make ui To build the module. You can then execute $ npm serve --prefix = derived/ui to host the page. Notes: 1. For any of the following commands that uses npm , if you prefer to run from the root directory of the WFL repo instead of running from within wfl/ui , please be sure to append --prefix=ui to the npm command you run. 2. When using the environment as configured by make , append --prefix=derived/ui to your npm commands. Install dependencies \u00b6 After cloning workflow-launcher , run the following command to install the necessary dependencies: $ npm install Compiles and hot-reloads for development \u00b6 $ npm run serve Compiles and minifies for production \u00b6 $ npm run build Lints and fixes files \u00b6 $ npm run lint Development \u00b6 It makes your life easier if you start the local server while developing on the ui, since you could preview the live changes in your browser. Styles \u00b6 This project follows and uses Material Design, especilly the Vue implementation of Material Design framework: Vuetify. Please check their docs before adding anything to the front-end. Add new components or views \u00b6 The development process is pretty straightforward as the above structure diagram shows. Usually you just need to create a new re-usable component under ui/src/components , which follows the Vue file format: <template> <!-- your HTML and template code --> </template> <script> // your JavaScript code following Vue- // component standards </script> <style> /* your CSS styles */ </style> You could either put the component you created in the App.vue directly, or use it in the views under views/ . Note the views files are also components, except they are designed to be specific not reusable. UI states \u00b6 Sometimes it's inevitable to store some states for components of UI to better control their behaviors, the state files should be added to store/modules/ and get registered in store/index.js . Single Page Routing \u00b6 The SPA application is achieved by an internal routing in UI. This is controlled by the routing tables in router/ . More refernces \u00b6 VueJS: https://vuejs.org/v2/guide/ Vuetify: https://vuetifyjs.com/en/ Vue-router: https://router.vuejs.org/","title":"Frontend"},{"location":"dev-frontend/#workflow-launcher-ui","text":"This is the front-end interface of the Workflow Launcher. It is a VueJS based SPA (Single-Page Application) that works as an ordinary client of the Workflow Launcher Server. You could find its position in the following Diagram:","title":"Workflow Launcher UI"},{"location":"dev-frontend/#structure","text":"ui \u251c\u2500\u2500 README.md \u251c\u2500\u2500 babel.config.js \u251c\u2500\u2500 package-lock.json \u251c\u2500\u2500 package.json \u251c\u2500\u2500 public/ \u251c\u2500\u2500 src/ \u2502 \u251c\u2500\u2500 App.vue \u2502 \u251c\u2500\u2500 assets/ \u2502 \u251c\u2500\u2500 components/ \u2502 \u251c\u2500\u2500 main.js \u2502 \u251c\u2500\u2500 plugins/ \u2502 \u251c\u2500\u2500 router/ \u2502 \u251c\u2500\u2500 store/ \u2502 \u2514\u2500\u2500 views/ \u2514\u2500\u2500 vue.config.js In the above structure: public/ hosts the template static index HTML file that will be injected. package-*.json files hold various metadata relevant to the project. This file is used to give information to npm that allows it to identify the project as well as handle the project's dependencies. src/ folder hosts the source code of the UI application: App.vue is the main Vue component and it glues all other components together. components/ hosts all reusable Vue components. main.js helps inject some project-wide tools and plugins such as vue-router or vuetify and make them available to all sub components. plugins/ holds plugin components' settgins files. router contains files that register the internal routing table for UI. store/ hosts state files and functions that used by vuex . views/ holds different views or \"pages\" for the single-page application. The views consume the re-usable components here. vue.config.js contains settings for the Vue applicationm, such as the proxy table for local development.","title":"Structure"},{"location":"dev-frontend/#project-setup","text":"","title":"Project setup"},{"location":"dev-frontend/#quick-start","text":"Run the following command from the workflow-launcher root directory: $ make ui To build the module. You can then execute $ npm serve --prefix = derived/ui to host the page. Notes: 1. For any of the following commands that uses npm , if you prefer to run from the root directory of the WFL repo instead of running from within wfl/ui , please be sure to append --prefix=ui to the npm command you run. 2. When using the environment as configured by make , append --prefix=derived/ui to your npm commands.","title":"Quick Start"},{"location":"dev-frontend/#install-dependencies","text":"After cloning workflow-launcher , run the following command to install the necessary dependencies: $ npm install","title":"Install dependencies"},{"location":"dev-frontend/#compiles-and-hot-reloads-for-development","text":"$ npm run serve","title":"Compiles and hot-reloads for development"},{"location":"dev-frontend/#compiles-and-minifies-for-production","text":"$ npm run build","title":"Compiles and minifies for production"},{"location":"dev-frontend/#lints-and-fixes-files","text":"$ npm run lint","title":"Lints and fixes files"},{"location":"dev-frontend/#development","text":"It makes your life easier if you start the local server while developing on the ui, since you could preview the live changes in your browser.","title":"Development"},{"location":"dev-frontend/#styles","text":"This project follows and uses Material Design, especilly the Vue implementation of Material Design framework: Vuetify. Please check their docs before adding anything to the front-end.","title":"Styles"},{"location":"dev-frontend/#add-new-components-or-views","text":"The development process is pretty straightforward as the above structure diagram shows. Usually you just need to create a new re-usable component under ui/src/components , which follows the Vue file format: <template> <!-- your HTML and template code --> </template> <script> // your JavaScript code following Vue- // component standards </script> <style> /* your CSS styles */ </style> You could either put the component you created in the App.vue directly, or use it in the views under views/ . Note the views files are also components, except they are designed to be specific not reusable.","title":"Add new components or views"},{"location":"dev-frontend/#ui-states","text":"Sometimes it's inevitable to store some states for components of UI to better control their behaviors, the state files should be added to store/modules/ and get registered in store/index.js .","title":"UI states"},{"location":"dev-frontend/#single-page-routing","text":"The SPA application is achieved by an internal routing in UI. This is controlled by the routing tables in router/ .","title":"Single Page Routing"},{"location":"dev-frontend/#more-refernces","text":"VueJS: https://vuejs.org/v2/guide/ Vuetify: https://vuetifyjs.com/en/ Vue-router: https://router.vuejs.org/","title":"More refernces"},{"location":"dev-logging/","text":"WFL Logging \u00b6 TL;DR \u00b6 Import clojure.tools.logging :as log and use log/error / log/info etc. clojure.tools.logging uses SLF4J under the hood which in turn uses Log4j2 as its implementation. Below is more detailed information for those interested. Usage \u00b6 We use clojure.tools.logging aliased to log . Require it like any other dependency: ( ns \"...\" ( :require ... [ clojure.tools.logging :as log ] ... )) ( log/info \"Hello!\" ) Full documentation is available here . Behavior \u00b6 Currently, all error-level messages are routed to STDERR, and everything else is routed to STDOUT. Thus, to have WFL log everything to a file you'd want to use something like my-command >output.log 2 > & 1 That'll capture both STDOUT and STDERR to the same file. Note that this specific syntax is more universal than just &>output.log . Testing \u00b6 clojure.tools.logging provides a test namespace . An example of usage is in test/wfl/unit/logging_test.clj , which exists to test that the service loaders are correctly resolving our dependencies. Under the Hood \u00b6 This section might be a bit verbose but hopefully it won't be too out-of-date since logging setup doesn't change all that much. The key takeaway here is that JVM logging libraries use service loaders and other runtime configuration to find each other. WFL's logging works as follows: clojure.tools.logging is imported and used directly Why clojure.tools.logging? It is Clojure-native so it has intuitive syntax Why not wrap or delegate to it? It already works just as a wrapper to any other logging implementation so wrapping it would duplicate its purpose clojure.tools.logging delegates to SLF4J Why SLF4J? Jetty already uses it, so configuring it ourselves helps keep it quiet SLF4J delegates to Log4j 2 Why delegate? SLF4J is a facade that still needs an implementation to actually log things Why Log4j 2? It is a fair default implementation: highly configurable, well-tested, well-supported Even without making our own wrapper around clojure.tools.logging, we have a lot of flexibility. Suppose Jetty removes their dependency on SLF4J: we could remove our own dependency on SLF4J and clojure.tools.logging would immediately begin interacting directly with Log4j 2.","title":"Logging"},{"location":"dev-logging/#wfl-logging","text":"","title":"WFL Logging"},{"location":"dev-logging/#tldr","text":"Import clojure.tools.logging :as log and use log/error / log/info etc. clojure.tools.logging uses SLF4J under the hood which in turn uses Log4j2 as its implementation. Below is more detailed information for those interested.","title":"TL;DR"},{"location":"dev-logging/#usage","text":"We use clojure.tools.logging aliased to log . Require it like any other dependency: ( ns \"...\" ( :require ... [ clojure.tools.logging :as log ] ... )) ( log/info \"Hello!\" ) Full documentation is available here .","title":"Usage"},{"location":"dev-logging/#behavior","text":"Currently, all error-level messages are routed to STDERR, and everything else is routed to STDOUT. Thus, to have WFL log everything to a file you'd want to use something like my-command >output.log 2 > & 1 That'll capture both STDOUT and STDERR to the same file. Note that this specific syntax is more universal than just &>output.log .","title":"Behavior"},{"location":"dev-logging/#testing","text":"clojure.tools.logging provides a test namespace . An example of usage is in test/wfl/unit/logging_test.clj , which exists to test that the service loaders are correctly resolving our dependencies.","title":"Testing"},{"location":"dev-logging/#under-the-hood","text":"This section might be a bit verbose but hopefully it won't be too out-of-date since logging setup doesn't change all that much. The key takeaway here is that JVM logging libraries use service loaders and other runtime configuration to find each other. WFL's logging works as follows: clojure.tools.logging is imported and used directly Why clojure.tools.logging? It is Clojure-native so it has intuitive syntax Why not wrap or delegate to it? It already works just as a wrapper to any other logging implementation so wrapping it would duplicate its purpose clojure.tools.logging delegates to SLF4J Why SLF4J? Jetty already uses it, so configuring it ourselves helps keep it quiet SLF4J delegates to Log4j 2 Why delegate? SLF4J is a facade that still needs an implementation to actually log things Why Log4j 2? It is a fair default implementation: highly configurable, well-tested, well-supported Even without making our own wrapper around clojure.tools.logging, we have a lot of flexibility. Suppose Jetty removes their dependency on SLF4J: we could remove our own dependency on SLF4J and clojure.tools.logging would immediately begin interacting directly with Log4j 2.","title":"Under the Hood"},{"location":"dev-process/","text":"Development Process \u00b6 This is a development process we are tying to standardize within the team, and encourage ourselves to follow in most cases. The Swagger page \u00b6 WFL ships with a swaggger UI that documents all of the available endpoints. It's available at path /swagger , e.g. https://dev-wfl.gotc-dev.broadinstitute.org/swagger Development Setup \u00b6 Clojure development feels very different from Scala and Java development. It even differs markedly from development in other dynamic languages such as Python or Ruby. Get a demonstration from someone familiar with Clojure development before you spend too much time trying to figure things out on your own. Find a local Cursive user for guidance if you like IntelliJ. Rex Wang and Saman Ehsan know how to use it. Cursive licences are available here . The steps for getting this project set up with very recent versions of IntelliJ differ from Cursive's docs: Outside of IntelliJ , clone the repo and run boot at the top-level to generate the project.clj (see below) Now inside of IntelliJ , import the project by specifically targeting the project.clj file (it should offer to import the entire project, and targeting the project.clj will make use of Leiningen to work with Cursive) Use the Project Structure window (Help -> Find Action -> Project Structure) to set a JDK as the Project SDK There is also a Calva plugin for Visual Studio Code . I hack Clojure in Emacs using CIDER and nREPL . CIDER is not trivial to set up, but not especially difficult if you are used to Emacs. (I can help if CIDER gives you trouble.) Every time boot runs, it generates a project.clj file to support lein , Cursive, and Calva users. Running boot build will not only build a fat jar ( uberjar ) for the WFL project, but will add an executable symbolic link wfl to conveniently execute the Clojure code as a script. Process \u00b6 We always make feature branches from master , make pull requests, ask for reviews and merge back to master on Github. Currently we always deploy the latest master to the development environment after merge, but in the future we might need to cut off releases on master and deploy the released versions to the server only. It's not decided yet. Clone the repo git@github.com:broadinstitute/wfl.git Start from the latest copy of the remote master git checkout master git pull origin master Create a feature branch It is highly recommend that you follow the naming convention shown below so JIRA could pick up the branch and link it to our JIRA board. git checkout -b tbl/GH-666-feature-branch-something Start your work, add and commit your changes git add \"README.md\" git commit -m \"Update the readme file.\" [Optional] Rebase onto lastet master: only if you want to get updates from the master git checkout master git pull origin master git checkout tbl/GH-666-feature-branch-something git rebase master alternatively, you could use the following commands without switching branches: git checkout tbl/GH-666-feature-branch-something git fetch origin master git merge master Push branch to Github in the early stage of your development (recommended): git push --set-upstream origin tbl/GH-666-feature-branch-something Create the pull request on Github UI. Be sure to fill out the PR description following the PR template instructions. If the PR is still in development, make sure use the dropdown menu and choose Create draft pull request If the PR is ready for review, click Create pull request . Look for a reviewer in the team. Address reviewer comments with more commits. Receive approval from reviewers. Make sure build the backend code at least once with: make api Merge the PR. The feature branch will be automatically cleaned up. [Temporary] Fetch the lastest master branch again and deploy it to dev server. git checkout master git pull origin master boot deploy you might need to login to vault and google by the following commands before you want to deploy: vault auth -method=github token=$(cat ~/.github-token) gcloud auth login Note: this action might interfere other people's work that is under QA, please always coordinate before you do this! Development Tips \u00b6 Here are some tips for WFL development. Some of this advice might help when testing Liquibase migration or other changes that affect WFL's Postgres database. migrating a database \u00b6 To change WFL's Postgres database schema, add a changeset XML file in the database/changesets directory. Name the file for a recent or the current date followed by something describing the change. That will ensure that the changesets list in the order in which they apply. Note that the id and logicalFilePath attributes are derived from the changeset's file name. Then add the changeset file to the database/changlog.xml file. Test the changes against a local scratch database . See the next section for suggestions. debugging JDBC SQL \u00b6 Something seems to swallow SQL exceptions raised by Postgres and the JDBC library. Wrap suspect clojure.java.jdbc calls in wfl.util/do-or-nil to ensure that any exceptions show up in the server logs. debugging API specs \u00b6 If an API references an undefined spec, HTTP requests and responses might silently fail or the Swagger page will fail to render. Check the clojure.spec.alpha/def s in wfl.api.routes for typos before tearing your hair out. hacking a scratch database \u00b6 You can test against a local Postgres before running Liquibase or SQL against a shared database in gotc-dev or gasp production. First install Postgres locally. brew install postgresql@11 You need version 11 because that is what Google's hosted service supports, and there are differences in the SQL syntax. Modify the value of WFL_POSTGRES_URL in (postgres/wfl-db-config) to redirect the WFL server's database to a local Postgres server. With that hack in place, running ./ops/server.sh (or however you launch a local WFL server) will connect the server to a local Postgres. Now any changes to WFL state will affect only your local database. That includes running Liquibase, so don't forget to reset :debug to env before deploying your changes after merging a PR. Useful hacks for debugging Postgres/Liquibase locally \u00b6 Starting postgres : pg_ctl -D /usr/local/var/postgresql@11 start Tip It might be useful to set up some an alias for postgres if you are using zsh, for example: alias pq=\"pg_ctl -D /usr/local/var/postgresql@11\" thus you could use pq start or pq stop to easily spin up and turn down the db. Running liquibase update : liquibase --classpath = $( clojure -Spath ) --url = jdbc:postgresql:wfl --changeLogFile = database/changelog.xml --username = $USER update For the above, the username and password need to be correct for the target environment. If you're running a local server with the postgres command above, you don't need a password and can omit it. Otherwise, you may be able to find this data in the Vault entry for the environment's server -- resources/wfl/environments.clj has some environments if you've built locally. You can use --password=$ENV_SOMETHING to supply it.","title":"Development Process and Tips"},{"location":"dev-process/#development-process","text":"This is a development process we are tying to standardize within the team, and encourage ourselves to follow in most cases.","title":"Development Process"},{"location":"dev-process/#the-swagger-page","text":"WFL ships with a swaggger UI that documents all of the available endpoints. It's available at path /swagger , e.g. https://dev-wfl.gotc-dev.broadinstitute.org/swagger","title":"The Swagger page"},{"location":"dev-process/#development-setup","text":"Clojure development feels very different from Scala and Java development. It even differs markedly from development in other dynamic languages such as Python or Ruby. Get a demonstration from someone familiar with Clojure development before you spend too much time trying to figure things out on your own. Find a local Cursive user for guidance if you like IntelliJ. Rex Wang and Saman Ehsan know how to use it. Cursive licences are available here . The steps for getting this project set up with very recent versions of IntelliJ differ from Cursive's docs: Outside of IntelliJ , clone the repo and run boot at the top-level to generate the project.clj (see below) Now inside of IntelliJ , import the project by specifically targeting the project.clj file (it should offer to import the entire project, and targeting the project.clj will make use of Leiningen to work with Cursive) Use the Project Structure window (Help -> Find Action -> Project Structure) to set a JDK as the Project SDK There is also a Calva plugin for Visual Studio Code . I hack Clojure in Emacs using CIDER and nREPL . CIDER is not trivial to set up, but not especially difficult if you are used to Emacs. (I can help if CIDER gives you trouble.) Every time boot runs, it generates a project.clj file to support lein , Cursive, and Calva users. Running boot build will not only build a fat jar ( uberjar ) for the WFL project, but will add an executable symbolic link wfl to conveniently execute the Clojure code as a script.","title":"Development Setup"},{"location":"dev-process/#process","text":"We always make feature branches from master , make pull requests, ask for reviews and merge back to master on Github. Currently we always deploy the latest master to the development environment after merge, but in the future we might need to cut off releases on master and deploy the released versions to the server only. It's not decided yet. Clone the repo git@github.com:broadinstitute/wfl.git Start from the latest copy of the remote master git checkout master git pull origin master Create a feature branch It is highly recommend that you follow the naming convention shown below so JIRA could pick up the branch and link it to our JIRA board. git checkout -b tbl/GH-666-feature-branch-something Start your work, add and commit your changes git add \"README.md\" git commit -m \"Update the readme file.\" [Optional] Rebase onto lastet master: only if you want to get updates from the master git checkout master git pull origin master git checkout tbl/GH-666-feature-branch-something git rebase master alternatively, you could use the following commands without switching branches: git checkout tbl/GH-666-feature-branch-something git fetch origin master git merge master Push branch to Github in the early stage of your development (recommended): git push --set-upstream origin tbl/GH-666-feature-branch-something Create the pull request on Github UI. Be sure to fill out the PR description following the PR template instructions. If the PR is still in development, make sure use the dropdown menu and choose Create draft pull request If the PR is ready for review, click Create pull request . Look for a reviewer in the team. Address reviewer comments with more commits. Receive approval from reviewers. Make sure build the backend code at least once with: make api Merge the PR. The feature branch will be automatically cleaned up. [Temporary] Fetch the lastest master branch again and deploy it to dev server. git checkout master git pull origin master boot deploy you might need to login to vault and google by the following commands before you want to deploy: vault auth -method=github token=$(cat ~/.github-token) gcloud auth login Note: this action might interfere other people's work that is under QA, please always coordinate before you do this!","title":"Process"},{"location":"dev-process/#development-tips","text":"Here are some tips for WFL development. Some of this advice might help when testing Liquibase migration or other changes that affect WFL's Postgres database.","title":"Development Tips"},{"location":"dev-process/#migrating-a-database","text":"To change WFL's Postgres database schema, add a changeset XML file in the database/changesets directory. Name the file for a recent or the current date followed by something describing the change. That will ensure that the changesets list in the order in which they apply. Note that the id and logicalFilePath attributes are derived from the changeset's file name. Then add the changeset file to the database/changlog.xml file. Test the changes against a local scratch database . See the next section for suggestions.","title":"migrating a database"},{"location":"dev-process/#debugging-jdbc-sql","text":"Something seems to swallow SQL exceptions raised by Postgres and the JDBC library. Wrap suspect clojure.java.jdbc calls in wfl.util/do-or-nil to ensure that any exceptions show up in the server logs.","title":"debugging JDBC SQL"},{"location":"dev-process/#debugging-api-specs","text":"If an API references an undefined spec, HTTP requests and responses might silently fail or the Swagger page will fail to render. Check the clojure.spec.alpha/def s in wfl.api.routes for typos before tearing your hair out.","title":"debugging API specs"},{"location":"dev-process/#hacking-a-scratch-database","text":"You can test against a local Postgres before running Liquibase or SQL against a shared database in gotc-dev or gasp production. First install Postgres locally. brew install postgresql@11 You need version 11 because that is what Google's hosted service supports, and there are differences in the SQL syntax. Modify the value of WFL_POSTGRES_URL in (postgres/wfl-db-config) to redirect the WFL server's database to a local Postgres server. With that hack in place, running ./ops/server.sh (or however you launch a local WFL server) will connect the server to a local Postgres. Now any changes to WFL state will affect only your local database. That includes running Liquibase, so don't forget to reset :debug to env before deploying your changes after merging a PR.","title":"hacking a scratch database"},{"location":"dev-process/#useful-hacks-for-debugging-postgresliquibase-locally","text":"Starting postgres : pg_ctl -D /usr/local/var/postgresql@11 start Tip It might be useful to set up some an alias for postgres if you are using zsh, for example: alias pq=\"pg_ctl -D /usr/local/var/postgresql@11\" thus you could use pq start or pq stop to easily spin up and turn down the db. Running liquibase update : liquibase --classpath = $( clojure -Spath ) --url = jdbc:postgresql:wfl --changeLogFile = database/changelog.xml --username = $USER update For the above, the username and password need to be correct for the target environment. If you're running a local server with the postgres command above, you don't need a password and can omit it. Otherwise, you may be able to find this data in the Vault entry for the environment's server -- resources/wfl/environments.clj has some environments if you've built locally. You can use --password=$ENV_SOMETHING to supply it.","title":"Useful hacks for debugging Postgres/Liquibase locally"},{"location":"dev-sandbox/","text":"WFL Sandboxes \u00b6 \u00b6 We have the ability to deploy isolated instances of WFL infrastructure so that developers can work in parallel. Summary of the steps: 1. Copy another dev's sandbox in gotc-deploy/deploy/ and change every instance of their username to yours, PR in the changes, apply them 2. Change your Cloud SQL instance's wfl password to the one in secret/dsde/gotc/dev/zero 3. Add the new URL to the OAuth credentials 4. Deploy WFL via ./ops/cli.py gotc-dev <username> Because this process might well be a new team member's first introduction to WFL, Terraform, and/or GCP, I've added more in-depth instructions below. 1. Make your sandbox \u00b6 Within our deployment config repo we keep our sandboxes in the deploy folder in folders like {username}-sandbox . They should look pretty similar from person to person but ask if you're not sure who has a good sandbox for you to copy (folks might've done special stuff to theirs). Once you know which one to copy, do so and change every instance of their username to yours (both in file names and contents). If you don't change occurances of their username within your files you may break their sandbox. . \u2514\u2500\u2500 deploy/ \u251c\u2500\u2500 ... \u2514\u2500\u2500 {username}-sandbox/ \u2514\u2500\u2500 terraform/ \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 README.md \u251c\u2500\u2500 gotc-dev-remote_state.tf \u251c\u2500\u2500 {username}-sandbox_state.tf \u251c\u2500\u2500 {username}-sandbox-provider.tf \u2514\u2500\u2500 {username}-wfl-instance.tf PR this in (anyone on the team can review). To enact the changes, run: Warning You'll need to be on the non-split VPN from here on out (you'll get TCP timeouts if you're not) # in deploy/{username}-sandbox/terraform on the main branch ../../../scripts/terraform.sh init ../../../scripts/terraform.sh plan -out plan.out Look over plan 's output, nothing should be destroyed. If all looks good: ../../../scripts/terraform.sh apply plan.out rm plan.out 2. Change the SQL password \u00b6 Terraform makes a random password for the wfl account in your new Cloud SQL instance but for simplicity the WFL installations tied to gotc-dev all use the same one (you'll end up deploying using gotc-dev 's WFL values file which will supply a certain password). The password is defined in Vault at secret/dsde/gotc/dev/zero . You'll want to copy it to your clipboard ideally without having it end up in your console history: - If you use Vault's UI, go to https://clotho.broadinstitute.org:8200/ui/vault/secrets/secret/show/dsde/gotc/dev/zero and click the little copy button next to the password field - If you use a Mac, you can run vault read -field=password secret/dsde/gotc/dev/zero | pbcopy Next, find your Cloud SQL instance. It is named starting with {username}-wfl in the broad-gotc-dev GCP project. Go to the list of SQL instances , click on yours, go to \"Users\", change the wfl user's password, and paste the value you copied earlier. 3. Add URL to OAuth credentials \u00b6 WFL makes use of OAuth and the new URL of your deployment will need to be added to what the OAuth client will accept. The credentials to edit correspond to the oauth2_client_id field at secret/dsde/gotc/dev/zero (match to the \"Client ID\" column here ). If you'd rather copy-paste, go to https://console.cloud.google.com/apis/credentials/oauthclient/{oauth2_client_id} . You'll need to the following URI to both the \"Authorized JavaScript origins\" and the \"Authorized redirect URIs\": https://{username}-wfl.gotc-dev.broadinstitute.org . 4. Deploy WFL \u00b6 See the Quickstart section for more info on building WFL and pushing images. Assuming you've done that, run ./ops/cli.py gotc-dev {username} to deploy the version in ./version to your sandbox. The help text on ./ops/cli.py has more options for customizing the deployment.","title":"Local Development SandBox"},{"location":"dev-sandbox/#wfl-sandboxes","text":"","title":"WFL Sandboxes"},{"location":"dev-sandbox/#_1","text":"We have the ability to deploy isolated instances of WFL infrastructure so that developers can work in parallel. Summary of the steps: 1. Copy another dev's sandbox in gotc-deploy/deploy/ and change every instance of their username to yours, PR in the changes, apply them 2. Change your Cloud SQL instance's wfl password to the one in secret/dsde/gotc/dev/zero 3. Add the new URL to the OAuth credentials 4. Deploy WFL via ./ops/cli.py gotc-dev <username> Because this process might well be a new team member's first introduction to WFL, Terraform, and/or GCP, I've added more in-depth instructions below.","title":""},{"location":"dev-sandbox/#1-make-your-sandbox","text":"Within our deployment config repo we keep our sandboxes in the deploy folder in folders like {username}-sandbox . They should look pretty similar from person to person but ask if you're not sure who has a good sandbox for you to copy (folks might've done special stuff to theirs). Once you know which one to copy, do so and change every instance of their username to yours (both in file names and contents). If you don't change occurances of their username within your files you may break their sandbox. . \u2514\u2500\u2500 deploy/ \u251c\u2500\u2500 ... \u2514\u2500\u2500 {username}-sandbox/ \u2514\u2500\u2500 terraform/ \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 README.md \u251c\u2500\u2500 gotc-dev-remote_state.tf \u251c\u2500\u2500 {username}-sandbox_state.tf \u251c\u2500\u2500 {username}-sandbox-provider.tf \u2514\u2500\u2500 {username}-wfl-instance.tf PR this in (anyone on the team can review). To enact the changes, run: Warning You'll need to be on the non-split VPN from here on out (you'll get TCP timeouts if you're not) # in deploy/{username}-sandbox/terraform on the main branch ../../../scripts/terraform.sh init ../../../scripts/terraform.sh plan -out plan.out Look over plan 's output, nothing should be destroyed. If all looks good: ../../../scripts/terraform.sh apply plan.out rm plan.out","title":"1. Make your sandbox"},{"location":"dev-sandbox/#2-change-the-sql-password","text":"Terraform makes a random password for the wfl account in your new Cloud SQL instance but for simplicity the WFL installations tied to gotc-dev all use the same one (you'll end up deploying using gotc-dev 's WFL values file which will supply a certain password). The password is defined in Vault at secret/dsde/gotc/dev/zero . You'll want to copy it to your clipboard ideally without having it end up in your console history: - If you use Vault's UI, go to https://clotho.broadinstitute.org:8200/ui/vault/secrets/secret/show/dsde/gotc/dev/zero and click the little copy button next to the password field - If you use a Mac, you can run vault read -field=password secret/dsde/gotc/dev/zero | pbcopy Next, find your Cloud SQL instance. It is named starting with {username}-wfl in the broad-gotc-dev GCP project. Go to the list of SQL instances , click on yours, go to \"Users\", change the wfl user's password, and paste the value you copied earlier.","title":"2. Change the SQL password"},{"location":"dev-sandbox/#3-add-url-to-oauth-credentials","text":"WFL makes use of OAuth and the new URL of your deployment will need to be added to what the OAuth client will accept. The credentials to edit correspond to the oauth2_client_id field at secret/dsde/gotc/dev/zero (match to the \"Client ID\" column here ). If you'd rather copy-paste, go to https://console.cloud.google.com/apis/credentials/oauthclient/{oauth2_client_id} . You'll need to the following URI to both the \"Authorized JavaScript origins\" and the \"Authorized redirect URIs\": https://{username}-wfl.gotc-dev.broadinstitute.org .","title":"3. Add URL to OAuth credentials"},{"location":"dev-sandbox/#4-deploy-wfl","text":"See the Quickstart section for more info on building WFL and pushing images. Assuming you've done that, run ./ops/cli.py gotc-dev {username} to deploy the version in ./version to your sandbox. The help text on ./ops/cli.py has more options for customizing the deployment.","title":"4. Deploy WFL"},{"location":"dev-wgs/","text":"ExternalWholeGenomeReprocessing workload \u00b6 An ExternalWholeGenomeReprocessing workload requires the following inputs for each workflow in the workload. input_cram sample_name base_file_name final_gvcf_base_name unmapped_bam_suffix The input_cram is the path to a file relative to the input URL prefix in the workload definition above. The input_cram contains the sample_name but we break it out into a separate input here to avoid parsing every (often large) CRAM file. The base_file_name is used to name result files. The workflow uses the base_file_name together with one or more filetype suffixes to name intermediate and output files. It is usually just the leaf name of input_cram with the .cram extension removed. The final_gvcf_base_name is the root of the leaf name of the pathname of the final VCF. The final VCF will have some variant of a .vcf suffix added by the workflow WDL. It is common for base_file_name and final_gvcf_base_name to be identical to sample_name . If no base_file_name is specified for any workflow in a workload, base_file_name defaults to sample_name . Likewise, if no final_gvcf_base_name is specified for any workflow in a workload, then final_gvcf_base_name also defaults to sample_name . It is used to recover the filename resulting from re-aligning a reverted CRAM file. The unmapped_bam_suffix is almost always .unmapped.bam , so that is its default value unless it is specified.","title":"Whole Genome"},{"location":"dev-wgs/#externalwholegenomereprocessing-workload","text":"An ExternalWholeGenomeReprocessing workload requires the following inputs for each workflow in the workload. input_cram sample_name base_file_name final_gvcf_base_name unmapped_bam_suffix The input_cram is the path to a file relative to the input URL prefix in the workload definition above. The input_cram contains the sample_name but we break it out into a separate input here to avoid parsing every (often large) CRAM file. The base_file_name is used to name result files. The workflow uses the base_file_name together with one or more filetype suffixes to name intermediate and output files. It is usually just the leaf name of input_cram with the .cram extension removed. The final_gvcf_base_name is the root of the leaf name of the pathname of the final VCF. The final VCF will have some variant of a .vcf suffix added by the workflow WDL. It is common for base_file_name and final_gvcf_base_name to be identical to sample_name . If no base_file_name is specified for any workflow in a workload, base_file_name defaults to sample_name . Likewise, if no final_gvcf_base_name is specified for any workflow in a workload, then final_gvcf_base_name also defaults to sample_name . It is used to recover the filename resulting from re-aligning a reverted CRAM file. The unmapped_bam_suffix is almost always .unmapped.bam , so that is its default value unless it is specified.","title":"ExternalWholeGenomeReprocessing workload"},{"location":"modules-arrays/","text":"Arrays module \u00b6 WorkFlow Launcher (WFL) implements aou-arrays module to support secure and efficient processing of the AllOfUs Arrays samples. This page documents the design principles and assumptions of the module as well as summarizes the general process of module development. aou-arrays module implements arrays workload as a continuous workload , which means all samples are coming in like a continuous stream, and WFL does not make any assumption of how many samples will be in the workload or how to group the samples together: it hands off the workload creation and starting process to its caller. API \u00b6 aou-arrays module, like others, implements the following multimethod dispatchers : start-workload! add-workload! It supports the following API endpoints: Verb Endpoint Description GET /api/v1/workload List all workloads GET /api/v1/workload/{uuid} Query for a workload by its UUID POST /api/v1/create Create a new workload POST /api/v1/start Start a or multiple workload(s) POST /api/v1/append_to_aou Append a new or multiple sample(s) to an existing AOU workload Different from the fixed workload types that caller only needs to create a workload with a series of sample inputs and then simply start the workload, aou-arrays module requires the caller to manage the life cycle of a workload on their own in a multi-stage manner: The caller needs to create a workload and specify the type to be AllOfUsArrays , the caller will receive the information of the created workload if everything goes well, one of which is the uuid of the workload. Once the workload information is being reviewed, the caller needs to \"start\" the newly created workload to tell WFL that \"this workload is ready to accept incoming samples\". Without this \"start\" signal WFL will refuse to append any sample to this workload. The caller can append new individual samples to an existing started workload, and these new samples will be analyzed, processed and submitted to Cromwell as long as it has valid information. To give more information, here are some example inputs to the above endpoints: GET /api/v1/workload curl \"http://localhost:8080/api/v1/workload\" \\ -H 'Accept: application/json' GET /api/v1/workload/{uuid} curl \"http://localhost:8080/api/v1/workload?uuid=00000000-0000-0000-0000-000000000000\" \\ -H 'Accept: application/json' GET /api/v1/workload/create curl -X \"POST\" \"http://localhost:8080/api/v1/create\" \\ -H 'Accept: application/json' \\ -H 'Content-Type: application/json' \\ -d $'{\"cromwell\": \"https://cromwell-gotc-auth.gotc-dev.broadinstitute.org/\", \"input\": \"aou-inputs-placeholder\", \"output\": \"aou-outputs-placeholder\", \"project\": \"gotc-dev\", \"creator\": \"rex\", \"pipeline\": \"AllOfUsArrays\", \"items\": [ {} ]}' GET /api/v1/workload/start curl -X \"POST\" \"http://localhost:8080/api/v1/start\" \\ -H 'Accept: application/json' \\ -H 'Content-Type: application/json' \\ -d $'[ { \"uuid\": \"00000000-0000-0000-0000-000000000000\" } ]' GET /api/v1/workload/append_to_aou curl -X \"POST\" \"http://localhost:8080/api/v1/append_to_aou\" \\ -H 'Accept: application/json' \\ -H 'Content-Type: application/json' \\ -d $'{ \"environment\": \"aou-dev\", \"cromwell\": \"https://cromwell-gotc-auth.gotc-dev.broadinstitute.org/\", \"uuid\": \"00000000-0000-0000-0000-000000000000\", \"notifications\": [ { \"zcall_thresholds_file\": \"foo\", \"sample_lsid\": \"foo\", \"bead_pool_manifest_file\": \"foo\", \"chip_well_barcode\": \"foo\", \"sample_alias\": \"foo\", \"green_idat_cloud_path\": \"foo\", \"red_idat_cloud_path\": \"foo\", \"cluster_file\": \"foo\", \"reported_gender\": \"foo\", \"gender_cluster_file\": \"foo\", \"params_file\": \"foo\", \"extended_chip_manifest_file\": \"foo\", \"analysis_version_number\": 1 }, { \"zcall_thresholds_file\": \"foo\", \"sample_lsid\": \"foo\", \"bead_pool_manifest_file\": \"foo\", \"chip_well_barcode\": \"foo\", \"sample_alias\": \"foo\", \"green_idat_cloud_path\": \"foo\", \"red_idat_cloud_path\": \"foo\", \"cluster_file\": \"foo\", \"reported_gender\": \"foo\", \"gender_cluster_file\": \"foo\", \"params_file\": \"foo\", \"extended_chip_manifest_file\": \"foo\", \"analysis_version_number\": 5 } ] }' Workload Model \u00b6 WFL designed the following workload model in order to support the above API and workload submission mechanism. Initially, it has the following schema: List of relations Schema | Name | Type | Owner | Size | Description --------+--------------------------------+----------+----------+------------+------------- public | databasechangelog | table | foo | 16 kB | public | databasechangeloglock | table | foo | 8192 bytes | public | workload | table | foo | 16 kB | public | workload_id_seq | sequence | foo | 8192 bytes | the workload table looks like: id | commit | created | creator | cromwell | finished | input | items | output | pipeline | project | release | started | uuid | version | wdl ----+--------+---------+---------+----------+----------+-------+-------+--------+----------+---------+---------+---------+------+---------+----- (0 rows) Note that different from the fixed workload types, input , output and items are not useful to aou-arrays workload since these fields vary from sample to sample. Any information the caller provided to these fields will stored as placeholders. More importantly, eve though id is the primary key here, (pipeline, project, release) works as the unique identifier for arrays workloads, for instance, if there's already a workload with values: (AllOfUsArrays, gotc-dev, Arrays_v1.9) , any further attempts to create a new workload with exact the same values will return the information of this existing workload rather than create a new row. Once the caller successfully creates a new sample, there will be a new row added to the above workload table, and a new table will be created accordingly: List of relations Schema | Name | Type | Owner | Size | Description --------+--------------------------------+----------+----------+------------+------------- public | allofusarrays_000000001 | table | foo | 16 kB | public | allofusarrays_000000001_id_seq | sequence | foo | 8192 bytes | public | databasechangelog | table | foo | 16 kB | public | databasechangeloglock | table | foo | 8192 bytes | public | workload | table | foo | 16 kB | public | workload_id_seq | sequence | foo | 8192 bytes | The allofusarrays_00000000X table has the following fields: id | analysis_version_number | chip_well_barcode | status | updated | uuid ----+-------------------------+-------------------+-----------+-------------------------------+-------------------------------------- 1 | 1 | 0000000000_R01C01 | Succeeded | 2020-07-21 00:00:00.241218-04 | 00000000-0000-0000-0000-000000000000 2 | 5 | 0000000000_R01C01 | Failed | 2020-07-21 00:00:00.028976-04 | 00000000-0000-0000-0000-000000000000 Among which (analysis_version_number, chip_well_barcode) works as the primary key , any new samples that collide with the existing primary-keys will be omitted.","title":"Arrays"},{"location":"modules-arrays/#arrays-module","text":"WorkFlow Launcher (WFL) implements aou-arrays module to support secure and efficient processing of the AllOfUs Arrays samples. This page documents the design principles and assumptions of the module as well as summarizes the general process of module development. aou-arrays module implements arrays workload as a continuous workload , which means all samples are coming in like a continuous stream, and WFL does not make any assumption of how many samples will be in the workload or how to group the samples together: it hands off the workload creation and starting process to its caller.","title":"Arrays module"},{"location":"modules-arrays/#api","text":"aou-arrays module, like others, implements the following multimethod dispatchers : start-workload! add-workload! It supports the following API endpoints: Verb Endpoint Description GET /api/v1/workload List all workloads GET /api/v1/workload/{uuid} Query for a workload by its UUID POST /api/v1/create Create a new workload POST /api/v1/start Start a or multiple workload(s) POST /api/v1/append_to_aou Append a new or multiple sample(s) to an existing AOU workload Different from the fixed workload types that caller only needs to create a workload with a series of sample inputs and then simply start the workload, aou-arrays module requires the caller to manage the life cycle of a workload on their own in a multi-stage manner: The caller needs to create a workload and specify the type to be AllOfUsArrays , the caller will receive the information of the created workload if everything goes well, one of which is the uuid of the workload. Once the workload information is being reviewed, the caller needs to \"start\" the newly created workload to tell WFL that \"this workload is ready to accept incoming samples\". Without this \"start\" signal WFL will refuse to append any sample to this workload. The caller can append new individual samples to an existing started workload, and these new samples will be analyzed, processed and submitted to Cromwell as long as it has valid information. To give more information, here are some example inputs to the above endpoints: GET /api/v1/workload curl \"http://localhost:8080/api/v1/workload\" \\ -H 'Accept: application/json' GET /api/v1/workload/{uuid} curl \"http://localhost:8080/api/v1/workload?uuid=00000000-0000-0000-0000-000000000000\" \\ -H 'Accept: application/json' GET /api/v1/workload/create curl -X \"POST\" \"http://localhost:8080/api/v1/create\" \\ -H 'Accept: application/json' \\ -H 'Content-Type: application/json' \\ -d $'{\"cromwell\": \"https://cromwell-gotc-auth.gotc-dev.broadinstitute.org/\", \"input\": \"aou-inputs-placeholder\", \"output\": \"aou-outputs-placeholder\", \"project\": \"gotc-dev\", \"creator\": \"rex\", \"pipeline\": \"AllOfUsArrays\", \"items\": [ {} ]}' GET /api/v1/workload/start curl -X \"POST\" \"http://localhost:8080/api/v1/start\" \\ -H 'Accept: application/json' \\ -H 'Content-Type: application/json' \\ -d $'[ { \"uuid\": \"00000000-0000-0000-0000-000000000000\" } ]' GET /api/v1/workload/append_to_aou curl -X \"POST\" \"http://localhost:8080/api/v1/append_to_aou\" \\ -H 'Accept: application/json' \\ -H 'Content-Type: application/json' \\ -d $'{ \"environment\": \"aou-dev\", \"cromwell\": \"https://cromwell-gotc-auth.gotc-dev.broadinstitute.org/\", \"uuid\": \"00000000-0000-0000-0000-000000000000\", \"notifications\": [ { \"zcall_thresholds_file\": \"foo\", \"sample_lsid\": \"foo\", \"bead_pool_manifest_file\": \"foo\", \"chip_well_barcode\": \"foo\", \"sample_alias\": \"foo\", \"green_idat_cloud_path\": \"foo\", \"red_idat_cloud_path\": \"foo\", \"cluster_file\": \"foo\", \"reported_gender\": \"foo\", \"gender_cluster_file\": \"foo\", \"params_file\": \"foo\", \"extended_chip_manifest_file\": \"foo\", \"analysis_version_number\": 1 }, { \"zcall_thresholds_file\": \"foo\", \"sample_lsid\": \"foo\", \"bead_pool_manifest_file\": \"foo\", \"chip_well_barcode\": \"foo\", \"sample_alias\": \"foo\", \"green_idat_cloud_path\": \"foo\", \"red_idat_cloud_path\": \"foo\", \"cluster_file\": \"foo\", \"reported_gender\": \"foo\", \"gender_cluster_file\": \"foo\", \"params_file\": \"foo\", \"extended_chip_manifest_file\": \"foo\", \"analysis_version_number\": 5 } ] }'","title":"API"},{"location":"modules-arrays/#workload-model","text":"WFL designed the following workload model in order to support the above API and workload submission mechanism. Initially, it has the following schema: List of relations Schema | Name | Type | Owner | Size | Description --------+--------------------------------+----------+----------+------------+------------- public | databasechangelog | table | foo | 16 kB | public | databasechangeloglock | table | foo | 8192 bytes | public | workload | table | foo | 16 kB | public | workload_id_seq | sequence | foo | 8192 bytes | the workload table looks like: id | commit | created | creator | cromwell | finished | input | items | output | pipeline | project | release | started | uuid | version | wdl ----+--------+---------+---------+----------+----------+-------+-------+--------+----------+---------+---------+---------+------+---------+----- (0 rows) Note that different from the fixed workload types, input , output and items are not useful to aou-arrays workload since these fields vary from sample to sample. Any information the caller provided to these fields will stored as placeholders. More importantly, eve though id is the primary key here, (pipeline, project, release) works as the unique identifier for arrays workloads, for instance, if there's already a workload with values: (AllOfUsArrays, gotc-dev, Arrays_v1.9) , any further attempts to create a new workload with exact the same values will return the information of this existing workload rather than create a new row. Once the caller successfully creates a new sample, there will be a new row added to the above workload table, and a new table will be created accordingly: List of relations Schema | Name | Type | Owner | Size | Description --------+--------------------------------+----------+----------+------------+------------- public | allofusarrays_000000001 | table | foo | 16 kB | public | allofusarrays_000000001_id_seq | sequence | foo | 8192 bytes | public | databasechangelog | table | foo | 16 kB | public | databasechangeloglock | table | foo | 8192 bytes | public | workload | table | foo | 16 kB | public | workload_id_seq | sequence | foo | 8192 bytes | The allofusarrays_00000000X table has the following fields: id | analysis_version_number | chip_well_barcode | status | updated | uuid ----+-------------------------+-------------------+-----------+-------------------------------+-------------------------------------- 1 | 1 | 0000000000_R01C01 | Succeeded | 2020-07-21 00:00:00.241218-04 | 00000000-0000-0000-0000-000000000000 2 | 5 | 0000000000_R01C01 | Failed | 2020-07-21 00:00:00.028976-04 | 00000000-0000-0000-0000-000000000000 Among which (analysis_version_number, chip_well_barcode) works as the primary key , any new samples that collide with the existing primary-keys will be omitted.","title":"Workload Model"},{"location":"modules-general/","text":"Modules Design Principles and Assumptions \u00b6 WorkFlow Launcher is responsible for preparing the required workflow WDLs, inputs and options for Cromwell in a large scale. This work involves in inputs validation, pipeline WDL orchestration and Cromwell workflow management. Similar to other WFL modules, the aou-arrays module takes advantage of the workload concept in order to manage workflows efficiently. In general, WFL classify all workloads into 2 categories: continuous and fixed. For instance, aou-arrays module implements arrays workload as a continuous workload, which means all samples are coming in like a continuous stream, and WFL does not make any assumption of how many samples will be in the workload or how to group the samples together: it hands off the workload creation and starting process to its caller. wgs module implements External Whole Genome workloads as a discrete workload that WFL has full knowledge about the number and properties of the samples it's going to process, and the samples can be grouped into batches (workloads) by a set of properties. To learn more about the details of each module, please check their own sections in this documentation. Create a workload \u00b6 Defining a workload type requires these top-level parameters. Parameter Type project text cromwell URL pipeline pipeline input URL prefix output URL prefix The parameters are used this way. The project is just some text to identify a researcher, billing entity, or cost object responsible for the workload. The cromwell URL specifies the Cromwell instance to service the workload . The pipeline enumeration implicitly identifies a data schema for the inputs to and outputs from the workload. You can think of it as the kind of workflow specified for the workload. People sometimes refer to this as the tag in that it is a well-known name for a Cromwell pipeline defined in WDL. You might also think of pipeline as the external or official name of a WFL processing module.","title":"General"},{"location":"modules-general/#modules-design-principles-and-assumptions","text":"WorkFlow Launcher is responsible for preparing the required workflow WDLs, inputs and options for Cromwell in a large scale. This work involves in inputs validation, pipeline WDL orchestration and Cromwell workflow management. Similar to other WFL modules, the aou-arrays module takes advantage of the workload concept in order to manage workflows efficiently. In general, WFL classify all workloads into 2 categories: continuous and fixed. For instance, aou-arrays module implements arrays workload as a continuous workload, which means all samples are coming in like a continuous stream, and WFL does not make any assumption of how many samples will be in the workload or how to group the samples together: it hands off the workload creation and starting process to its caller. wgs module implements External Whole Genome workloads as a discrete workload that WFL has full knowledge about the number and properties of the samples it's going to process, and the samples can be grouped into batches (workloads) by a set of properties. To learn more about the details of each module, please check their own sections in this documentation.","title":"Modules Design Principles and Assumptions"},{"location":"modules-general/#create-a-workload","text":"Defining a workload type requires these top-level parameters. Parameter Type project text cromwell URL pipeline pipeline input URL prefix output URL prefix The parameters are used this way. The project is just some text to identify a researcher, billing entity, or cost object responsible for the workload. The cromwell URL specifies the Cromwell instance to service the workload . The pipeline enumeration implicitly identifies a data schema for the inputs to and outputs from the workload. You can think of it as the kind of workflow specified for the workload. People sometimes refer to this as the tag in that it is a well-known name for a Cromwell pipeline defined in WDL. You might also think of pipeline as the external or official name of a WFL processing module.","title":"Create a workload"},{"location":"terra/","text":"WorkFlow Launcher's Role in Terra \u00b6 Summary \u00b6 The Data Sciences Platform (DSP) is building a new system (around Terra ) for storing and processing biological data. The system design includes a Data Repository where data is stored, and a Methods Repository that executably describes transformations on that data. In the new system, the DSP needs something to fulfill the role that Zamboni currently plays in DSP's current infrastructure to support the Genomics Platform (GP). Zamboni watches various queues for messages describing new data and how to process it. Zamboni interprets those messages to dispatch work to workflow engines (running on the premises or in the cloud) and monitors the progress of those workflows. The Zamboni web UI allows users to track the progress of workflows, and enables Ops engineers to debug problems and resume or restart failed workflows. Zamboni can run workflows on both a local Sun Grid Engine (SGE), and on Cromwell on premises and in the cloud. We think that WFL can fill the role of Zamboni in the new data storage and processing system that DSP is developing now. History \u00b6 WFL began as a project to replace a Zamboni starter , with the old name \"Zero\". A starter is a Zamboni component that brokers messages among the queues that Zamboni watches. It interprets messages queued from a Laboratory Information Management System (LIMS), such as the Mercury web service, and demultiplexes them to other Zamboni queues. Zero was later adapted to manage the reprocessing of the first batch of UK Biobank exomes. It has since been adapted to drive workflows for other projects at the Broad. Zero is unusual in that it usually runs as a stateless command line program without special system privilege, and interfaces with services running both on premises and in Google Cloud. It also manages a processing workload as a set of inputs mapped to outputs instead of tracking the progress of individual sample workflows. A Zero user need only specify a source of inputs, a workflow to run, an execution environment, and an output location. Then each time it is invoked, Zero ensures that workflows are started and retried as needed until an output exists for every input. Zero has recently been adapted again to deploy as a web service under Google App Engine (GAE) though most of the value of Zero is still not available to the server. And now it has the new name WFL. The role of WFL in Terra \u00b6 Diagrams of the new DSP processing system show a WFL service subscribed to event streams from the Data Repository (DR), with interfaces to both the Data and the Method Repositories. The implication is that something notifies WFL of new data in the Data Repository and WFL determines how to process it somehow. WFL then looks up whatever is required from the Method Repository, calls on other services as necessary to process the data and writes the results back to the DR. There is also, presumably, a web UI to track and debug the workflows managed by WFL. Many details are yet to be worked out. WFL Concepts \u00b6 WFL is designed around several novel concepts. Manage workloads instead of workflows. This is the biggest difference between Zamboni and Zero (WFL). Zamboni manages workflows whereas WFL manages workloads . Zamboni's unit of work is the workflow . Zamboni manages each workflow separately. A workflow is a transformation specified in WDL or Scala code that succeeds or fails to produce a result. The input to a workflow and its result may consist of multiple files, but they represent a single unit of work managed by a workflow engine such as Cromwell. Zamboni prepares a new workflow for each message it receives by packaging up the input and submitting it to a workflow engine. It then monitors that workflow and reports on its success or failure. WFL manages a workload , which indirectly comprises multiple workflows. Each workflow maps an input to some output, but WFL generally tracks only the inputs and outputs instead of the workflows themselves. Think of a workload as a set of inputs transformed via a workflow engine into a set of outputs. Call that set of outputs the result set . WFL generally does not care whether any individual workflow succeeds or fails. It merely considers all possible inputs specified by the workload, and looks for inputs whose outputs are missing from the result set. If some input lacks an output in the result set, WFL starts a new workflow to process that input. Note: This characterization is unfair to Zamboni. Zamboni also had to manage multiple workflows before the advent of Cromwell and still does when running workflows on SGE. But WFL can take advantage of Cromwell's job management to simplify its implementation. Specify inputs and outputs by general predicates. Each Zamboni message explicitly specifies an input to be processed. Zamboni then starts a workflow for that input and reports its status. Zamboni reports failure so a user can debug and manually succeed , reconsider, or restart the workflow. The output of a successful workflow is not Zamboni's concern. WFL finds inputs by applying a predicate specified by the user subject to some run-time constraint. Then WFL applies a function to each input to find how that input maps to the result set. Another predicate applied to the input, and its output in the result set, determines whether WFL will launch a workflow on that input. Those predicates and function can be anything expressed in a programming language. The run-time constraint is some strings passed on the command line. Minimize user input and decisions at run time. WFL gathers the predicates and mapping functions described above into a module that also knows how to generate everything a workflow engine needs to launch the workflow to process an input into a result output. That module name is one of a few run-time constraints specified by strings in a web form or on a command line. Further constraints are usually one or two of the following: - a processing environment ( `dev` `prod` `pharma5` ), - a file system directory ( `/seq/tng/tbl/` `file://home/tbl/` ), - a cloud object prefix ( `gs://bucket/folder/` `s3://bucket/` ), - a pathname suffix ( `.cram` `.vcf` ), - a spreadsheet ( or JSON , TSV , CSV , XML ) filename - or a count to limit the scope of a predicate . The module interprets the other constraints, determines which processing environments are allowed, and parses any files named accordingly. Maintain provenance. WFL runs out of a single JARfile built entirely from sources pulled from Git repositories. WFL records the Git commit hashes in the JARfile and adds them to every Cromwell workflow it starts. WFL can also preserve the Cromwell metadata alongside any result files generated by the workflow. Run with minimal privilege. Zamboni runs as a service with system account credentials such as picard . WFL is designed to run as whoever invokes it, such as tbl@broadinstitute.org . WFL fetches the users credentials from the environment when invoked from the command line. WFL requires authentication when running as a server, and constructs a JSON Web Token (JWT) to authorize other services as needed. Limit dependencies. WFL depends on a Java runtime, boot-clj to manage dependencies, Google Cloud SDK to deploy to Google App Engine (GAE). Of course, it also pulls in numerous Clojure and Java libraries at build time, and sources WDL files from the dsde-pipelines repository. A programmer needs only clone the wfl Git repositories, and run boot-clj to bootstrap WFL from source. And boot-clj is a single file: its own installer. Similarly, boot build builds WFL, and boot deploy deploys it to GAE. WFL attempts to be self-describing and self-documenting. It includes monitoring and diagnostic modules for tracking workload progress and debugging failures. WFL server \u00b6 The WFL client is a command-line batch program that a person runs intermittently on a laptop or virtual machine (VM). We are working to port the client functions of WFL to a ubiquitous web service (WFL server) running in Google Cloud. That port requires we solve several problems. State The WFL client is a stateless program that relies on consistent command line arguments to provide the constraints needed to drive the input discovery predicates and so on. Each user runs a separate process that lasts only as long as necessary to complete some stage of a workload. The WFL server is shared among all its users and runs continually. Therefore it requires some kind of data store (a database) to maintain the state of each workload across successive connections from web browsers. We intend to use the hosted Postgres service available to GAE applications for this. This work is already underway (GH-573). Authorization The WFL client assumes it runs in an authenticated context. It can pull credentials from the environment on every invocation that requires authorization to a service. The WFL server will also need to authorize services to run as some authenticated user, but cannot assume the credentials are always available, nor that there is a user present to provide them. WFL can already use OAuth2.0 to authenticate users against an identity provider and use the resulting credentials to build a JWT. It can also derive the bearer token required by most of our authorized services from a JWT. But WFL also needs some secure JWT store, so tokens are available to authorize services even when there is no active user connection. It also needs some mechanism to refresh tokens as they expire to support long-running workloads. Workload specification The user of a WFL service needs some way to specify a workload. A workload may be some set of inputs and the kind of workflow to run on them. A WFL client user now specifies a workload with a module name and a constraint . For example, ukb pharma5 110000 gs://broad-ukb/in/ gs://broad-ukb/out/ means find up to 110000 cloud objects with names prefixed with gs://broad-ukb/in/ , process them in the Cromwell set up for pharma5 , and store their outputs under gs://broad-ukb/out/ somewhere. The ukb module knows how to find .aligned.cram files under the gs://broad-ukb/in/ cloud path and set up the WDL and Cromwell dependencies and options necessary to reprocess them into .cram output files. The ukb module also knows how to find the Cromwell deployed to support pharma5 workloads, how to authorize the user to that Cromwell, and how to read any supporting data from other services. And finally, the ukb module knows how to determine which inputs do not yet have outputs under the gs://broad-ukb/out/ cloud path, and do not have workflows running in the pharma5 Cromwell. In an ideal design, this workload specification would integrate conveniently with the Data Repository's subscription or eventing service. In any case though, WFL needs some interface through which a user can specify what needs to be done. Workload management Workloads need to be started, stopped, and monitored somehow. This implies that there is some way to find active or suspended workloads, and affordances for acting on them. Users need some way to monitor the progress of a workload, and to find and debug workloads encountering unacceptable workflow failures. Monitoring and diagnostic code already exists in various WFL modules, but there is no easy way to use them from a web browser. Service interface WFL should be useful to programs other than web browsers. It is easy to imagine Terra users wanting to query WFL for the status of workloads directly without buggy and tedious screen scraping. WFL should at least export a query endpoint for use by other reporting services as well as its own browser interface. It would be nice to provide a familiar JSON or GraphQL query syntax to other services. Browser interface A browser interface should require little in addition to WFL's service interface. Ideally, one should be able to adapt WFL to new workloads via a browser interface without requiring a redeployment.","title":"WorkFlow Launcher's Role in Terra"},{"location":"terra/#workflow-launchers-role-in-terra","text":"","title":"WorkFlow Launcher's Role in Terra"},{"location":"terra/#summary","text":"The Data Sciences Platform (DSP) is building a new system (around Terra ) for storing and processing biological data. The system design includes a Data Repository where data is stored, and a Methods Repository that executably describes transformations on that data. In the new system, the DSP needs something to fulfill the role that Zamboni currently plays in DSP's current infrastructure to support the Genomics Platform (GP). Zamboni watches various queues for messages describing new data and how to process it. Zamboni interprets those messages to dispatch work to workflow engines (running on the premises or in the cloud) and monitors the progress of those workflows. The Zamboni web UI allows users to track the progress of workflows, and enables Ops engineers to debug problems and resume or restart failed workflows. Zamboni can run workflows on both a local Sun Grid Engine (SGE), and on Cromwell on premises and in the cloud. We think that WFL can fill the role of Zamboni in the new data storage and processing system that DSP is developing now.","title":"Summary"},{"location":"terra/#history","text":"WFL began as a project to replace a Zamboni starter , with the old name \"Zero\". A starter is a Zamboni component that brokers messages among the queues that Zamboni watches. It interprets messages queued from a Laboratory Information Management System (LIMS), such as the Mercury web service, and demultiplexes them to other Zamboni queues. Zero was later adapted to manage the reprocessing of the first batch of UK Biobank exomes. It has since been adapted to drive workflows for other projects at the Broad. Zero is unusual in that it usually runs as a stateless command line program without special system privilege, and interfaces with services running both on premises and in Google Cloud. It also manages a processing workload as a set of inputs mapped to outputs instead of tracking the progress of individual sample workflows. A Zero user need only specify a source of inputs, a workflow to run, an execution environment, and an output location. Then each time it is invoked, Zero ensures that workflows are started and retried as needed until an output exists for every input. Zero has recently been adapted again to deploy as a web service under Google App Engine (GAE) though most of the value of Zero is still not available to the server. And now it has the new name WFL.","title":"History"},{"location":"terra/#the-role-of-wfl-in-terra","text":"Diagrams of the new DSP processing system show a WFL service subscribed to event streams from the Data Repository (DR), with interfaces to both the Data and the Method Repositories. The implication is that something notifies WFL of new data in the Data Repository and WFL determines how to process it somehow. WFL then looks up whatever is required from the Method Repository, calls on other services as necessary to process the data and writes the results back to the DR. There is also, presumably, a web UI to track and debug the workflows managed by WFL. Many details are yet to be worked out.","title":"The role of WFL in Terra"},{"location":"terra/#wfl-concepts","text":"WFL is designed around several novel concepts. Manage workloads instead of workflows. This is the biggest difference between Zamboni and Zero (WFL). Zamboni manages workflows whereas WFL manages workloads . Zamboni's unit of work is the workflow . Zamboni manages each workflow separately. A workflow is a transformation specified in WDL or Scala code that succeeds or fails to produce a result. The input to a workflow and its result may consist of multiple files, but they represent a single unit of work managed by a workflow engine such as Cromwell. Zamboni prepares a new workflow for each message it receives by packaging up the input and submitting it to a workflow engine. It then monitors that workflow and reports on its success or failure. WFL manages a workload , which indirectly comprises multiple workflows. Each workflow maps an input to some output, but WFL generally tracks only the inputs and outputs instead of the workflows themselves. Think of a workload as a set of inputs transformed via a workflow engine into a set of outputs. Call that set of outputs the result set . WFL generally does not care whether any individual workflow succeeds or fails. It merely considers all possible inputs specified by the workload, and looks for inputs whose outputs are missing from the result set. If some input lacks an output in the result set, WFL starts a new workflow to process that input. Note: This characterization is unfair to Zamboni. Zamboni also had to manage multiple workflows before the advent of Cromwell and still does when running workflows on SGE. But WFL can take advantage of Cromwell's job management to simplify its implementation. Specify inputs and outputs by general predicates. Each Zamboni message explicitly specifies an input to be processed. Zamboni then starts a workflow for that input and reports its status. Zamboni reports failure so a user can debug and manually succeed , reconsider, or restart the workflow. The output of a successful workflow is not Zamboni's concern. WFL finds inputs by applying a predicate specified by the user subject to some run-time constraint. Then WFL applies a function to each input to find how that input maps to the result set. Another predicate applied to the input, and its output in the result set, determines whether WFL will launch a workflow on that input. Those predicates and function can be anything expressed in a programming language. The run-time constraint is some strings passed on the command line. Minimize user input and decisions at run time. WFL gathers the predicates and mapping functions described above into a module that also knows how to generate everything a workflow engine needs to launch the workflow to process an input into a result output. That module name is one of a few run-time constraints specified by strings in a web form or on a command line. Further constraints are usually one or two of the following: - a processing environment ( `dev` `prod` `pharma5` ), - a file system directory ( `/seq/tng/tbl/` `file://home/tbl/` ), - a cloud object prefix ( `gs://bucket/folder/` `s3://bucket/` ), - a pathname suffix ( `.cram` `.vcf` ), - a spreadsheet ( or JSON , TSV , CSV , XML ) filename - or a count to limit the scope of a predicate . The module interprets the other constraints, determines which processing environments are allowed, and parses any files named accordingly. Maintain provenance. WFL runs out of a single JARfile built entirely from sources pulled from Git repositories. WFL records the Git commit hashes in the JARfile and adds them to every Cromwell workflow it starts. WFL can also preserve the Cromwell metadata alongside any result files generated by the workflow. Run with minimal privilege. Zamboni runs as a service with system account credentials such as picard . WFL is designed to run as whoever invokes it, such as tbl@broadinstitute.org . WFL fetches the users credentials from the environment when invoked from the command line. WFL requires authentication when running as a server, and constructs a JSON Web Token (JWT) to authorize other services as needed. Limit dependencies. WFL depends on a Java runtime, boot-clj to manage dependencies, Google Cloud SDK to deploy to Google App Engine (GAE). Of course, it also pulls in numerous Clojure and Java libraries at build time, and sources WDL files from the dsde-pipelines repository. A programmer needs only clone the wfl Git repositories, and run boot-clj to bootstrap WFL from source. And boot-clj is a single file: its own installer. Similarly, boot build builds WFL, and boot deploy deploys it to GAE. WFL attempts to be self-describing and self-documenting. It includes monitoring and diagnostic modules for tracking workload progress and debugging failures.","title":"WFL Concepts"},{"location":"terra/#wfl-server","text":"The WFL client is a command-line batch program that a person runs intermittently on a laptop or virtual machine (VM). We are working to port the client functions of WFL to a ubiquitous web service (WFL server) running in Google Cloud. That port requires we solve several problems. State The WFL client is a stateless program that relies on consistent command line arguments to provide the constraints needed to drive the input discovery predicates and so on. Each user runs a separate process that lasts only as long as necessary to complete some stage of a workload. The WFL server is shared among all its users and runs continually. Therefore it requires some kind of data store (a database) to maintain the state of each workload across successive connections from web browsers. We intend to use the hosted Postgres service available to GAE applications for this. This work is already underway (GH-573). Authorization The WFL client assumes it runs in an authenticated context. It can pull credentials from the environment on every invocation that requires authorization to a service. The WFL server will also need to authorize services to run as some authenticated user, but cannot assume the credentials are always available, nor that there is a user present to provide them. WFL can already use OAuth2.0 to authenticate users against an identity provider and use the resulting credentials to build a JWT. It can also derive the bearer token required by most of our authorized services from a JWT. But WFL also needs some secure JWT store, so tokens are available to authorize services even when there is no active user connection. It also needs some mechanism to refresh tokens as they expire to support long-running workloads. Workload specification The user of a WFL service needs some way to specify a workload. A workload may be some set of inputs and the kind of workflow to run on them. A WFL client user now specifies a workload with a module name and a constraint . For example, ukb pharma5 110000 gs://broad-ukb/in/ gs://broad-ukb/out/ means find up to 110000 cloud objects with names prefixed with gs://broad-ukb/in/ , process them in the Cromwell set up for pharma5 , and store their outputs under gs://broad-ukb/out/ somewhere. The ukb module knows how to find .aligned.cram files under the gs://broad-ukb/in/ cloud path and set up the WDL and Cromwell dependencies and options necessary to reprocess them into .cram output files. The ukb module also knows how to find the Cromwell deployed to support pharma5 workloads, how to authorize the user to that Cromwell, and how to read any supporting data from other services. And finally, the ukb module knows how to determine which inputs do not yet have outputs under the gs://broad-ukb/out/ cloud path, and do not have workflows running in the pharma5 Cromwell. In an ideal design, this workload specification would integrate conveniently with the Data Repository's subscription or eventing service. In any case though, WFL needs some interface through which a user can specify what needs to be done. Workload management Workloads need to be started, stopped, and monitored somehow. This implies that there is some way to find active or suspended workloads, and affordances for acting on them. Users need some way to monitor the progress of a workload, and to find and debug workloads encountering unacceptable workflow failures. Monitoring and diagnostic code already exists in various WFL modules, but there is no easy way to use them from a web browser. Service interface WFL should be useful to programs other than web browsers. It is easy to imagine Terra users wanting to query WFL for the status of workloads directly without buggy and tedious screen scraping. WFL should at least export a query endpoint for use by other reporting services as well as its own browser interface. It would be nice to provide a familiar JSON or GraphQL query syntax to other services. Browser interface A browser interface should require little in addition to WFL's service interface. Ideally, one should be able to adapt WFL to new workloads via a browser interface without requiring a redeployment.","title":"WFL server"}]}