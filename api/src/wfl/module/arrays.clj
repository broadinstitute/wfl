(ns wfl.module.arrays
  "Process Arrays for the Broad Genomics Platform."
  (:require [clojure.data.json :as json]
            [clojure.string :as str]
            [wfl.environments :as env]
            [wfl.api.workloads :as workloads :refer [defoverload]]
            [wfl.jdbc :as jdbc]
            [wfl.module.batch :as batch]
            [wfl.references :as references]
            [wfl.service.postgres :as postgres]
            [wfl.service.terra :as terra]
            [wfl.util :as util])
  (:import [java.time OffsetDateTime]))

(def pipeline "GPArrays")

(def method-configuration-name "Arrays")

(def workflow-wdl
  "The top-level WDL file and its version."
  {:release "Arrays_v2.3.0"
   :top     "pipelines/broad/arrays/single_sample/Arrays.wdl"})

(def primary-keys
  "An arrays workflow can be uniquely identified by its `chip_well_barcode` and
  `analysis_version_number`. Consequently, these are the primary keys in the
  database."
  [:chip_well_barcode :analysis_version_number])

(def fingerprinting
  "Fingerprinting inputs for arrays."
  {:haplotype_database_file "gs://gcp-public-data--broad-references/hg19/v0/Homo_sapiens_assembly19.haplotype_database.txt"
   :variant_rsids_file      "gs://broad-references-private/hg19/v0/Homo_sapiens_assembly19.haplotype_database.snps.list"})

(def other-inputs
  "Miscellaneous inputs for arrays."
  {:contamination_controls_vcf       nil
   :subsampled_metrics_interval_list nil
   :disk_size                        100
   :preemptible_tries                3})

(defn env-inputs
  "Array inputs for ENVIRONMENT that do not depend on the input file."
  [environment]
  {:vault_token_path (get-in env/stuff [environment :vault_token_path])
   :environment      ({:arrays-dev "dev" :arrays-prod "prod"} environment)})

(defn get-per-sample-inputs
  "Throw or return per-sample INPUTS."
  [inputs]
  (let [mandatory-keys [:analysis_version_number
                        :bead_pool_manifest_file
                        :call_rate_threshold
                        :chip_well_barcode
                        :cluster_file
                        :extended_chip_manifest_file
                        :fingerprint_genotypes_vcf_file
                        :fingerprint_genotypes_vcf_index_file
                        :green_idat_cloud_path
                        :params_file
                        :red_idat_cloud_path
                        :reported_gender
                        :sample_alias
                        :sample_lsid]
        optional-keys  [;; genotype concordance inputs
                        :control_sample_vcf_file
                        :control_sample_vcf_index_file
                        :control_sample_intervals_file
                        :control_sample_name
                        :genotype_concordance_threshold
                        ;; cloud path of a thresholds file to be used with zCall
                        :zcall_thresholds_file
                        ;; cloud path of the Illumina gender cluster file
                        :gender_cluster_file
                        ;; arbitrary path to be used by BAFRegress
                        :minor_allele_frequency_file]
        mandatory      (select-keys inputs mandatory-keys)
        optional       (select-keys inputs optional-keys)
        missing        (vec (keep (fn [k] (when (nil? (k mandatory)) k)) mandatory-keys))]
    (when (seq missing)
      (throw (Exception. (format "Missing per-sample inputs: %s" missing))))
    (merge optional mandatory)))

(defn make-inputs
  "Return inputs for arrays processing in ENVIRONMENT from PER-SAMPLE-INPUTS."
  [environment per-sample-inputs]
  (-> (merge references/hg19-arrays-references
             fingerprinting
             other-inputs
             (env-inputs environment)
             (get-per-sample-inputs per-sample-inputs))
      (util/prefix-keys :Arrays)))

;; The table is named with the id generated by the jdbc/insert!
;; so this needs to update the workload table inline after creating the
;; GPArrays table, so far we cannot find a better way to bypass
;; https://www.postgresql.org/docs/current/datatype-numeric.html#DATATYPE-SERIAL
;;
(defn add-arrays-workload!
  "Use transaction TX to add the workload described by REQUEST."
  [tx {:keys [items] :as request}]
  (let [[id table] (batch/add-workload-table! tx workflow-wdl request)]
    (letfn [(form [m id] (-> m
                             (update :inputs json/write-str)
                             (assoc :id id)))]
      (jdbc/insert-multi! tx table (map form items (range))))
    id))

(defn start-arrays-workload!
  "Use transaction TX to start the WORKLOAD."
  [tx {:keys [executor items project uuid] :as workload}]
  (let [now (OffsetDateTime/now)
        method-configuration-namespace (first (str/split project #"/"))]
    (letfn [(submit! [{:keys [id] :as workflow}]
              (let [inputs           (:inputs workflow)
                    entity-type      (:entity-type inputs)
                    entity-name      (:entity-name inputs)]
                [id (terra/create-submission executor project method-configuration-name
                                             method-configuration-namespace entity-type entity-name)]))
            (update! [tx [id uuid]]
              (when uuid
                (jdbc/update! tx items
                              {:updated now :uuid uuid :status "Submitted"}
                              ["id = ?" id])))]
      (let [ids-uuids (map submit! (:workflows workload))]
        (run! (partial update! tx) ids-uuids)
        (jdbc/update! tx :workload {:started now} ["uuid = ?" uuid])))))

(defmethod workloads/create-workload!
  pipeline
  [tx request]
  (->> request
       (add-arrays-workload! tx)
       (workloads/load-workload-for-id tx)))

(defmethod workloads/start-workload!
  pipeline
  [tx {:keys [id] :as workload}]
  (do
    (start-arrays-workload! tx workload)
    (workloads/load-workload-for-id tx id)))

(defmethod workloads/update-workload!
  pipeline
  [tx workload]
  (try
    (postgres/update-terra-workflow-statuses! tx workload)
    (postgres/update-workload-status! tx workload)
    (workloads/load-workload-for-id tx (:id workload))
    (catch Throwable cause
      (throw (ex-info "Error updating arrays workload"
                      {:workload workload} cause)))))

(defmethod workloads/load-workload-impl
  pipeline
  [tx {:keys [items] :as workload}]
  (letfn [(unnilify [m] (into {} (filter second m)))]
    (->> (postgres/get-table tx items)
         (mapv (comp #(update % :inputs util/parse-json)
                     unnilify))
         (assoc workload :workflows)
         unnilify)))
